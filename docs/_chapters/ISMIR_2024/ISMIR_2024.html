<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Towards Automated Personal Value Estimation in Song Lyrics – Ground Truthing is a Field</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../_chapters/ISMIR_2020/ISMIR_2020.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../_chapters/ISMIR_2024/ISMIR_2024.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automated Personal Value Estimation in Song Lyrics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Ground Truthing is a Field</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2018/ISMIR_2018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2020/ISMIR_2020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">“Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2024/ISMIR_2024.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automated Personal Value Estimation in Song Lyrics</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec:introduction" id="toc-sec:introduction" class="nav-link active" data-scroll-target="#sec\:introduction"><span class="header-section-number">6</span> Introduction</a></li>
  <li><a href="#personal-values" id="toc-personal-values" class="nav-link" data-scroll-target="#personal-values"><span class="header-section-number">7</span> Personal Values</a></li>
  <li><a href="#primary-lyrics-data" id="toc-primary-lyrics-data" class="nav-link" data-scroll-target="#primary-lyrics-data"><span class="header-section-number">8</span> Primary Lyrics Data</a>
  <ul class="collapse">
  <li><a href="#sec:primarylyricdata:fuzzysampling" id="toc-sec:primarylyricdata:fuzzysampling" class="nav-link" data-scroll-target="#sec\:primarylyricdata\:fuzzysampling"><span class="header-section-number">8.1</span> Fuzzy Stratified Sampling</a></li>
  </ul></li>
  <li><a href="#sec:groundtruthing" id="toc-sec:groundtruthing" class="nav-link" data-scroll-target="#sec\:groundtruthing"><span class="header-section-number">9</span> Ground-Truthing Procedure</a>
  <ul class="collapse">
  <li><a href="#reliability-agreement-and-initial-validation" id="toc-reliability-agreement-and-initial-validation" class="nav-link" data-scroll-target="#reliability-agreement-and-initial-validation"><span class="header-section-number">9.1</span> Reliability, Agreement and Initial Validation</a></li>
  </ul></li>
  <li><a href="#automated-scoring" id="toc-automated-scoring" class="nav-link" data-scroll-target="#automated-scoring"><span class="header-section-number">10</span> Automated Scoring</a></li>
  <li><a href="#descriptive-analyses" id="toc-descriptive-analyses" class="nav-link" data-scroll-target="#descriptive-analyses"><span class="header-section-number">11</span> Descriptive Analyses</a></li>
  <li><a href="#limitations-and-future-work" id="toc-limitations-and-future-work" class="nav-link" data-scroll-target="#limitations-and-future-work"><span class="header-section-number">12</span> Limitations and Future Work</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">13</span> Conclusion</a></li>
  <li><a href="#ethics-statement" id="toc-ethics-statement" class="nav-link" data-scroll-target="#ethics-statement"><span class="header-section-number">14</span> Ethics Statement</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automated Personal Value Estimation in Song Lyrics</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Most music widely consumed in Western Countries contains song lyrics, with U.S. samples reporting almost all of their song libraries contain lyrics. In parallel, social science theory suggests that personal values - the abstract goals that guide our decisions and behaviors - play an important role in communication: we share what is important to us to coordinate efforts, solve problems and meet challenges. Thus, the values communicated in song lyrics may be similar or different to those of the listener, and by extension affect the listener’s reaction to the song. This suggests that working towards automated estimation of values in lyrics may assist in downstream MIR tasks, in particular, personalization. However, as highly subjective text, song lyrics present a challenge in terms of sampling songs to be annotated, annotation methods, and in choosing a method for aggregation. In this project, we take a perspectivist approach, guided by social science theory, to gathering annotations, estimating their quality, and aggregating them. We then compare aggregated ratings to estimates based on pre-trained sentence/word embedding models by employing a validated value dictionary. We discuss conceptually ‘fuzzy’ solutions to sampling and annotation challenges, promising initial results in annotation quality and in automated estimations, and future directions.</p>
  </div>
</div>


</header>


<section id="sec:introduction" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Introduction</h1>
<p>Popular music in Western countries almost always contains lyrics, making song lyrics a widely, repeatedly consumed <span class="citation" data-cites="conrad2019extreme"><a href="#ref-conrad2019extreme" role="doc-biblioref">[1]</a></span> form of text. Over 616 million people subscribe to streaming services worldwide<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, many of whom stream more than an hour of music every day<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Lyrics have been shown to be a salient component of music <span class="citation" data-cites="demetriou2018vocals"><a href="#ref-demetriou2018vocals" role="doc-biblioref">[2]</a></span>, and out of over 1400 number-1 singles in the UK charts, only 30 were instrumental<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. The two representative US population samples that were our annotators indicate a median 90%&nbsp;of songs in their libraries contain lyrics (Figure&nbsp;<a href="#fig:lyrics_percentages" data-reference-type="ref" data-reference="fig:lyrics_percentages">1</a>).</p>
<figure id="fig:lyrics_percentages" class="figure">
<figcaption>
Distribution of self-reported percentage of music library containing lyrics from two representative US samples, n=505 and n=600 respectively.
</figcaption>
</figure>
<p>It is thus not surprising that informative relationships between popular songs and their lyrical content have been shown: e.g., country music lyrics rarely include political concepts <span class="citation" data-cites="van2005world"><a href="#ref-van2005world" role="doc-biblioref">[3]</a></span>, and songs with more typical <span class="citation" data-cites="north2020relationship"><a href="#ref-north2020relationship" role="doc-biblioref">[4]</a></span> and more negative <span class="citation" data-cites="brand2019cultural"><a href="#ref-brand2019cultural" role="doc-biblioref">[5]</a></span> lyrics appear to be more successful. <span class="citation" data-cites="howlin2020patients"><a href="#ref-howlin2020patients" role="doc-biblioref">[6]</a></span> showed that patients are more likely to choose music with lyrics when participating in music-based pain reduction interventions, although melody had an overall larger effect <span class="citation" data-cites="ali2006songs"><a href="#ref-ali2006songs" role="doc-biblioref">[7]</a></span> showed that lyrics enhance self reported emotional responses to music, and <span class="citation" data-cites="brattico2011functional"><a href="#ref-brattico2011functional" role="doc-biblioref">[8]</a></span> showed a number of additional brain regions were active during the listening of sad music with lyrics, vs.&nbsp;sad music without lyrics. In fields closer to MIR, <span class="citation" data-cites="kim2020butter"><a href="#ref-kim2020butter" role="doc-biblioref">[9]</a></span> show that estimating psychological concepts from lyrics showed a small benefit in a number of MIR tasks, and <span class="citation" data-cites="preniqi2022more"><a href="#ref-preniqi2022more" role="doc-biblioref">[10]</a></span> showed a correlation between moral principles estimated from song lyrics and music preferences.</p>
<p>A connection between music lyrics and music preferences anticipated by theory involves the personal values perceived in the lyrics by listeners. Prior work has shown correlations between an individual’s values, and the music they listen to <span class="citation" data-cites="manolios2019influence gardikiotis2012rock swami2013metalheads preniqi2022more"><a href="#ref-preniqi2022more" role="doc-biblioref">[10]</a>, <a href="#ref-manolios2019influence" role="doc-biblioref">[11]</a>, <a href="#ref-gardikiotis2012rock" role="doc-biblioref">[12]</a>, <a href="#ref-swami2013metalheads" role="doc-biblioref">[13]</a></span>, suggesting that we seek music in line with our principles. Yet we have not seen an attempt to measure perceived personal values expressed in the lyrics themselves via human annotation or automated methods.</p>
<p>In this work we take a first step towards the automated estimating the values perceived in song lyrics. As artistic and expressive language, lyrics are ambiguous text: they contain different forms of analogy and wordplay <span class="citation" data-cites="sandri2023don"><a href="#ref-sandri2023don" role="doc-biblioref">[14]</a></span>. Thus we take a perspectivist approach to the annotations: because we expect that perceptions will vary substantially more than in other annotation tasks, we aim to represent the general perceptions of only one population. We account for the subjectivity by gathering a large number of ratings (median 27) per song from a targeted population sample (U.S.), of 360 carefully sampled song lyrics, using a psychometric questionnaire that we adjust for this purpose. We treat values in line with theory: as ranked lists, using Robust Ranking Aggregation (RRA) to arrive at our ‘ground truth’. We then gather estimates from word embedding models, by measuring semantic similarity between the lyrics and a validated dictionary. We show that ranked lists from estimates correlate moderately with annotation aggregates. We then discuss the implications of our results, the limitations of this project, and anticipated future work.</p>
</section>
<section id="personal-values" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Personal Values</h1>
<p>The modern study of human values spans over 500 samples in nearly 100 countries over the past 30 years, and has shown a relatively stable structure <span class="citation" data-cites="sagiv2022personal"><a href="#ref-sagiv2022personal" role="doc-biblioref">[15]</a></span>, as illustrated in Figure&nbsp;<a href="#fig:circle" data-reference-type="ref" data-reference="fig:circle">2</a>. Personal values are a component of personality, defined as the hierarchy of principles that guide a person’s thoughts, behaviors, and the way they evaluate events <span class="citation" data-cites="schwartz1987toward schwartz2012overview"><a href="#ref-schwartz1987toward" role="doc-biblioref">[16]</a>, <a href="#ref-schwartz2012overview" role="doc-biblioref">[17]</a></span>. Basic human values can be used to describe people or groups: social science theory suggests that each person uses a hierarchical list of values as life-guiding principles <span class="citation" data-cites="rokeach1973nature"><a href="#ref-rokeach1973nature" role="doc-biblioref">[18]</a></span>, such that we prioritize some values over others as we make decisions. Schwartz’s theory is the most widely used in social and cultural psychology, and has shown correlations with important behaviors, ranging from political affiliation to personal preferences <span class="citation" data-cites="sagiv2022personal"><a href="#ref-sagiv2022personal" role="doc-biblioref">[15]</a></span>.</p>
<p>We communicate our values in order to gain cooperation and coordinate our efforts, according to Schwartz <span class="citation" data-cites="schwartz1992universals"><a href="#ref-schwartz1992universals" role="doc-biblioref">[19]</a></span>. Thus our values will manifest in the words that we use&nbsp;<span class="citation" data-cites="boyd2017language"><a href="#ref-boyd2017language" role="doc-biblioref">[20]</a></span>. Although personal values are traditionally measured by having individual people complete validated psychological questionnaires, it has been argued that values may be clearly expressed in the speech and text that we produce <span class="citation" data-cites="boyd2017language"><a href="#ref-boyd2017language" role="doc-biblioref">[20]</a></span>.</p>
<figure id="fig:circle" class="figure">
<figcaption>
Visualization of the Schwartz 10-value inventory from <span class="citation" data-cites="schwartz1992universals"></span> used in this paper, such that more abstract values of Conservation, vs.&nbsp;Openness to Change, and Self-transcendence vs.&nbsp;Self-enhancement form 4 higher-order abstract values. Illustration adapted from <span class="citation" data-cites="maio2010mental"></span>.
</figcaption>
</figure>
<p>A common approach to measuring psychological aspects in text is to validate dictionaries: curated sets of words, with subsets aimed at measuring each component of the psychological aspect in question <span class="citation" data-cites="pennebaker2015development graham2009liberals holtrop2022exploring ponizovskiy2020development"><a href="#ref-pennebaker2015development" role="doc-biblioref">[21]</a>, <a href="#ref-graham2009liberals" role="doc-biblioref">[22]</a>, <a href="#ref-holtrop2022exploring" role="doc-biblioref">[23]</a>, <a href="#ref-ponizovskiy2020development" role="doc-biblioref">[24]</a></span>. Some work estimating the values of the authors of text has been conducted on individuals who have written personal essays and social media posts e.g.&nbsp;<span class="citation" data-cites="maheshwari2017societal ponizovskiy2020development"><a href="#ref-ponizovskiy2020development" role="doc-biblioref">[24]</a>, <a href="#ref-maheshwari2017societal" role="doc-biblioref">[25]</a></span>, and in arguments abstracted from various forms of public facing text <span class="citation" data-cites="kiesel2022identifying"><a href="#ref-kiesel2022identifying" role="doc-biblioref">[26]</a></span>. However, we have not seen work aimed at measuring values <em>perceived</em> in text, measuring them along a scale as in prior work <span class="citation" data-cites="schwartz1992universals"><a href="#ref-schwartz1992universals" role="doc-biblioref">[19]</a></span>, or ultimately treating them as a hierarchical list in line with theory <span class="citation" data-cites="rokeach1973nature"><a href="#ref-rokeach1973nature" role="doc-biblioref">[18]</a></span>.</p>
</section>
<section id="primary-lyrics-data" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Primary Lyrics Data</h1>
<p>We aim to collect a sample of lyric data where the lyrics are as accurate as possible, and our sample is as representative as possible. We sampled from the population of songs in the Million Playlist Dataset (MPD)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> as it is large and recent compared to other similar datasets. The lyrics themselves were obtained through the API of <a href="https://www.musixmatch.com/">Musixmatch</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, a lyrics and music language platform. Musixmatch lyrics are crowdsourced by users who add, correct, sync, and translate them. Musixmatch then engages in several steps to verify quality of content, including spam detection, formatting, spelling and translation checking, as well as manual verification by over 2000 community curators, and a local team of Musixmatch editors. Via their API, Musixmatch provided us with an estimated first 30% of the lyrics of each song.</p>
<p>Using the ‘fuzzy’ stratified sampling method described below, we sampled 2200 songs. Three members of the research team manually screened approximately 600 of the 2200 songs for inclusion. Each set of lyrics was confirmed to be a match to the actual song, and for suitability<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Lyrics were unsuitable if they were: 1) not English, 2) completely onomatopoetic, 3) repetitions of single words or phrases, 4) too few words to estimate values present or, 5) were not a match to the meta-data of the song, e.g.&nbsp;artist title, song name. This resulted in 380 songs, 20 of which were used in a pilot study to determine the number of ratings to gather per song, and 360 were used for annotation.</p>
<section id="sec:primarylyricdata:fuzzysampling" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec:primarylyricdata:fuzzysampling"><span class="header-section-number">8.1</span> Fuzzy Stratified Sampling</h2>
<p>An initial challenge is determining how to represent a corpus. In our case, the population of songs is known to be very large<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. An ideal scenario would be one in which we aim for a known number of songs, randomly sampled from within clearly defined strata, i.e.&nbsp;relevant subgroups, also known as <em>stratified random sampling</em>&nbsp;<span class="citation" data-cites="groves2009survey"><a href="#ref-groves2009survey" role="doc-biblioref">[27]</a></span>. However, for music, we do not know how many songs we would need to sample in order to reach saturation, what the relevant strata to randomly sample within should be, and how to measure relevant parameters from each stratum.</p>
<p>Some measurable strata that affect the use of language in song lyrics are clear: e.g., the year of release, which may reflect different events or time-specific colloquial slang. Others are less clear: e.g., there is no single metric of popularity for music, although it can be estimated from various sources such as hit charts. Some may be very subjective, such as genre, for which there may be some overlap of human labelling, but no clear taxonomy exists in the eyes of musicological domain experts&nbsp;<span class="citation" data-cites="Liem2012MusicGap"><a href="#ref-Liem2012MusicGap" role="doc-biblioref">[28]</a></span>.</p>
<p>Based upon these considerations, we aimed for a stratified random sampling procedure, based on strata that we acknowledge to be justifiable given our purpose, yet in some cases conceptually ‘fuzzy’: (1) release date; (2) popularity, operationalized as artist playlist frequency from the MPD&nbsp;<span class="citation" data-cites="DBLP:conf/recsys/ChenLSZ18"><a href="#ref-DBLP:conf/recsys/ChenLSZ18" role="doc-biblioref">[29]</a></span>; (3) genre, estimated from topic modeling on Million Song Dataset artist tags <span class="citation" data-cites="schindler2012facilitating"><a href="#ref-schindler2012facilitating" role="doc-biblioref">[30]</a></span>; (4) lyric topic, through a bag-of-words representation of the lyrics data. Popularity and Release date were divided into equally spaced bins; e.g.&nbsp;we divided release year into decades (60s, 70s, 80s, and so on), and genre and lyric topic were divided into categories.</p>
<p>Release date was quantized into 14 bins in 10-year increments from 1890-2030. Popularity was exponentially distributed, and thus manually binned, to make the quantiles per each of the 7 bins as similar as possible. Thus, the first bin contained the lowest 40% of the population in terms of popularity, while the 7th bin contained the highest 9%. Topic modelling was applied on a bag-of-word representation of the lyrics data and artist-tag data to yield <span class="math inline">\(25\)</span> estimated genres and <span class="math inline">\(9\)</span> lyrics topic strata, respectively.</p>
<p>We observed a skewness of data concentration with regard to several of our strata, e.g., songs that are recent and widely popular are most likely be drawn. To correct for this and thus get a more representative sample of an overall song catalogue, we oversample from less populated bins. For this, we use the maximum-a-posteriori (MAP) estimate of the categorical distribution of each stratum. The oversampling is controlled by concentration parameter <span class="math inline">\(a\)</span> of the symmetric Dirichlet distribution. We heuristically set this parameter such that songs in underpopulated bins still will make up up 5-10 % of our overall pool<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Through this method, we subsampled our initial 2200 songs lyrics.</p>
<figure id="fig:mds_plots" class="figure">
<figcaption>
MDS plots derived from the correlation plot reported in <span class="citation" data-cites="schwartz2001extending"></span>, and our participant responses as confidence-weighted means
</figcaption>
</figure>
</section>
</section>
<section id="sec:groundtruthing" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Ground-Truthing Procedure</h1>
<p>We chose to obtain our annotations from samples of the US population, representative in terms of self-reported sex, ethnicity and age, through the Prolific<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> platform. Annotator pools comprised of two samples, the first n=505 wave participated in a pilot study to estimate the number of ratings per song needed on average, and the second n=600 wave comprised our main data collection. Participants completed the survey on the Qualtrics <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> platform.</p>
<p>We clearly differentiate between the Author and the Speaker of lyrics by explaining to participants that the Author of song lyrics may write from the perspective of someone or something else (the Speaker). 17 randomly selected sets of lyrics were then shown to each participant along with instructions to annotate each with the values of the Speaker. We adapted the 10-item questionnaire used in <span class="citation" data-cites="lindeman2005measuring"><a href="#ref-lindeman2005measuring" role="doc-biblioref">[31]</a></span> for the value annotations, as it is the shortest questionnaire for assessing personal values whose validity and reliability have been assessed<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. As in <span class="citation" data-cites="lindeman2005measuring"><a href="#ref-lindeman2005measuring" role="doc-biblioref">[31]</a></span>, each questionnaire item is a specific value along with additional descriptive words e.g.&nbsp;POWER (social power, authority, wealth). We adjusted it by asking participants to indicate the values of the Speaker of the lyrics, and by having them indicate on a bar with -100 (opposed to their principles) on one end, and +100 (of supreme importance) on the other end instead of a likert scale. In addition, we asked participants to indicate how confident they were in their ratings, on a scale of 0 (not at all confident) to 100 (extremely confident), inspired by work that has shown that self-reported confidence in ratings can be used to estimate the accuracy of individual ratings <span class="citation" data-cites="cabitza2020if"><a href="#ref-cabitza2020if" role="doc-biblioref">[32]</a></span>.</p>
<p>We used a procedure similar to <span class="citation" data-cites="DeBruine_Jones_2018"><a href="#ref-DeBruine_Jones_2018" role="doc-biblioref">[33]</a></span> in order to determine the number of raters. Specifically, we recruited a representative 500+ participant sample of the US using the Prolific platform, who completed our survey for 20 songs. We then computed canonical mean ratings of each of the 10 values per song, and inter-rater reliability using Cronbach’s Alpha. We then estimated Cronbach’s alpha for a range of subsample sizes (5 to 50 participants in increments of 5), for each of the 10 values. We repeated this procedure 10 times per increment, separately for each of the 10 values, and examined the distribution of Cronbach’s Alpha. We specifically looked for the sample size with which Alpha exceeded .7 <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. We arrived at a conservative estimate of 25 ratings per set of lyrics, with songs receiving a median 27 ratings (range 22-30).</p>
<section id="reliability-agreement-and-initial-validation" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="reliability-agreement-and-initial-validation"><span class="header-section-number">9.1</span> Reliability, Agreement and Initial Validation</h2>
<p>The rater reliability was estimated via intra-class correlation for each personal value, (type 2k: see <span class="citation" data-cites="koo2016guideline"><a href="#ref-koo2016guideline" role="doc-biblioref">[34]</a></span>) using the ‘<code>psych</code>’ package in R <span class="citation" data-cites="Psychcitation"><a href="#ref-Psychcitation" role="doc-biblioref">[35]</a></span>, all of which exceeded .9 (excellent reliability). As an initial validation, we compare data simulated from values in the upper triangle of a correlation matrix reported in&nbsp;<span class="citation" data-cites="schwartz2001extending"><a href="#ref-schwartz2001extending" role="doc-biblioref">[36]</a></span> to those derived from our study. To aggregate our participants rankings for this purpose, we compute confidence-weighted means inspired by <span class="citation" data-cites="cabitza2020if"><a href="#ref-cabitza2020if" role="doc-biblioref">[32]</a></span>: we estimate confidence-weights by dividing participant’s self-reported confidence of a given rating by the highest possible response (100), and then compute aggregated means weighted by these. For both the simulated data and confidence-weighted mean scores, we generate a multi-dimensional scaling plot (MDS) <span class="citation" data-cites="davison2000multidimensional"><a href="#ref-davison2000multidimensional" role="doc-biblioref">[37]</a></span> for visual comparison, which has previously been used as method to assess measurements conform to theory&nbsp;<span class="citation" data-cites="ponizovskiy2020development lindeman2005measuring"><a href="#ref-ponizovskiy2020development" role="doc-biblioref">[24]</a>, <a href="#ref-lindeman2005measuring" role="doc-biblioref">[31]</a></span>. Note: the interpretation is to observe whether each of the values appears next to expected neighboring values, and not each value’s orientation. From these plots (Figure&nbsp;<a href="#fig:mds_plots" data-reference-type="ref" data-reference="fig:mds_plots">3</a>), in as little as our 360 annotated lyrics, we surprisingly see similar clusters and relative positioning relations emerging as those obtained from a formal cross-cultural study.</p>
<p>We coerced the annotated scores to ranked lists of values, such that the highest scoring value was at the top. We derived ranked lists per participant per song, and then used Robust Ranking Aggregation (RRA) to extract a single ranked list per song. Aggregation was conducted using R version 4.2.2.<span class="citation" data-cites="Rcitation"><a href="#ref-Rcitation" role="doc-biblioref">[38]</a></span>, and the <code>RobustRankAggreg</code> package <span class="citation" data-cites="RRAcitation"><a href="#ref-RRAcitation" role="doc-biblioref">[39]</a></span>. Briefly, RRA produces a ranked list by comparing the probability of the observed ranking of items to rankings from a uniform distribution. Essentially, scores are determined by comparing the height of an item on a set of lists to where it would appear if its rank were randomly distributed across lists. These scores are then subjected to statistical tests, where the resulting <em>p</em> value is Bonferronni corrected by the number of input lists<span class="citation" data-cites="kolde2012robust"><a href="#ref-kolde2012robust" role="doc-biblioref">[40]</a></span>. Thus, when an item appears in different positions on a list, the resulting p value is high, as its position appears randomly distributed.</p>
<p>As lyrics are ambiguous, we expect that some songs’ values are completely subjective. We operationalize these as randomly distributed rankings for all personal values for completely subjective songs, i.e. <em>p</em> values above .05 for all 10 items on the ranked list. Results from the RRA show 62 songs with <em>p</em> values above .05 for all 10 values, and 96 songs with only 1 value ranked. At most, 5 values were ranked, which occurred for 35 songs. Thus, we confirm that although there was correspondence in the scores that participants assigned per value per song, ranked lists did not always agree.</p>
<figure id="fig:rank_corr" class="figure">
<figcaption>
Rank correlations between NLP systems / word counts and Robust Ranking Aggregation lists, by normalization scheme.
</figcaption>
</figure>
</section>
</section>
<section id="automated-scoring" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Automated Scoring</h1>
<p>For automated scoring, we use a dictionary of words associated with the 10 Schwartz values&nbsp;<span class="citation" data-cites="ponizovskiy2020development"><a href="#ref-ponizovskiy2020development" role="doc-biblioref">[24]</a></span>. With this dictionary as reference, we computationally estimate the degree to which each value is reflected in the lyrics text according to traditional word counting&nbsp;<span class="citation" data-cites="ponizovskiy2020development"><a href="#ref-ponizovskiy2020development" role="doc-biblioref">[24]</a></span>, as well as by assessing cosine similarity between dictionary words and lyrics texts using four classes of pre-trained word embeddings: <code>word2vec</code>, a generic English word embedding trained on Google News dataset&nbsp;<span class="citation" data-cites="DBLP:conf/nips/MikolovSCCD13"><a href="#ref-DBLP:conf/nips/MikolovSCCD13" role="doc-biblioref">[41]</a></span>; <code>glove</code>, another generic English word embedding trained on Common Crawl dataset&nbsp;<span class="citation" data-cites="DBLP:conf/emnlp/PenningtonSM14"><a href="#ref-DBLP:conf/emnlp/PenningtonSM14" role="doc-biblioref">[42]</a></span>; <code>mxm-far-[1</code><span class="math inline">\(\sim\)</span><code>10]</code>, trained on the collected initial lyrics candidate pool, employing the Glove model&nbsp;<span class="citation" data-cites="DBLP:conf/emnlp/PenningtonSM14"><a href="#ref-DBLP:conf/emnlp/PenningtonSM14" role="doc-biblioref">[42]</a></span> (using ten models populated from ten cross-validation folds, whose parameters are tuned based on English word similarity judgement data&nbsp;<span class="citation" data-cites="DBLP:conf/acl/FaruquiD14"><a href="#ref-DBLP:conf/acl/FaruquiD14" role="doc-biblioref">[43]</a></span>.); <code>mxm-cv-[1</code><span class="math inline">\(\sim\)</span><code>10]</code>, ten variants of lyrics based word-embeddings from cross-validation folds selected by Glove loss values on the validation set; and finally, <code>sent-bert</code>, a transformer model that encodes sentence into a embedding vector, fine-tuning of a generic self-supervised language model called MPNet, which is trained on a large scale English corpus&nbsp;<span class="citation" data-cites="DBLP:conf/emnlp/ReimersG19"><a href="#ref-DBLP:conf/emnlp/ReimersG19" role="doc-biblioref">[44]</a></span>. Our process thus resulted in 24 sets of scores: 5 from models and one from word-counting, normalized using four methods.</p>
<p>We take the perspective from theory that that value assessments should be seen as ranked lists, and thus coerce scores to ranked lists per model per song. We then compute rank correlations between ranked lists derived from model scores and RRA lists from participants. As RRA lists assess lack of consensus on rankings, personal values with high <em>p</em> values received tied rankings, at the bottom of the list. Correlations were computed using Kendall’s <span class="math inline">\(\tau\)</span> which is robust to ties (Figure&nbsp;<a href="#fig:rank_corr" data-reference-type="ref" data-reference="fig:rank_corr">4</a>).</p>
<p>In earlier work&nbsp;<span class="citation" data-cites="richard2003one ponizovskiy2020development"><a href="#ref-ponizovskiy2020development" role="doc-biblioref">[24]</a>, <a href="#ref-richard2003one" role="doc-biblioref">[45]</a></span>, Pearson correlations of 0.1-0.2 were considered as moderate evidence of the validity of a proposed dictionary in relation to a psychometrically validated instrument. Although we are using a different metric, we observe several models whose mean rank correlations exceed the .10 mark. The mean Kendall’s <span class="math inline">\(\tau\)</span> values were highest for the word2vec, sent-bert, and wordcount models with null normalization (SD=.24, .30, and .34 respectively). We further observe that 76%&nbsp;of the rank correlations for word2vec exceed the .10 mark, followed by 56.1%&nbsp;from sent-bert, and 47.8%&nbsp;from wordcounts. Although none of these models had been thoroughly optimized and thus this cannot be interpreted as a thorough benchmark, we do see evidence of higher than expected correlations.</p>
<p>We also explored whether our fuzzy strata might hint towards more or less automatically scorable lyrics. We found most strata to be uninformative. However, when examining the rank correlations for our overall best performing model, word2vec, we did observe higher mean correlations for some artist tag topics than others (Figure&nbsp;<a href="#fig:artist_tag_topic" data-reference-type="ref" data-reference="fig:artist_tag_topic">5</a>). In particular, topics 10 (which included the tags: ‘jazz’, ‘chillout’, ‘lounge’, ‘trip-hop’, ‘downtempo’), 11 (which included the tags like: ‘metal’, ‘celtic’, ‘thrash metal’, ‘dutch’, ‘seen live’), and 16 (which included tags like: ‘country’, ‘Soundtrack’, ‘americana’, ‘danish’, ‘Disney’). Although speculative, we do expect that certain genres are more difficult to interpret than others, in particular for people who are generally unfamiliar with such music.</p>
<figure id="fig:artist_tag_topic" class="figure">
<figcaption>
Rank correlations between word2vec scores Robust Ranking Aggregation lists, per genre grouping operationalized as Artist Tag Topic.
</figcaption>
</figure>
</section>
<section id="descriptive-analyses" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Descriptive Analyses</h1>
<p>We conduct a further exploratory data analysis by examining the gathered value annotations with respect to the song strata introduced in Section&nbsp;<a href="#sec:primarylyricdata:fuzzysampling" data-reference-type="ref" data-reference="sec:primarylyricdata:fuzzysampling">3.1</a>. To better understand the overall patterns of value rankings in songs we visualize the average ranking of each value for each level of each stratum. To reflect the uncertainty of aggregated ranking from RRA, we employ ‘truncated’ rankings: the values within each aggregated ranked list are considered ties if their p-values higher than the threshold (<span class="math inline">\(p=0.05\)</span>), hence with high uncertainty in their ranking positions.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>In all results, we observe that there is a tendency of overall value ranking: 1) a generally strong presence of HEDONISM in higher ranks in all cases, followed by STIMULATION and SELF (SELF-DIRECTION). 2) ACHIEVEMENT and POWER generally follow next across all figures, and 3) the rest of the values, including BENEVOLENCE, UNIVERSALISM, SECURITY, CONFORMITY, and TRADITION overall rank lower, but show higher variability across strata. We refer to these three groups of values as <em>Group1</em> (HEDONISM, STIMULATION and SELF), <em>Group2</em> (ACHIEVEMENT and POWER), and <em>Group3</em> (the rest) for the rest of the section.</p>
<figure id="fig:avg_rank_trend_12" class="figure">
<figcaption>
Average value ranking from ‘release year’ (A) and ‘artist-playlist frequency’ (B). <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span> axis represent the strata and average ranking measure from RRA, respectively. Each point in different point shapes and vertical bars denote the average ranking value and its confidence interval (at 95% level). For visual convenience, we connected the same values with lines.
</figcaption>
</figure>
<p>Zooming in each to stratum, in Figure <a href="#fig:avg_rank_trend_12" data-reference-type="ref" data-reference="fig:avg_rank_trend_12">6</a>, we observe that the ‘release year’ (sub-figure <strong>A</strong>) strata show the most consistent and visible trend especially for Group3, which generally declines over time. Such a trend is not as obvious in Group1 and only partially observed in Group2. The low presence of Group3 is especially noticeable in the 1990s, although it regained its presence to some degree, a pattern which the SELF value from Group1 partially shares. Such visible movements suggest that the rank of specific values may evolve over time. In sub-figure <strong>B</strong>, we observe the most flat response across all strata considered: beyond the fluctuation pattern that is shared by all groups, there is no substantial variability among groups, which implies that popularity might not be as correlated as the ‘release year’.</p>
<figure id="fig:avg_rank_trend_34" class="figure">
<figcaption>
Average value ranking from ‘artist-tag topic’ (C) and ‘lyrics topic’ (D).
</figcaption>
</figure>
<p>Moving onto Figure&nbsp;<a href="#fig:avg_rank_trend_34" data-reference-type="ref" data-reference="fig:avg_rank_trend_34">7</a>, we discuss the value presence pattern in two ‘topic’ strata. First, in sub-figure <strong>C</strong>, we observe that Group3 values show overall higher variability than ‘artist playlist frequency’. It is notable that there are a few distinct topics in which Group3 values show a significant difference; the sixth, seventh and fourteenth topics, which correspond to the ‘under 2000 listeners/musical’, ‘folk/singer-songwriter’, and ‘Hip-Hop/rap’ topics when represented in primary topic terms. Specifically, we see that first two topics show a high presence of Group3 values, while the latter topics show the least presence of Group3 values. It suggests that the artists in these styles/genres were perceived on average to present clearly different sets of values through their lyrics, distinguished by the inclusion/exclusion of values such as BENEVOLENCE or UNIVERSALISM.</p>
<p>Finally, considering sub-figure <strong>D</strong>, we observe a similar pattern as ‘artist playlist frequency’ in <a href="#fig:avg_rank_trend_12" data-reference-type="ref" data-reference="fig:avg_rank_trend_12">6</a>, albeit with relatively more variability in Group3 values. Notably, the ‘rap/hip-hop’ lyrics topic shows the least presence of Group3 values, which aligns to the observation from previous sub-figure. The ‘sad/romantic1’ topic, on the other hand, shows the highest ranking of Group3 values. Another remarkable topic is ‘gospel/reggae’ topic, where HEDONISM value is least present, which semantically aligns well with the typical lyrical theme of those songs.</p>
</section>
<section id="limitations-and-future-work" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Limitations and Future Work</h1>
<p>In this work we attempt to ground-truth perceptions of ambiguous song lyrics for perceived human values. We adopt a validated questionnaire from the social sciences for this purpose, in addition to a purposeful, if conceptually ‘fuzzy’, stratified sampling strategy, and estimate the average number of ratings needed to estimate the average perception of values in a song. We acknowledge our current sample of 360 lyrics is small and may need expansion for more typical work, and that, while we had a representative population sample, not every member of the sample rated every song. We thus did gather diverse opinions, but cannot claim they fully represent the target population. In addition, the small sample of songs allowed for only limited observation of patterns that might emerge in larger samples with relation to our defined strata, and indefinite conclusions given the overall massive population of songs in existence. We also did not assess whether variations on the annotation instrument might result in substantial differences in the annotations we received <span class="citation" data-cites="kern2023annotation"><a href="#ref-kern2023annotation" role="doc-biblioref">[46]</a></span>, nor did we repeat our procedure <span class="citation" data-cites="inel2023collect"><a href="#ref-inel2023collect" role="doc-biblioref">[47]</a></span>. In addition, we acknowledge that participants from different groups will perceive and thus annotate corpora differently&nbsp;<span class="citation" data-cites="homan2022annotator prabhakaran2023framework"><a href="#ref-homan2022annotator" role="doc-biblioref">[48]</a>, <a href="#ref-prabhakaran2023framework" role="doc-biblioref">[49]</a></span>. Thus, we expect that lyrics may be especially sensitive to varying perceptions, which we did not explore in this work. Finally, we only provide a preliminary comparison to automated scoring methods, and did not leverage the most contemporary tools for this purpose (e.g.&nbsp;Large Language Models). All of these are rich and promising avenues for future work.</p>
<p>The most interesting avenues are potential relationships that could be revealed with more annotated songs, and eventual automated scoring methods. In particular, we see potential in understanding music consumption more broadly from patterns revealed in the dominant value hierarchies in specific music genres, popularity segments, lyrical topics, and even release year. And for understanding music consumption more narrowly, from patterns revealed in an individual’s music preferences, and the degree to which they conform with their own value hierarchy.</p>
</section>
<section id="conclusion" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Conclusion</h1>
<p>Song lyrics remain a widely and repeatedly consumed, yet ambiguous form of text, and thus a promising and challenging avenue for research into better understanding the people that consume them. We observe promising initial results for the annotation of personal values in songs, despite our limitations. MDS plots of aggregated ratings showed the beginnings of the expected structure of values, conforming more closely than might be expected from as little as 360 songs. We also observed high inter-rater reliability in the raw scores, suggesting a sufficiently reliable annotation procedure with 25 ratings. Thus, we see promise on our method for ground-truthing lyrics despite their ambiguity. A post-hoc procedure revealed that 15 ratings may be enough on average: we repeatedly subsampled 5, 10, 15 and 20 ratings for each value within each song, and calculated pearson correlations between subsample means and canonical means. From this, we see Pearson correlations to the canonical mean exceed 0.9 for all values from 15 subsampled ratings. Further lyric annotation may thus require fewer annotations per song than what was gathered in this work. In addition, we observe promising rank correlations between ranked rater scores and our automated methods, with over 75%&nbsp;of the rankings in our best performing model above a minimal threshold of .10. Despite inherent challenges in the task, our method shows initial promise, and multiple fruitful avenues for future work.</p>
</section>
<section id="ethics-statement" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> Ethics Statement</h1>
<p>Our study includes data gathered from people, and was approved by the Human Research Ethics board of our university. We follow Prolific’s guidelines on fair compensation to set our compensation rates. Survey design and data handling were pre-discussed with our institutional data management and research ethics advisors, we obtained formal data management plan and human research ethics approval. Participants gave informed consent before proceeding with the survey, which informed them of the intentions of use for their data, and that it could be withdrawn at any time.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-conrad2019extreme" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">F. Conrad, J. Corey, S. Goldstein, J. Ostrow, and M. Sadowsky, <span>“Extreme re-listening: Songs people love... And continue to love,”</span> <em>Psychology of Music</em>, vol. 47, no. 2, pp. 158–172, 2019.</div>
</div>
<div id="ref-demetriou2018vocals" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A. Demetriou, A. Jansson, A. Kumar, and R. M. Bittner, <span>“Vocals in music matter: The relevance of vocals in the minds of listeners.”</span> in <em>ISMIR</em>, 2018, pp. 514–520.</div>
</div>
<div id="ref-van2005world" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">R. W. Van Sickel, <span>“A world without citizenship: On (the absence of) politics and ideology in country music lyrics, 1960–2000,”</span> <em>Popular music and society</em>, vol. 28, no. 3, pp. 313–331, 2005.</div>
</div>
<div id="ref-north2020relationship" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">A. C. North, A. E. Krause, and D. Ritchie, <span>“The relationship between pop music and lyrics: A computerized content analysis of the united kingdom’s weekly top five singles, 1999–2013,”</span> <em>Psychology of Music</em>, p. 0305735619896409, 2020.</div>
</div>
<div id="ref-brand2019cultural" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">C. O. Brand, A. Acerbi, and A. Mesoudi, <span>“Cultural evolution of emotional expression in 50 years of song lyrics,”</span> <em>Evolutionary Human Sciences</em>, vol. 1, 2019.</div>
</div>
<div id="ref-howlin2020patients" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">C. Howlin and B. Rooney, <span>“Patients choose music with high energy, danceability, and lyrics in analgesic music listening interventions,”</span> <em>Psychology of Music</em>, p. 0305735620907155, 2020.</div>
</div>
<div id="ref-ali2006songs" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">S. O. Ali and Z. F. Peynircioğlu, <span>“Songs and emotions: Are lyrics and melodies equal partners?”</span> <em>Psychology of music</em>, vol. 34, no. 4, pp. 511–534, 2006.</div>
</div>
<div id="ref-brattico2011functional" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">E. Brattico <em>et al.</em>, <span>“A functional MRI study of happy and sad emotions in music with and without lyrics,”</span> <em>Frontiers in psychology</em>, vol. 2, p. 308, 2011.</div>
</div>
<div id="ref-kim2020butter" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">J. Kim, A. M. Demetriou, S. Manolios, M. S. Tavella, and C. C. Liem, <span>“Butter lyrics over hominy grit: Comparing audio and psychology-based text features in MIR tasks.”</span> in <em>ISMIR</em>, 2020, pp. 861–868.</div>
</div>
<div id="ref-preniqi2022more" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">V. Preniqi, K. Kalimeri, and C. Saitis, <span>“" more than words": Linking music preferences and moral values through lyrics,”</span> <em>arXiv preprint arXiv:2209.01169</em>, 2022.</div>
</div>
<div id="ref-manolios2019influence" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">S. Manolios, A. Hanjalic, and C. C. Liem, <span>“The influence of personal values on music taste: Towards value-based music recommendations,”</span> in <em>Proceedings of the 13th ACM conference on recommender systems</em>, 2019, pp. 501–505.</div>
</div>
<div id="ref-gardikiotis2012rock" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">A. Gardikiotis and A. Baltzis, <span>“<span>‘Rock music for myself and justice to the world!’</span>: Musical identity, values, and music preferences,”</span> <em>Psychology of Music</em>, vol. 40, no. 2, pp. 143–163, 2012.</div>
</div>
<div id="ref-swami2013metalheads" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">V. Swami <em>et al.</em>, <span>“Metalheads: The influence of personality and individual differences on preference for heavy metal.”</span> <em>Psychology of Aesthetics, Creativity, and the Arts</em>, vol. 7, no. 4, p. 377, 2013.</div>
</div>
<div id="ref-sandri2023don" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">M. Sandri, E. Leonardelli, S. Tonelli, and E. Ježek, <span>“Why don’t you do it right? Analysing annotators’ disagreement in subjective tasks,”</span> in <em>Proceedings of the 17th conference of the european chapter of the association for computational linguistics</em>, 2023, pp. 2420–2433.</div>
</div>
<div id="ref-sagiv2022personal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">L. Sagiv and S. H. Schwartz, <span>“Personal values across cultures,”</span> <em>Annual review of psychology</em>, vol. 73, pp. 517–546, 2022.</div>
</div>
<div id="ref-schwartz1987toward" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">S. H. Schwartz and W. Bilsky, <span>“Toward a universal psychological structure of human values.”</span> <em>Journal of personality and social psychology</em>, vol. 53, no. 3, p. 550, 1987.</div>
</div>
<div id="ref-schwartz2012overview" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">S. H. Schwartz, <span>“An overview of the schwartz theory of basic values,”</span> <em>Online readings in Psychology and Culture</em>, vol. 2, no. 1, p. 11, 2012.</div>
</div>
<div id="ref-rokeach1973nature" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">M. Rokeach, <em>The nature of human values.</em> Free press, 1973.</div>
</div>
<div id="ref-schwartz1992universals" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">S. H. Schwartz, <span>“Universals in the content and structure of values: Theoretical advances and empirical tests in 20 countries,”</span> in <em>Advances in experimental social psychology</em>, vol. 25, Elsevier, 1992, pp. 1–65.</div>
</div>
<div id="ref-boyd2017language" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">R. L. Boyd and J. W. Pennebaker, <span>“Language-based personality: A new approach to personality in a digital world,”</span> <em>Current opinion in behavioral sciences</em>, vol. 18, pp. 63–68, 2017.</div>
</div>
<div id="ref-pennebaker2015development" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">J. W. Pennebaker, R. L. Boyd, K. Jordan, and K. Blackburn, <span>“The development and psychometric properties of LIWC2015,”</span> 2015.</div>
</div>
<div id="ref-graham2009liberals" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">J. Graham, J. Haidt, and B. A. Nosek, <span>“Liberals and conservatives rely on different sets of moral foundations.”</span> <em>Journal of personality and social psychology</em>, vol. 96, no. 5, p. 1029, 2009.</div>
</div>
<div id="ref-holtrop2022exploring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">D. Holtrop, J. K. Oostrom, W. R. J. van Breda, A. Koutsoumpis, and R. E. de Vries, <span>“Exploring the application of a text-to-personality technique in job interviews,”</span> <em>European Journal of Work and Organizational Psychology</em>, vol. 31, no. 6, pp. 799–816, 2022.</div>
</div>
<div id="ref-ponizovskiy2020development" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">V. Ponizovskiy, M. Ardag, L. Grigoryan, R. Boyd, H. Dobewall, and P. Holtz, <span>“Development and validation of the personal values dictionary: A theory–driven tool for investigating references to basic human values in text,”</span> <em>European Journal of Personality</em>, vol. 34, no. 5, pp. 885–902, 2020.</div>
</div>
<div id="ref-maheshwari2017societal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">T. Maheshwari <em>et al.</em>, <span>“A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content,”</span> in <em>Proceedings of the 15th conference of the european chapter of the association for computational linguistics: Volume 1, long papers</em>, Association for Computational Linguistics, 2017, pp. 731–741.</div>
</div>
<div id="ref-kiesel2022identifying" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">J. Kiesel, M. Alshomary, N. Handke, X. Cai, H. Wachsmuth, and B. Stein, <span>“Identifying the human values behind arguments,”</span> in <em>Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, Association for Computational Linguistics, 2022, pp. 4459–4471.</div>
</div>
<div id="ref-groves2009survey" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">R. M. Groves, F. J. Fowler Jr, M. P. Couper, J. M. Lepkowski, E. Singer, and R. Tourangeau, <em>Survey methodology</em>, vol. 561. John Wiley &amp; Sons, 2009.</div>
</div>
<div id="ref-Liem2012MusicGap" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">C. C. S. Liem <em>et al.</em>, <span>“<span class="nocase">Music Information Technology and Professional Stakeholder Audiences: Mind the Adoption Gap</span>,”</span> in <em>Dagstuhl follow-ups</em>, vol. 3, Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2012. doi: <a href="https://doi.org/10.4230/DFU.VOL3.11041.227">10.4230/DFU.VOL3.11041.227</a>.</div>
</div>
<div id="ref-DBLP:conf/recsys/ChenLSZ18" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">C.-W. Chen, P. Lamere, M. Schedl, and H. Zamani, <span>“Recsys challenge 2018: Automatic music playlist continuation,”</span> in <em>Proceedings of the 12th <span>ACM</span> conference on recommender systems, RecSys 2018, vancouver, BC, canada, october 2-7, 2018</em>, S. Pera, M. D. Ekstrand, X. Amatriain, and J. O’Donovan, Eds., <span>ACM</span>, 2018, pp. 527–528. doi: <a href="https://doi.org/10.1145/3240323.3240342">10.1145/3240323.3240342</a>.</div>
</div>
<div id="ref-schindler2012facilitating" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">A. Schindler, R. Mayer, and A. Rauber, <span>“Facilitating comprehensive benchmarking experiments on the million song dataset.”</span> in <em>ISMIR</em>, International Society for Music Information Retrieval, 2012, pp. 469–474.</div>
</div>
<div id="ref-lindeman2005measuring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">M. Lindeman and M. Verkasalo, <span>“Measuring values with the short schwartz’s value survey,”</span> <em>Journal of personality assessment</em>, vol. 85, no. 2, pp. 170–178, 2005.</div>
</div>
<div id="ref-cabitza2020if" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">F. Cabitza, A. Campagner, and L. M. Sconfienza, <span>“As if sand were stone. New concepts and metrics to probe the ground on which to build trustable AI,”</span> <em>BMC Medical Informatics and Decision Making</em>, vol. 20, no. 1, pp. 1–21, 2020.</div>
</div>
<div id="ref-DeBruine_Jones_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">L. M. DeBruine and B. C. Jones, <span>“Determining the number of raters for reliable mean ratings.”</span> OSF, Aug. 2018. doi: <a href="https://doi.org/10.17605/OSF.IO/X7FUS">10.17605/OSF.IO/X7FUS</a>.</div>
</div>
<div id="ref-koo2016guideline" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">T. K. Koo and M. Y. Li, <span>“A guideline of selecting and reporting intraclass correlation coefficients for reliability research,”</span> <em>Journal of chiropractic medicine</em>, vol. 15, no. 2, pp. 155–163, 2016.</div>
</div>
<div id="ref-Psychcitation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">William Revelle, <em>Psych: Procedures for psychological, psychometric, and personality research</em>. Evanston, Illinois: Northwestern University, 2023. Available: <a href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</a></div>
</div>
<div id="ref-schwartz2001extending" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">S. H. Schwartz, G. Melech, A. Lehmann, S. Burgess, M. Harris, and V. Owens, <span>“Extending the cross-cultural validity of the theory of basic human values with a different method of measurement,”</span> <em>Journal of cross-cultural psychology</em>, vol. 32, no. 5, pp. 519–542, 2001.</div>
</div>
<div id="ref-davison2000multidimensional" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">M. L. Davison and S. G. Sireci, <span>“Multidimensional scaling,”</span> in <em>Handbook of applied multivariate statistics and mathematical modeling</em>, Elsevier, 2000, pp. 323–352.</div>
</div>
<div id="ref-Rcitation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">R Core Team, <em>R: A language and environment for statistical computing</em>. Vienna, Austria: R Foundation for Statistical Computing, 2022. Available: <a href="https://www.R-project.org/">https://www.R-project.org/</a></div>
</div>
<div id="ref-RRAcitation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline">R. Kolde, <em>RobustRankAggreg: Methods for robust rank aggregation</em>. 2022. Available: <a href="https://CRAN.R-project.org/package=RobustRankAggreg">https://CRAN.R-project.org/package=RobustRankAggreg</a></div>
</div>
<div id="ref-kolde2012robust" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">R. Kolde, S. Laur, P. Adler, and J. Vilo, <span>“Robust rank aggregation for gene list integration and meta-analysis,”</span> <em>Bioinformatics</em>, vol. 28, no. 4, pp. 573–580, 2012.</div>
</div>
<div id="ref-DBLP:conf/nips/MikolovSCCD13" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, <span>“Distributed representations of words and phrases and their compositionality,”</span> in <em>Advances in neural information processing systems 26: 27th annual conference on neural information processing systems 2013. Proceedings of a meeting held december 5-8, 2013, lake tahoe, nevada, united states</em>, C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013, pp. 3111–3119. Available: <a href="https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html</a></div>
</div>
<div id="ref-DBLP:conf/emnlp/PenningtonSM14" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">J. Pennington, R. Socher, and C. D. Manning, <span>“Glove: Global vectors for word representation,”</span> in <em>Proceedings of the 2014 conference on empirical methods in natural language processing, <span>EMNLP</span> 2014, october 25-29, 2014, doha, qatar, <span>A</span> meeting of SIGDAT, a special interest group of the <span>ACL</span></em>, A. Moschitti, B. Pang, and W. Daelemans, Eds., <span>ACL</span>, 2014, pp. 1532–1543. doi: <a href="https://doi.org/10.3115/V1/D14-1162">10.3115/V1/D14-1162</a>.</div>
</div>
<div id="ref-DBLP:conf/acl/FaruquiD14" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">M. Faruqui and C. Dyer, <span>“Community evaluation and exchange of word vectors at wordvectors.org,”</span> in <em>Proceedings of the 52nd annual meeting of the association for computational linguistics, <span>ACL</span> 2014, june 22-27, 2014, baltimore, MD, USA, system demonstrations</em>, The Association for Computer Linguistics, 2014, pp. 19–24. doi: <a href="https://doi.org/10.3115/V1/P14-5004">10.3115/V1/P14-5004</a>.</div>
</div>
<div id="ref-DBLP:conf/emnlp/ReimersG19" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">N. Reimers and I. Gurevych, <span>“Sentence-BERT: Sentence embeddings using siamese BERT-networks,”</span> in <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing, <span>EMNLP-IJCNLP</span> 2019, hong kong, china, november 3-7, 2019</em>, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds., Association for Computational Linguistics, 2019, pp. 3980–3990. doi: <a href="https://doi.org/10.18653/V1/D19-1410">10.18653/V1/D19-1410</a>.</div>
</div>
<div id="ref-richard2003one" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">F. D. Richard, C. F. Bond Jr, and J. J. Stokes-Zoota, <span>“One hundred years of social psychology quantitatively described,”</span> <em>Review of general psychology</em>, vol. 7, no. 4, pp. 331–363, 2003.</div>
</div>
<div id="ref-kern2023annotation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div class="csl-right-inline">C. Kern, S. Eckman, J. Beck, R. Chew, B. Ma, and F. Kreuter, <span>“Annotation sensitivity: Training data collection methods affect model performance,”</span> <em>arXiv preprint arXiv:2311.14212</em>, 2023.</div>
</div>
<div id="ref-inel2023collect" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div class="csl-right-inline">O. Inel, T. Draws, and L. Aroyo, <span>“Collect, measure, repeat: Reliability factors for responsible AI data collection,”</span> in <em>Proceedings of the AAAI conference on human computation and crowdsourcing</em>, 2023, pp. 51–64.</div>
</div>
<div id="ref-homan2022annotator" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div class="csl-right-inline">C. Homan, T. C. Weerasooriya, L. Aroyo, and C. Welty, <span>“Annotator response distributions as a sampling frame,”</span> in <em>Proceedings of the 1st workshop on perspectivist approaches to NLP@ LREC2022</em>, 2022, pp. 56–65.</div>
</div>
<div id="ref-prabhakaran2023framework" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div class="csl-right-inline">V. Prabhakaran <em>et al.</em>, <span>“A framework to assess (dis) agreement among diverse rater groups,”</span> <em>arXiv preprint arXiv:2311.05074</em>, 2023.</div>
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://www.musicbusinessworldwide.com/files/2022/12/f23d5bc086957241e6177f054507e67b.png" class="uri">https://www.musicbusinessworldwide.com/files/2022/12/f23d5bc086957241e6177f054507e67b.png</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://www.gwi.com/reports/music-streaming-around-the-world" class="uri">https://www.gwi.com/reports/music-streaming-around-the-world</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://en.wikipedia.org/wiki/List_of_instrumental_number_ones_on_the_UK_Singles_Chart" class="uri">https://en.wikipedia.org/wiki/List_of_instrumental_number_ones_on_the_UK_Singles_Chart</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://research.atspotify.com/2020/09/the-million-playlist-dataset-remastered/" class="uri">https://research.atspotify.com/2020/09/the-million-playlist-dataset-remastered/</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://www.musixmatch.com/" class="uri">https://www.musixmatch.com/</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Each member independently screened each lyric and the screening process overall was discussed at length.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>e.g., Spotify reports over 100 million songs in its catalogue<a href="https://newsroom.spotify.com/company-info/" class="uri">https://newsroom.spotify.com/company-info/</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Full code of our sampling procedure is at&nbsp;<a href="https://anonymous.4open.science/r/lyrics-value-estimators-CE33/1_stimulus_sampling/stratified_sampling.py" class="uri">https://anonymous.4open.science/r/lyrics-value-estimators-CE33/1_stimulus_sampling/stratified_sampling.py</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://prolific.co" class="uri">https://prolific.co</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a href="https://qualtrics.com" class="uri">https://qualtrics.com</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>It has shown correlations ranging from .45-.70 per value with longer more established procedures, test-retest reliability, as well as the typical values structure shown in Figure&nbsp;<a href="#fig:circle" data-reference-type="ref" data-reference="fig:circle">2</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>.7 is a commonly considered an acceptable level of reliability in the form of internal consistency<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>We assume that the adjusted exact p-value from RRA monotonically decreases as the rank position ascends (i.e., the lower the p-value is, the higher the ranking position is).<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../_chapters/ISMIR_2020/ISMIR_2020.html" class="pagination-link" aria-label="&quot;Butter Lyrics Over Hominy Grit&quot;: Comparing Audio and Psychology-Based Text Features in MIR Tasks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">“Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>