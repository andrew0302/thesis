[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ground Truthing is a Field",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee [1] for additional discussion of literate programming.\n\n\n\n\n[1] D. E. Knuth, “Literate programming,” Comput. J., vol. 27, no. 2, pp. 97–111, May 1984, doi: 10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "",
    "text": "4 Introduction\nThe Music Information Retrieval (MIR) community has historically focused on content-based understanding of music. The type of content-based analysis studied over time is typically driven by the data available to the task, or the interests of the specific researchers. An alternative motivator could be to study topics that are salient in the minds of listeners, especially with respect to listener’s musical preference. Specifically, understanding which attributes of music contribute the most to music preference, and their relative weight, could help guide research efforts. One attribute of music we would expect to be salient in the minds of listeners is the singing voice.\nPsychology research anticipates the importance of the human voice as a salient stimulus, and as a component of music in particular. The human ability to communicate exceeds that of any other species studied thus far, with both speech and singing being cultural universals reliant on vocal production. It is theorized that the advanced human ability to communicate, discriminate, and to experience emotional responses in vocalizations has allowed for the emergence of music [1]. Our emotions are often accompanied by involuntary changes in our physiology and nonverbal expressions, such as facial expressions and vocalizations [2]. Our reactions to the emotional content expressed in the vocals in music may have similar effects. As such, much psychological research has focused on the singing voice even more than speech, due to the precision required to execute and process musical vocalizations [3]. This makes musical vocals a well-anticipated candidate for study as a feature of music, as we would expect people to have a sophisticated ability to deliver, empathize with, and process vocal communications.\nWe would therefore expect that the vocals in music would be an especially salient component, if not the most salient. While a complete review is beyond the scope of this paper, some research is particularly worth noting. For example, it has been shown that both adults [4] and children [5] recall melodies more correctly when sung with the voice than when played with instruments. Hutchins and Moreno [3] review literature that shows relatively precise perception of pitch in the human voice, yet fewer noticeable pitch errors in the voice relative to musical instruments or synthesized voices [6]. Neuroscience studies show specific areas of the brain involved in processing human voices [7]. Although similar regions of the brain are involved in processing both music and voices, there is differential processing of the human voice relative to music [8]. As such, the human voice may be processed as a uniquely significant sound.\nHowever, while prior research suggests that vocals would be especially relevant to music preference, no study to our knowledge has assessed the importance of the voice in music, relative to other musical components. To address this gap, we test the hypothesis that the voice is as or more important than other musical components across implicit and explicit datasets, using traditional social science techniques, as well as data mining techniques. First, we mine data available from Spotify, including playlist titles, search data and artist biographies, to test whether terms related to vocals are prevalent. However, we show that the results of the data mining are inconclusive as to whether or not vocals are salient in the minds of listeners. Specifically, it is not clear whether the vocals can be disentangled from other factors in playlist titles and search queries, such as genre. For more conclusive results, we gather data from users explicitly. To this aim we conduct two online survey studies: the first gathered subjective data on the salient components of music directly from listener reports, which were separated into semantic categories using card-sorting. The second asked participants to rank the semantic categories from the first study in terms of importance to their musical preference. We conclude that two aspects related to the voice are especially salient, namely the voice itself, and the lyrics of the song. Furthermore, we highlight the importance of gathering explicit data to complement implicit techniques, in situations where factors may not be easily disentangled.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html#playlist-tags-and-search-queries",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html#playlist-tags-and-search-queries",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "5.1 Playlist Tags and Search Queries",
    "text": "5.1 Playlist Tags and Search Queries\nNon-common words or groups of words and emojis appearing in the titles of a large number of Spotify’s user-generated playlists were aggregated to create a list of the 1000 most frequently occurring tags. Each of these 1000 tags was assigned a category by a professional curator based on the tag itself and information from the tracks most frequently associated with the tag. The categories, determined by the curator, were Genre (e.g. “K-Pop”), Mood (e.g. “sad”), Activity (e.g. “gym”), Popularity (e.g. “Today’s hits”), Artist (e.g. “Justin Timberlake”), Era (e.g. “70’s”), Culture (e.g. “Latin”), Lyrics (e.g. “clean”), Rhythm (e.g. “groove”), Instrument (e.g. “guitar”), Tempo (e.g. “slow”), Voice (e.g. “female singers”), or Other (e.g. “favorites”, “Jenna”, “hi”). The percentage of playlists containing each of these tag categories is displayed in Figure 1, top.\nSurprisingly, we see that tags explicitly related to vocals are not at all common compared to other types of tags, with the most common tags being related to genre, mood, or activity. Playlist titles can be viewed as labels for groups of music, and this analysis suggests that people do not often label groups of music based on explicit characteristics of the vocals. However, specific vocal characteristics (as well as many other musical attributes) may be implicit in many of the other tag categories, particularly for genre, mood, and artist. As vocal delivery style and genre are closely related, emotions communicated by the voice and the mood of the collection of songs may be related, and as each artist has a unique voice, we conclude that the relative weight of vocals may not have been disentangled from other factors.\n\n\n\n\n\n(Top) Percentage of Spotify playlists containing one of the top 1000 tags corresponding to each category. (Middle) Percentage of descriptive search queries corresponding to each tag category, sampled from one day of search data. (Bottom) tf-idf for each term category in artist biographies compared with Wikipedia term frequencies.\n\n\nWe perform a similar analysis on descriptive terms from one day’s worth of Spotify search queries, and obtained similarly inconclusive results, shown in Figure 1, middle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html#artist-biographies",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html#artist-biographies",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "5.2 Artist Biographies",
    "text": "5.2 Artist Biographies\nFinally, we analyze descriptive terms that occur in 100,000 professionally authored artist biographies on Spotify. We use TF-IDF [10] to retrieve terms that are distinctive to music writers, by comparing the frequency of terms in artist biographies to the frequency of the same terms in Wikipedia. The 100 most distinctive terms, grouped into semantic categories, are displayed in Figure 1, bottom. While many terms are much more frequent in music text (e.g. “bassist”, “jazz”, “songwriter”), vocals specifically were not more frequently mentioned than other musical aspects. One can hypothesize that the TF-IDF method is insufficient for this particular task, due to vocals being commonly discussed outside the context of music, and thus a relatively more common word in Wikipedia.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html#conclusions",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html#conclusions",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nOur results thus far do not show support for our general hypothesis. It may be the case that the intuitive notion of the relevance of vocals to user preference is misleading. On the other hand, it may also be the case that the importance of vocals is implicit in this data, as certain vocal styles are indicative of genre or mood. As such, the overlap between the voice and a number of the tags and descriptors analyzed prevents us from disentangling the unique effect of the voice from other musical components.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html#survey-1-semantic-components-of-music",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html#survey-1-semantic-components-of-music",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "6.1 Survey 1: Semantic Components of Music",
    "text": "6.1 Survey 1: Semantic Components of Music\nThe aim of our first survey was to establish an unranked set of self-reported salient components of music. While our hypothesis was that the vocals would be prominent, it was crucial to avoid biasing respondents as the data collected were explicit. As such, our first survey asked participants what they notice when listening to music that might make them like or dislike a song. We deliberately did not specify anything further, such as the type of music, or that we were interested in components of music, nor were participants asked to listen to musical excerpts so as not to bias responses. As an exploratory measure, we then asked participants to describe what about vocals specifically might make them like or dislike a song after the previous open ended questions, so as not to bias responses. Responses to these two open-response questions were manually sorted into semantic categories by the researchers.\n\n6.1.1 Recruitment\nA random sample of 50,000 people was drawn from the database of Spotify’s Monthly Active Users (MUAs), divided approximately equally between the United States and Canada. 860 individuals responded to the survey, however 224 did not respond to any questions beyond the consent form, and 9 were removed for giving nonsensical responses. 626 individuals — 338 women (average age 33.6 years with a standard deviation of 16.1); 288 men (average age 30.6 years with a standard deviation of 15.5) — completed the survey in its entirety.\n\n\n6.1.2 Survey\nAn online consent form was first presented to respondents. We then asked:\n\nQ1: When you listen to music, what things about the music do you notice? Please list as many as you can think of here:\n\nThe respondents were shown a screen with open-response format fields to complete, in which they could complete up to seven fields. On the following screen, respondents were presented with a list of their responses in random order, and asked:\n\nQ2: Please rank how important the aspects you listed are to your musical preference, where 1 is the most important.\n\nThey were then asked the following two questions about the items they ranked from 1 to 3:\n\nQ3: (a) What about ____ would make you like a song? (b) What about ____ would make you dislike a song?\n\nLastly, to explore what aspects of vocals may be relevant, participants responded to the following:\n\nQ4: (Please ignore these questions if you’ve already mentioned the vocals, the voice, the singer/rapper etc.) (a) When would vocals make you like a song? (b) When would vocals make you dislike a song?\n\nThey were then given the opportunity to comment on the survey, and were shown a final debriefing screen.\n\n\n6.1.3 Semantic Categorization\nA number of partially completed surveys contained responses sufficiently complete for card sorting. 317 sufficient responses — 262 from the completed surveys as well as 55 sufficiently complete partial – were then card-sorted by a team of researchers. Card-sorting is a common technique used in social sciences and elsewhere to discover clusters of related concepts [11]. Traditionally, individuals are presented with physical paper “cards” that have terms and/or descriptions printed on them, printed pictures, or a group of objects. They are then asked to group items in a way that makes sense, given the research question. Here, we apply card-sorting to derive semantically meaningful groupings of musical components from the freely entered words and phrases that participants entered in each field.\nParticipant responses to Q1 (i.e. “When you listen to music, what things do you notice?”) were printed twice, once next to their response to Q3a (“What about _ would make you like a song?”), and again next to the response to Q3b (“What about _ would make you dislike a song?”). As such, researchers had respondents’ top 3 terms printed out twice, once next to the positive descriptive aspects of the term, and once next to the negative descriptive aspects. A term (e.g. “the lyrics”) and its descriptor (e.g. “when they have meaning”) comprised a card. Figure 2 shows examples of positive and negative cards that were used in card sorting.\n\n\n\n\n\nSurvey 1 sample answers for Q3. (Top) Card for an answer to Q3a. (Bottom) Card for an answer to Q3b.\n\n\nAs some responses were unclear (e.g. “the melody” was mentioned, but the descriptor clearly focused on the quality of the singer’s voice), the research team was instructed to look at both the term and its descriptor when determining its semantic category. The researchers then reviewed the cards a second time, and defined sub-categories where necessary.\n\n\n6.1.4 Results\nThe output of this study was two sets of semantic categories: broad semantic categories of music, and vocal-specific semantic categories. Statistical testing was not possible, given the intentionally imprecise nature of the responses. However, out of the 626 responses to the first question, 186 (29.7%) mentioned the vocals, the voice, or the singer, 348 (55.6%) mentioned the lyrics, or the words, and 101 (16.1%) mentioned both. While this is no indication of relative importance, it does demonstrate that the voice and the lyrics were salient musical components to our respondents.\nThe broad semantic categories determined by the researchers are presented in the left column of Table [tab:mus-attributes] (note that the other results in Table [tab:mus-attributes] are from Study 2). The category of Emotion/mood referred to the ability of a song to evoke emotion, whether the emotion was a match or a mismatch to the current or desired mood or current activity, whether the emotion was desirable or undesirable, and nostalgia. Voice included genre related terms (e.g. mumble rap, metal, auto-tune, speechiness/rapping), descriptions of how the voice is used (e.g. unique/novel, screaming, pitch/pitch range, presence or absence of effects, intensity/effort/power, emotionality, authenticity, whininess/nasality, melodic-ness), skill, the innate qualities of the voice, liking/disliking, and the mix/blend. The Lyrics category represented items that indicated whether or not lyrics were present, their intelligibility, the presence of profanity, how “well” crafted they were, the “message”, the meaning behind them or general lyrical content and how relatable they are. Beat/Rhythm referred to whether it was liked/disliked, whether it “fit” the song, danceability, and uniqueness. The Structure/complexity of songs included liking or disliking the hook or chorus, and the song length. Instrumentation referred to drums, bass, and guitar. Sound referred to audio quality and related concerns. Self-explanatory categories included Tempo/BPM, the mention of a Specific Artist, Genre, Harmony, Chords, Musicianship, Melody, and Popularity/Novelty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html#survey-2-component-ranking",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html#survey-2-component-ranking",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "6.2 Survey 2: Component Ranking",
    "text": "6.2 Survey 2: Component Ranking\nWhile the first study aimed at determining what attributes of music were salient in the minds of listeners, the aim of the second survey was to determine the relative importance of each of the components. Specifically, we explored whether the voice would be ranked highest among a list of musical attributes. To accomplish this, participants were asked to rank a list of attributes derived from the results of our first survey, thus allowing an assessment of whether or not vocals rank above other components.\n\n6.2.1 Recruitment\nA randomized sampling method was employed among the database of Spotify’s Monthly Active Users (MAUs) that had not opted-out of email correspondence. An email with a link to an online survey was sent to 50,000 potential respondents, approximately equally divided among the United States and Canada.\nA total of 531 respondents — 263 of which were women (average age 31.8 years, with a standard deviation of 16.5); 268 were men (average age 34.2 years, with a standard deviation of 14.8) — completed the survey in its entirety. 429 participants completed the first half of the survey (broad semantic categories), whereas 360 participants completed the second half (vocal semantic categories).\n\n\n6.2.2 Survey\nAn online consent form was first presented to respondents. The derived semantic categories were rephrased to be more easily understood (see Table [tab:mus-attributes], Description). Participants were presented with the new list of descriptions in random order, and asked to “Please click all the items below that would make you like or dislike a song.” They were then presented with a list of all the items they had clicked, also in random order, and asked to rank them.\n\n\n\n\n\nBroad Semantic Category\nDescription\nBorda score\n\\(p\\)-value\n\n\n\n\nEmotion/mood\nHow it makes you feel - the emotions/mood\n4641\n&lt;0.001\n\n\nVoice\nVoice/vocals\n3688\n&lt;0.001\n\n\nLyrics\nLyrics\n3656\n&lt;0.001\n\n\nBeat/rhythm\nBeat/rhythm\n3460\n&lt;0.001\n\n\nStructure/Complexity\nHow it’s composed, the hook, the structure\n2677\n1.000\n\n\nMusicianship\nSkill of the musicians, musicianship\n2583\n1.000\n\n\nMelody\nThe main melody\n2577\n1.000\n\n\nSound\nThe “sound”, or the recording quality\n2406\n1.000\n\n\nSpecific Artist\nThe specific artist\n2349\n1.000\n\n\nGenre\nThe specific genre\n2293\n1.000\n\n\nInstrumentation\nThe musical instruments (e.g. drums, bass, guitar)\n2084\n1.000\n\n\nTempo/BPM\nHow fast or slow the song is\n1828\n1.000\n\n\nHarmony\nHarmony\n1763\n1.000\n\n\nChords\nThe chords\n1086\n1.000\n\n\nPopularity/Novelty\nHow popular or unique it is\n777\n1.000\n\n\n\n\n\nAs a continuation of our exploratory study of vocal characteristics, a second list was then presented, comprised of terms derived from the vocal and lyrics semantic categories. For clarity, the terms were rephrased as they appear in Table [tab:voc-attributes].\n\n\n6.2.3 Analytic Strategy\nResponses were subjected to Borda counting [12] and Robust Rank Aggregation [13]. Borda counting is a simple procedure for aggregating votes by summing ranks. The Borda score \\(B_i\\) for an item \\(i\\) is computed as \\(B_i = \\sum_{p=0}^N \\left( |r_p| - r_{p, i} \\right)\\) where \\(N\\) is the number of participants, \\(r_{p,i}\\) is participant \\(p\\)’s rank of item \\(i\\), starting at zero, and \\(|r_p|\\) is the number of items ranked by \\(p\\). The Borda method does not naturally extend to partial lists[14] — we have chosen to award higher scores to preferred items in long lists.\nTo verify the statistical significance of our findings we supplement the Borda count with Robust Rank Aggregation (RRA), in which we compare our survey results to a null hypothesis. Each item receives a score based on its observed position, compared to an expected random ordering. Upper bounds to \\(p\\)-values are computed using Bonferroni correction, with values of 1.0 indicating null findings. In this work we used the implementation provided by the RobustRankAggreg package1.\n\n\n6.2.4 Results and Conclusion\nResults can be found in Tables [tab:mus-attributes] and [tab:voc-attributes], with categories ordered by descending Borda count. We are able to show statistical significance of both the most salient broad and vocal semantic categories. Importantly, our results show that the Vocals and Lyrics ranked second and third among the list of components (Borda scores and RRA agree on the order of the first four broad categories). This indicates that, relative to other musical components, respondents overall indicated the importance of the vocals and lyrics.\n\n\n\n\n\n\nVocal Semantic Categories\nBorda score\n\\(p\\)-value\n\n\n\n\nA\nSinging skill\n3423\n&lt;0.001\n\n\nB\nHow well the voice fits or matches the rest of the music\n3380\n&lt;0.001\n\n\nC\nLyrical skill / cleverness / wit\n3145\n&lt;0.001\n\n\nD\nThe meaning, or the “message” of the words\n3038\n0.048\n\n\nE\nAuthenticity / “realness”\n2884\n&lt;0.001\n\n\nF\nUniqueness\n2780\n&lt;0.001\n\n\nG\nIf the voice is emotional\n2771\n0.006\n\n\nH\nVoice strength / intensity / effort\n2721\n1.000\n\n\nI\nIf the voice sounds natural\n2480\n1.000\n\n\nJ\nBeing able to relate\n2256\n1.000\n\n\nK\nIf the voice is melodic\n2202\n1.000\n\n\nL\nWhether or not you can understand the lyrics\n2056\n1.000\n\n\nM\nIf it’s whiny or nasal\n1801\n1.000\n\n\nN\nWhether or not there’s screaming\n1771\n1.000\n\n\nO\nThe overall pitch, or the range of the pitch\n1400\n1.000\n\n\nP\nWhether or not there are lyrics\n1250\n1.000\n\n\nQ\nWhether it has production effects on it, like autotune\n1230\n1.000\n\n\nR\nProfanity, explicit lyrics\n1086\n1.000\n\n\nS\nWhether or not there is rapping\n909\n1.000",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2018/ISMIR_2018.html#footnotes",
    "href": "_chapters/ISMIR_2018/ISMIR_2018.html#footnotes",
    "title": "3  Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners",
    "section": "",
    "text": "cran.r-project.org/web/packages/RobustRankAggreg↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "",
    "text": "4.1 Introduction\n1\nPopular Western music very often contains lyrics. Social science research has shown informative relationships between popular songs and their lyrical content: e.g., country music lyrics rarely include political concepts [1], songs with more typical [2] and more negative [3] lyrics appear to be more successful, and the psychological content of song lyrics appears to correlate with cultural changes in psychological traits [4]. As for music consumption, lyrics have also been shown to be a salient component of music in the minds of listeners [5]. Furthermore, [6] showed that patients are more likely to choose music with lyrics when participating in music-based pain reduction interventions; [7] showed that lyrics enhance self reported emotional responses to music, although melody had an overall larger effect, and [8] showed a number of additional brain regions were active during the listening of sad music with lyrics, vs. sad music without lyrics.\nIn the Music Information Retrieval (MIR) field, some interest for lyrics and how they can be used to improve MIR tasks has been shown. Popular uses of lyrics for MIR tasks consider mood classification [9], [10], [11], [12], genre classification [13], [14] and topic detection for indexing and browsing [15], [16]. [17] also proposed a metric to assess the novelty of lyrics, and suggested that novelty can play a role in music preference.\nFrom these findings, one can conclude that lyrics are a rich data source. Although MIR interests have historically focused more on audio, lyrics information may fruitfully be leveraged for various MIR tasks. Still, there are many possible ways to extract information from lyrics text, and it is an open question what information extraction procedure will turn out most fruitful. To gain more insight into this, we present a study investigating several textual feature sets. In shaping these sets—acknowledging potential value of the topic for social science research—we are inspired by the way text analysis has been performed in the Psychology domain, and draw several of our extractors from prior work in that field. We will assess the performance of these textual feature sets on 3 common MIR tasks, and will statistically control for the effect of each chosen feature set, including an audio feature set for comparison. Our analysis will be performed on a large dataset from the online Musixmatch lyrics catalogue.\nIn the remainder of the paper, in Section2, we discuss relevant previous work on text information extraction in the Psychology literature. Section3 will subsequently explain our research design, after which Section4 discusses the feature sets we used. Section5 describes the data collection and pre-processing procedures, after which Section6 details the experimental design. Section7 justifies our chosen analytical strategy, followed by a presentation of results in Section8 and the conclusion in Section10.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:lingfeat",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:lingfeat",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "7.1 Linguistic Features",
    "text": "7.1 Linguistic Features\nAs baseline textual features for this study, we first extract several simple linguistic features:\n\nNumWords: the number of words included in the lyrics text.\nNumUniqueWords: the number of unique words in the lyrics text.\nNumStopWords: the number of stop words in the lyrics text2.\nNumRareWords the number of words that appeared in less than \\(5\\) lyrics.\nNumCommonWords the number of words extremely commonly used within a lyrics corpus. We set the threshold as the 30% percentile of the document frequency of words.\n\nAlong with the absolute number, we also compute the ratio over the total number of words for each lyrics text.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:topic",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:topic",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "7.2 Topic Modeling",
    "text": "7.2 Topic Modeling\nAs a more advanced feature extraction technique, we employ probabilistic Latent Semantic Analysis (pLSA) [24] for topic modeling. We treat each of the lyric texts as a document, and will take the found topic distribution for a given document as the document feature. We chose the number of topics \\(K=25\\), which maximizes validation log-perplexity. Taking advantage of the unsupervised learning setup, we use the total pool of songs to setup the training-validation-test split.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:LIWC",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:LIWC",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "7.3 LIWC",
    "text": "7.3 LIWC\nLinguistic Inquiry Word Count (LIWC) is a software package built on a lexicon that has been validated for text analysis in psychological studies [20]. It uses a curated lexicon, separated into 73 categories (e.g., the category ‘Social Processes’ includes references to family and friends). The software outputs the counts of words in a given text for each of the 73 categories. We employ the latest LIWC, released in 2015.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:inventory_scores",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:inventory_scores",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "7.4 Psychology Inventory Scores",
    "text": "7.4 Psychology Inventory Scores\nWe will consider two more feature sets, inspired by psychology inventory scores: a feature set focusing on personality and a feature set focusing on values. In both cases, we will use lexicons from literature. However, rather than performing a word count as was done in LIWC, we will use more contemporary NLP techniques based on word embeddings.\nContemporary personality theory is derived from lexical studies: it has been suggested that meaningful individual psychological differences between people are captured in the adjectives that describe people [25]. Although the number of meaningful clusters of adjectives (called Personality Dimensions) is under debate, the OCEAN or Big-Five model is often used. It is composed of 5 traits : Openness to Experience, Conscientiousness, Extroversion, Agreeableness and Neuroticism [25]. Our personality feature set consists of 2 word clusters per dimension, comprised of words representing positive and negative aspects for each personality dimension, derived from prior research [26].\nPersonal values are another important component of identity, though less studied. They are stable over time and represent who people want to be, targeting the most important things for them in life at the most abstract level. The traditional way to obtain people’s personal values is through questionnaires, but recent works focused on NLP techniques to extract them from text [27], [28], [29]. In our work, we used the value inventory and lexicon from [28].\nBoth for the personality and values feature sets, we will exploit the word2vec model [30] to approximate distances between lyrics and the various inventory categories in the feature sets. For this, we use the model pre-trained on the Google News dataset3. The average distance score \\(s_{d,c}\\) for each lyric text \\(d\\), and category \\(c\\) is computed by taking the average cosine distance between the words belonging to the lyrics and the categories, respectively: \\[s_{d, c} = \\frac{1}{|\\mathcal{W}_d||\\mathcal{W}_c|}\\sum_{n\\in\\mathcal{W}_d}\\sum_{m\\in\\mathcal{W}_c} \\frac{\\langle \\mathbf{v}_n, \\mathbf{v}_m \\rangle}{||\\mathbf{v}_n|| \\cdot ||\\mathbf{v}_m||}\\] where \\(\\mathcal{W}_d\\) and \\(\\mathcal{W}_c\\) represent the set of words belonging to the lyrics text \\(d\\) and the category \\(c\\). \\(v_n\\) and \\(v_m\\) denote the pre-trained word vectors corresponding to word \\(n\\) in the lyrics and word \\(m\\) in the category, respectively.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:mfcc",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:featset:mfcc",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "7.5 MFCC",
    "text": "7.5 MFCC\nFinally, we employ a set of audio features based on the Mel-Frequency Cepstral Coefficients (MFCC). We include these, such that the effect of the lyric-based text features can be compared to a commonly used feature set from the primary modality of interest in many MIR tasks. Specifically, we adopt the feature computation introduced in [31] with \\(40\\) mel bins.\n\n\nNumber of dimensions per feature set\n\n\nFeature Set\nDimensions\n\n\n\n\n\nAudio\n 240\n\n\n\nLIWC\n 73\n\n\n\nValues\n 49\n\n\n\nTopics\n 25\n\n\n\nPersonality\n 10\n\n\n\nLinguistic\n 9",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:setup:preproc",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:setup:preproc",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "8.1 Preprocessing",
    "text": "8.1 Preprocessing\nFor the given lyrics dataset, we consider the following preprocessing steps: the sentence strings are 1) tokenized and 2) lemmatized, followed by 3) stop-words filtering and 4) filtering extremely rare and extremely common words (see Section 4.1). Finally, we filter out non-English lyrics by a filtering process using the topic modeling. More precisely, we fit the topic model to detect whether the topics contain non-English words above a certain threshold. Songs that mostly load on non-English topics are removed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:exp:task",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:exp:task",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "9.1 Tasks & Systems",
    "text": "9.1 Tasks & Systems\nAs shown in Figure 1, to assess the lyrics feature set, we consider \\(3\\) popular MIR machine learning tasks; for each of these, we use \\(3\\) different commonly used types of systems, and a task-specific performance measure is considered, as detailed below.\n\n9.1.1 Music Genre Classification\nMusic Genre Classification (MGC) is a multi-class classification problem. Typically, a set of music genres is given as the classes, and music audio content or features are used as the observations. In this study, we examine \\(3\\) machine learning based systems: Gaussian Naive Bayes (GNV), Logistic Regression (LR) and the Multi-Layer Perceptron (MLP). For performance quantification, we opt for classification accuracy.\nFor this task, we use the data in the intersection between our lyrics database and the part of the MSD for which the music genre mapping introduced in [33] can be made. By choosing the intersection with the MSD, our audio features can be extracted from the MSD preview audio excerpts. Due to genre label availability, this leads to \\(67,719\\) songs being used in this task.\n\n\n9.1.2 Music Auto-Tagging\nMusic Auto-Tagging (MAT) is often formulated as a multi-label classification problem in which multiple positive labels may exist for one input music observation. We used the same set of systems as in the MGC task5. Again, we cross-match to the MSD, now also considering MSD’s LastFM social tags. Similarly to [31], we choose to focus on the \\(50\\) most frequent tags from the dataset. The Area Under Curve - Receiver Operating Characteristic (AUC-ROC) is used as the performance measure, which will be referred to as \\(AUC^{song}\\) for the rest of this paper6. Due to tagging label availability, \\(137,095\\) songs are used under this task.\n\n\n9.1.3 Music Recommendation\nFinally, Music Recommendation (MR) is considered for a user-related retrieval task. In particular, we consider a cold-start scenario, in which a batch of songs is newly introduced to the market, and required to be recommended to users. Due to the lack of previous interaction history, in such a scenario, a model will be maximally dependent on item attributes. As this is a substantially different type of task than the previous classification tasks, a different set of the systems common to the recommender systems field is used. Item Nearest Neighbor (INN) is a memory-based collaborative filtering method, which recommends the items closest to those that the user had consumed. We employ the feature vector introduced in Section 4 to compute the distance between entities using the cosine distance. We also use the Feature-augmented Matrix Factorization (FMF) [34] method, as well as the Factorization Machine [35] (FM). These models are more sophisticated collaborative recommenders, which also are capable of exploiting item attributes. The systems are developed and evaluated using the MSD-Echonest dataset [32]. Due to limits on available computational resources, we exploit a densified subset with \\(96,551\\) users and \\(66,850\\) songs from the initial song pool with the lyrics7. Finally, the binarized normalized Discounted Cumulative Gain (nDCG) is considered as performance measure, for the top-\\(100\\) songs recommended.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:exp:task:setup",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#sec:exp:task:setup",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "9.2 Task Simulation Setup",
    "text": "9.2 Task Simulation Setup\nAll MIR tasks above are machine learning tasks, but the systems and data we choose to use for them did not yet exist in a real-life system. Therefore, we ran the machine learning procedures to initiate them. For this, for each task, we randomly split the available song data into training/validation/test subsets by a ratio of \\(8:1:1\\). Each model is trained using the training set and evaluated on the validation set to tune the hyper-parameters. Once the optimal hyper-parameters are found, final performance is measured on the the test set.\nFor MLP and FMF, which have more than one hyper-parameter, automatic hyper-parameter tuning is conducted through a Bayesian approach, using the Gaussian Process89. Every search process iterates through \\(50\\) training-validation procedures to reach the optimal point. For the MGC and MAT tasks, the hyper-parameters are searched at every trial, while in the MR task, the search process runs only once and is used for all the other trials.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2020/ISMIR_2020.html#footnotes",
    "href": "_chapters/ISMIR_2020/ISMIR_2020.html#footnotes",
    "title": "4  “Butter Lyrics Over Hominy Grit”: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "section": "",
    "text": "Quoted words are lyrics written by Clifford Smith, from the song “The What”, by the Notorious B.I.G. featuring Methodman, on the album “Ready to Die”, released in 1994.↩︎\nAs we will focus on English lyrics in this study, we used the English stop words corpus from the Natural Language Toolkit (NLTK) [23]↩︎\nhttps://code.google.com/archive/p/word2vec/↩︎\nhttp://millionsongdataset.com/musixmatch/↩︎\nWe employ a one-vs-rest strategy for the LR and GNV, which transforms a multi-label classification problem to multiple binary classification problems.↩︎\nWe employed song-wise aggregation for this study↩︎\nWe initially matched the original Echonest dataset to our initial song pool and \\(30\\%\\) of randomly sampled users. Consequentially, we apply a filter, such that users who interacted with more than \\(5\\) songs remain, and vice versa for songs.↩︎\nWe use the implementation from the scikit-optimize package.↩︎\nWe do not search the hyper-parameters for FM and use a manually tuned setup, mostly due to the computational complexity required for this specific model.↩︎\nAnalyses were conducted on two servers running R 3.6.3. and 3.4.4.↩︎\nhttps://github.com/mmc-tudelft/lyricpsych-ISMIR20↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2024/ISMIR_2024.html",
    "href": "_chapters/ISMIR_2024/ISMIR_2024.html",
    "title": "5  Towards Automated Personal Value Estimation in Song Lyrics",
    "section": "",
    "text": "6 Introduction\nPopular music in Western countries almost always contains lyrics, making song lyrics a widely, repeatedly consumed [1] form of text. Over 616 million people subscribe to streaming services worldwide1, many of whom stream more than an hour of music every day2. Lyrics have been shown to be a salient component of music [2], and out of over 1400 number-1 singles in the UK charts, only 30 were instrumental3. The two representative US population samples that were our annotators indicate a median 90% of songs in their libraries contain lyrics (Figure 1).\nIt is thus not surprising that informative relationships between popular songs and their lyrical content have been shown: e.g., country music lyrics rarely include political concepts [3], and songs with more typical [4] and more negative [5] lyrics appear to be more successful. [6] showed that patients are more likely to choose music with lyrics when participating in music-based pain reduction interventions, although melody had an overall larger effect [7] showed that lyrics enhance self reported emotional responses to music, and [8] showed a number of additional brain regions were active during the listening of sad music with lyrics, vs. sad music without lyrics. In fields closer to MIR, [9] show that estimating psychological concepts from lyrics showed a small benefit in a number of MIR tasks, and [10] showed a correlation between moral principles estimated from song lyrics and music preferences.\nA connection between music lyrics and music preferences anticipated by theory involves the personal values perceived in the lyrics by listeners. Prior work has shown correlations between an individual’s values, and the music they listen to [10], [11], [12], [13], suggesting that we seek music in line with our principles. Yet we have not seen an attempt to measure perceived personal values expressed in the lyrics themselves via human annotation or automated methods.\nIn this work we take a first step towards the automated estimating the values perceived in song lyrics. As artistic and expressive language, lyrics are ambiguous text: they contain different forms of analogy and wordplay [14]. Thus we take a perspectivist approach to the annotations: because we expect that perceptions will vary substantially more than in other annotation tasks, we aim to represent the general perceptions of only one population. We account for the subjectivity by gathering a large number of ratings (median 27) per song from a targeted population sample (U.S.), of 360 carefully sampled song lyrics, using a psychometric questionnaire that we adjust for this purpose. We treat values in line with theory: as ranked lists, using Robust Ranking Aggregation (RRA) to arrive at our ‘ground truth’. We then gather estimates from word embedding models, by measuring semantic similarity between the lyrics and a validated dictionary. We show that ranked lists from estimates correlate moderately with annotation aggregates. We then discuss the implications of our results, the limitations of this project, and anticipated future work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Towards Automated Personal Value Estimation in Song Lyrics</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2024/ISMIR_2024.html#sec:primarylyricdata:fuzzysampling",
    "href": "_chapters/ISMIR_2024/ISMIR_2024.html#sec:primarylyricdata:fuzzysampling",
    "title": "5  Towards Automated Personal Value Estimation in Song Lyrics",
    "section": "8.1 Fuzzy Stratified Sampling",
    "text": "8.1 Fuzzy Stratified Sampling\nAn initial challenge is determining how to represent a corpus. In our case, the population of songs is known to be very large7. An ideal scenario would be one in which we aim for a known number of songs, randomly sampled from within clearly defined strata, i.e. relevant subgroups, also known as stratified random sampling [27]. However, for music, we do not know how many songs we would need to sample in order to reach saturation, what the relevant strata to randomly sample within should be, and how to measure relevant parameters from each stratum.\nSome measurable strata that affect the use of language in song lyrics are clear: e.g., the year of release, which may reflect different events or time-specific colloquial slang. Others are less clear: e.g., there is no single metric of popularity for music, although it can be estimated from various sources such as hit charts. Some may be very subjective, such as genre, for which there may be some overlap of human labelling, but no clear taxonomy exists in the eyes of musicological domain experts [28].\nBased upon these considerations, we aimed for a stratified random sampling procedure, based on strata that we acknowledge to be justifiable given our purpose, yet in some cases conceptually ‘fuzzy’: (1) release date; (2) popularity, operationalized as artist playlist frequency from the MPD [29]; (3) genre, estimated from topic modeling on Million Song Dataset artist tags [30]; (4) lyric topic, through a bag-of-words representation of the lyrics data. Popularity and Release date were divided into equally spaced bins; e.g. we divided release year into decades (60s, 70s, 80s, and so on), and genre and lyric topic were divided into categories.\nRelease date was quantized into 14 bins in 10-year increments from 1890-2030. Popularity was exponentially distributed, and thus manually binned, to make the quantiles per each of the 7 bins as similar as possible. Thus, the first bin contained the lowest 40% of the population in terms of popularity, while the 7th bin contained the highest 9%. Topic modelling was applied on a bag-of-word representation of the lyrics data and artist-tag data to yield \\(25\\) estimated genres and \\(9\\) lyrics topic strata, respectively.\nWe observed a skewness of data concentration with regard to several of our strata, e.g., songs that are recent and widely popular are most likely be drawn. To correct for this and thus get a more representative sample of an overall song catalogue, we oversample from less populated bins. For this, we use the maximum-a-posteriori (MAP) estimate of the categorical distribution of each stratum. The oversampling is controlled by concentration parameter \\(a\\) of the symmetric Dirichlet distribution. We heuristically set this parameter such that songs in underpopulated bins still will make up up 5-10 % of our overall pool8. Through this method, we subsampled our initial 2200 songs lyrics.\n\n\nMDS plots derived from the correlation plot reported in , and our participant responses as confidence-weighted means",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Towards Automated Personal Value Estimation in Song Lyrics</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2024/ISMIR_2024.html#reliability-agreement-and-initial-validation",
    "href": "_chapters/ISMIR_2024/ISMIR_2024.html#reliability-agreement-and-initial-validation",
    "title": "5  Towards Automated Personal Value Estimation in Song Lyrics",
    "section": "9.1 Reliability, Agreement and Initial Validation",
    "text": "9.1 Reliability, Agreement and Initial Validation\nThe rater reliability was estimated via intra-class correlation for each personal value, (type 2k: see [34]) using the ‘psych’ package in R [35], all of which exceeded .9 (excellent reliability). As an initial validation, we compare data simulated from values in the upper triangle of a correlation matrix reported in [36] to those derived from our study. To aggregate our participants rankings for this purpose, we compute confidence-weighted means inspired by [32]: we estimate confidence-weights by dividing participant’s self-reported confidence of a given rating by the highest possible response (100), and then compute aggregated means weighted by these. For both the simulated data and confidence-weighted mean scores, we generate a multi-dimensional scaling plot (MDS) [37] for visual comparison, which has previously been used as method to assess measurements conform to theory [24], [31]. Note: the interpretation is to observe whether each of the values appears next to expected neighboring values, and not each value’s orientation. From these plots (Figure 3), in as little as our 360 annotated lyrics, we surprisingly see similar clusters and relative positioning relations emerging as those obtained from a formal cross-cultural study.\nWe coerced the annotated scores to ranked lists of values, such that the highest scoring value was at the top. We derived ranked lists per participant per song, and then used Robust Ranking Aggregation (RRA) to extract a single ranked list per song. Aggregation was conducted using R version 4.2.2.[38], and the RobustRankAggreg package [39]. Briefly, RRA produces a ranked list by comparing the probability of the observed ranking of items to rankings from a uniform distribution. Essentially, scores are determined by comparing the height of an item on a set of lists to where it would appear if its rank were randomly distributed across lists. These scores are then subjected to statistical tests, where the resulting p value is Bonferronni corrected by the number of input lists[40]. Thus, when an item appears in different positions on a list, the resulting p value is high, as its position appears randomly distributed.\nAs lyrics are ambiguous, we expect that some songs’ values are completely subjective. We operationalize these as randomly distributed rankings for all personal values for completely subjective songs, i.e. p values above .05 for all 10 items on the ranked list. Results from the RRA show 62 songs with p values above .05 for all 10 values, and 96 songs with only 1 value ranked. At most, 5 values were ranked, which occurred for 35 songs. Thus, we confirm that although there was correspondence in the scores that participants assigned per value per song, ranked lists did not always agree.\n\n\nRank correlations between NLP systems / word counts and Robust Ranking Aggregation lists, by normalization scheme.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Towards Automated Personal Value Estimation in Song Lyrics</span>"
    ]
  },
  {
    "objectID": "_chapters/ISMIR_2024/ISMIR_2024.html#footnotes",
    "href": "_chapters/ISMIR_2024/ISMIR_2024.html#footnotes",
    "title": "5  Towards Automated Personal Value Estimation in Song Lyrics",
    "section": "",
    "text": "https://www.musicbusinessworldwide.com/files/2022/12/f23d5bc086957241e6177f054507e67b.png↩︎\nhttps://www.gwi.com/reports/music-streaming-around-the-world↩︎\nhttps://en.wikipedia.org/wiki/List_of_instrumental_number_ones_on_the_UK_Singles_Chart↩︎\nhttps://research.atspotify.com/2020/09/the-million-playlist-dataset-remastered/↩︎\nhttps://www.musixmatch.com/↩︎\nEach member independently screened each lyric and the screening process overall was discussed at length.↩︎\ne.g., Spotify reports over 100 million songs in its cataloguehttps://newsroom.spotify.com/company-info/↩︎\nFull code of our sampling procedure is at https://anonymous.4open.science/r/lyrics-value-estimators-CE33/1_stimulus_sampling/stratified_sampling.py↩︎\nhttps://prolific.co↩︎\nhttps://qualtrics.com↩︎\nIt has shown correlations ranging from .45-.70 per value with longer more established procedures, test-retest reliability, as well as the typical values structure shown in Figure 2↩︎\n.7 is a commonly considered an acceptable level of reliability in the form of internal consistency↩︎\nWe assume that the adjusted exact p-value from RRA monotonically decreases as the rank position ascends (i.e., the lower the p-value is, the higher the ranking position is).↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Towards Automated Personal Value Estimation in Song Lyrics</span>"
    ]
  }
]