---
title: "Towards Automatic Estimation of Personal Values in Song Lyrics"
format: html
bibliography: ISMIR2024_template.bib
---


# Introduction

Popular music in Western countries almost always contains lyrics, making
song lyrics a widely, repeatedly consumed (Conrad et al. 2019) form of
text. Over 616 million people subscribe to streaming services
worldwide[^1], many of whom stream more than an hour of music every
day[^2]. Lyrics have been shown to be a salient component of music
(Demetriou et al. 2018), and out of over 1400 number-1 singles in the UK
charts, only 30 were instrumental[^3]. The two representative US
population samples that were our annotators indicate a median 90% of
songs in their libraries contain lyrics
(Figure <a href="#fig:lyrics_percentages" data-reference-type="ref"
data-reference="fig:lyrics_percentages">1</a>).

<figure id="fig:lyrics_percentages">

<figcaption>Distribution of self-reported percentage of music library
containing lyrics from two representative US samples, n=505 and n=600
respectively.</figcaption>
</figure>

It is thus not surprising that informative relationships between popular
songs and their lyrical content have been shown: e.g., country music
lyrics rarely include political concepts (Van Sickel 2005), and songs
with more typical (North, Krause, and Ritchie 2020) and more negative
(Brand, Acerbi, and Mesoudi 2019) lyrics appear to be more successful.
(Howlin and Rooney 2020) showed that patients are more likely to choose
music with lyrics when participating in music-based pain reduction
interventions, although melody had an overall larger effect (Ali and
Peynircioğlu 2006) showed that lyrics enhance self reported emotional
responses to music, and (Brattico et al. 2011) showed a number of
additional brain regions were active during the listening of sad music
with lyrics, vs. sad music without lyrics. In fields closer to MIR, (Kim
et al. 2020) show that estimating psychological concepts from lyrics
showed a small benefit in a number of MIR tasks, and (Preniqi, Kalimeri,
and Saitis 2022) showed a correlation between moral principles estimated
from song lyrics and music preferences.

A connection between music lyrics and music preferences anticipated by
theory involves the personal values perceived in the lyrics by
listeners. Prior work has shown correlations between an individual’s
values, and the music they listen to (Manolios, Hanjalic, and Liem 2019;
Gardikiotis and Baltzis 2012; Swami et al. 2013; Preniqi, Kalimeri, and
Saitis 2022), suggesting that we seek music in line with our principles.
Yet we have not seen an attempt to measure perceived personal values
expressed in the lyrics themselves via human annotation or automated
methods.

In this work we take a first step towards the automated estimating the
values perceived in song lyrics. As artistic and expressive language,
lyrics are ambiguous text: they contain different forms of analogy and
wordplay (Sandri et al. 2023). Thus we take a perspectivist approach to
the annotations: because we expect that perceptions will vary
substantially more than in other annotation tasks, we aim to represent
the general perceptions of only one population. We account for the
subjectivity by gathering a large number of ratings (median 27) per song
from a targeted population sample (U.S.), of 360 carefully sampled song
lyrics, using a psychometric questionnaire that we adjust for this
purpose. We treat values in line with theory: as ranked lists, using
Robust Ranking Aggregation (RRA) to arrive at our ’ground truth’. We
then gather estimates from word embedding models, by measuring semantic
similarity between the lyrics and a validated dictionary. We show that
ranked lists from estimates correlate moderately with annotation
aggregates. We then discuss the implications of our results, the
limitations of this project, and anticipated future work.

# Personal Values

The modern study of human values spans over 500 samples in nearly 100
countries over the past 30 years, and has shown a relatively stable
structure (Sagiv and Schwartz 2022), as illustrated in
Figure <a href="#fig:circle" data-reference-type="ref"
data-reference="fig:circle">2</a>. Personal values are a component of
personality, defined as the hierarchy of principles that guide a
person’s thoughts, behaviors, and the way they evaluate events (Schwartz
and Bilsky 1987; Schwartz 2012). Basic human values can be used to
describe people or groups: social science theory suggests that each
person uses a hierarchical list of values as life-guiding principles
(Rokeach 1973), such that we prioritize some values over others as we
make decisions. Schwartz’s theory is the most widely used in social and
cultural psychology, and has shown correlations with important
behaviors, ranging from political affiliation to personal preferences
(Sagiv and Schwartz 2022).

We communicate our values in order to gain cooperation and coordinate
our efforts, according to Schwartz (Schwartz 1992). Thus our values will
manifest in the words that we use (Boyd and Pennebaker 2017). Although
personal values are traditionally measured by having individual people
complete validated psychological questionnaires, it has been argued that
values may be clearly expressed in the speech and text that we produce
(Boyd and Pennebaker 2017).

<figure id="fig:circle">

<figcaption>Visualization of the Schwartz 10-value inventory from <span
class="citation" data-cites="schwartz1992universals">(Schwartz
1992)</span> used in this paper, such that more abstract values of
Conservation, vs. Openness to Change, and Self-transcendence vs.
Self-enhancement form 4 higher-order abstract values. Illustration
adapted from <span class="citation" data-cites="maio2010mental">(Maio
2010)</span>. </figcaption>
</figure>

A common approach to measuring psychological aspects in text is to
validate dictionaries: curated sets of words, with subsets aimed at
measuring each component of the psychological aspect in question
(Pennebaker et al. 2015; Graham, Haidt, and Nosek 2009; Holtrop et al.
2022; Ponizovskiy et al. 2020). Some work estimating the values of the
authors of text has been conducted on individuals who have written
personal essays and social media posts e.g. (Maheshwari et al. 2017;
Ponizovskiy et al. 2020), and in arguments abstracted from various forms
of public facing text (Kiesel et al. 2022). However, we have not seen
work aimed at measuring values *perceived* in text, measuring them along
a scale as in prior work (Schwartz 1992), or ultimately treating them as
a hierarchical list in line with theory (Rokeach 1973).

# Primary Lyrics Data

We aim to collect a sample of lyric data where the lyrics are as
accurate as possible, and our sample is as representative as possible.
We sampled from the population of songs in the Million Playlist Dataset
(MPD)[^4] as it is large and recent compared to other similar datasets.
The lyrics themselves were obtained through the API of
[Musixmatch](https://www.musixmatch.com/)[^5], a lyrics and music
language platform. Musixmatch lyrics are crowdsourced by users who add,
correct, sync, and translate them. Musixmatch then engages in several
steps to verify quality of content, including spam detection,
formatting, spelling and translation checking, as well as manual
verification by over 2000 community curators, and a local team of
Musixmatch editors. Via their API, Musixmatch provided us with an
estimated first 30% of the lyrics of each song.

Using the ‘fuzzy’ stratified sampling method described below, we sampled
2200 songs. Three members of the research team manually screened
approximately 600 of the 2200 songs for inclusion. Each set of lyrics
was confirmed to be a match to the actual song, and for suitability[^6].
Lyrics were unsuitable if they were: 1) not English, 2) completely
onomatopoetic, 3) repetitions of single words or phrases, 4) too few
words to estimate values present or, 5) were not a match to the
meta-data of the song, e.g. artist title, song name. This resulted in
380 songs, 20 of which were used in a pilot study to determine the
number of ratings to gather per song, and 360 were used for annotation.

## Fuzzy Stratified Sampling

An initial challenge is determining how to represent a corpus. In our
case, the population of songs is known to be very large[^7]. An ideal
scenario would be one in which we aim for a known number of songs,
randomly sampled from within clearly defined strata, i.e. relevant
subgroups, also known as *stratified random sampling* (Groves et al.
2009). However, for music, we do not know how many songs we would need
to sample in order to reach saturation, what the relevant strata to
randomly sample within should be, and how to measure relevant parameters
from each stratum.

Some measurable strata that affect the use of language in song lyrics
are clear: e.g., the year of release, which may reflect different events
or time-specific colloquial slang. Others are less clear: e.g., there is
no single metric of popularity for music, although it can be estimated
from various sources such as hit charts. Some may be very subjective,
such as genre, for which there may be some overlap of human labelling,
but no clear taxonomy exists in the eyes of musicological domain
experts (Liem et al. 2012).

Based upon these considerations, we aimed for a stratified random
sampling procedure, based on strata that we acknowledge to be
justifiable given our purpose, yet in some cases conceptually ‘fuzzy’:
(1) release date; (2) popularity, operationalized as artist playlist
frequency from the MPD (Chen et al. 2018); (3) genre, estimated from
topic modeling on Million Song Dataset artist tags (Schindler, Mayer,
and Rauber 2012); (4) lyric topic, through a bag-of-words representation
of the lyrics data. Popularity and Release date were divided into
equally spaced bins; e.g. we divided release year into decades (60s,
70s, 80s, and so on), and genre and lyric topic were divided into
categories.

Release date was quantized into 14 bins in 10-year increments from
1890-2030. Popularity was exponentially distributed, and thus manually
binned, to make the quantiles per each of the 7 bins as similar as
possible. Thus, the first bin contained the lowest 40% of the population
in terms of popularity, while the 7th bin contained the highest 9%.
Topic modelling was applied on a bag-of-word representation of the
lyrics data and artist-tag data to yield $`25`$ estimated genres and
$`9`$ lyrics topic strata, respectively.

We observed a skewness of data concentration with regard to several of
our strata, e.g., songs that are recent and widely popular are most
likely be drawn. To correct for this and thus get a more representative
sample of an overall song catalogue, we oversample from less populated
bins. For this, we use the maximum-a-posteriori (MAP) estimate of the
categorical distribution of each stratum. The oversampling is controlled
by concentration parameter $`a`$ of the symmetric Dirichlet
distribution. We heuristically set this parameter such that songs in
underpopulated bins still will make up up 5-10 % of our overall
pool[^8]. Through this method, we subsampled our initial 2200 songs
lyrics.

<figure id="fig:mds_plots">

<figcaption>MDS plots derived from the correlation plot reported in
<span class="citation" data-cites="schwartz2001extending">(Schwartz et
al. 2001)</span>, and our participant responses as confidence-weighted
means</figcaption>
</figure>

# Ground-Truthing Procedure

We chose to obtain our annotations from samples of the US population,
representative in terms of self-reported sex, ethnicity and age, through
the Prolific[^9] platform. Annotator pools comprised of two samples, the
first n=505 wave participated in a pilot study to estimate the number of
ratings per song needed on average, and the second n=600 wave comprised
our main data collection. Participants completed the survey on the
Qualtrics [^10] platform.

We clearly differentiate between the Author and the Speaker of lyrics by
explaining to participants that the Author of song lyrics may write from
the perspective of someone or something else (the Speaker). 17 randomly
selected sets of lyrics were then shown to each participant along with
instructions to annotate each with the values of the Speaker. We adapted
the 10-item questionnaire used in (Lindeman and Verkasalo 2005) for the
value annotations, as it is the shortest questionnaire for assessing
personal values whose validity and reliability have been assessed[^11].
As in (Lindeman and Verkasalo 2005), each questionnaire item is a
specific value along with additional descriptive words e.g. POWER
(social power, authority, wealth). We adjusted it by asking participants
to indicate the values of the Speaker of the lyrics, and by having them
indicate on a bar with -100 (opposed to their principles) on one end,
and +100 (of supreme importance) on the other end instead of a likert
scale. In addition, we asked participants to indicate how confident they
were in their ratings, on a scale of 0 (not at all confident) to 100
(extremely confident), inspired by work that has shown that
self-reported confidence in ratings can be used to estimate the accuracy
of individual ratings (Cabitza, Campagner, and Sconfienza 2020).

We used a procedure similar to (DeBruine and Jones 2018) in order to
determine the number of raters. Specifically, we recruited a
representative 500+ participant sample of the US using the Prolific
platform, who completed our survey for 20 songs. We then computed
canonical mean ratings of each of the 10 values per song, and
inter-rater reliability using Cronbach’s Alpha. We then estimated
Cronbach’s alpha for a range of subsample sizes (5 to 50 participants in
increments of 5), for each of the 10 values. We repeated this procedure
10 times per increment, separately for each of the 10 values, and
examined the distribution of Cronbach’s Alpha. We specifically looked
for the sample size with which Alpha exceeded .7 [^12]. We arrived at a
conservative estimate of 25 ratings per set of lyrics, with songs
receiving a median 27 ratings (range 22-30).

## Reliability, Agreement and Initial Validation

The rater reliability was estimated via intra-class correlation for each
personal value, (type 2k: see (Koo and Li 2016)) using the ‘`psych`’
package in R (William Revelle 2023), all of which exceeded .9 (excellent
reliability). As an initial validation, we compare data simulated from
values in the upper triangle of a correlation matrix reported
in (Schwartz et al. 2001) to those derived from our study. To aggregate
our participants rankings for this purpose, we compute
confidence-weighted means inspired by (Cabitza, Campagner, and
Sconfienza 2020): we estimate confidence-weights by dividing
participant’s self-reported confidence of a given rating by the highest
possible response (100), and then compute aggregated means weighted by
these. For both the simulated data and confidence-weighted mean scores,
we generate a multi-dimensional scaling plot (MDS) (Davison and Sireci
2000) for visual comparison, which has previously been used as method to
assess measurements conform to theory (Ponizovskiy et al. 2020; Lindeman
and Verkasalo 2005). Note: the interpretation is to observe whether each
of the values appears next to expected neighboring values, and not each
value’s orientation. From these plots
(Figure <a href="#fig:mds_plots" data-reference-type="ref"
data-reference="fig:mds_plots">3</a>), in as little as our 360 annotated
lyrics, we surprisingly see similar clusters and relative positioning
relations emerging as those obtained from a formal cross-cultural study.

We coerced the annotated scores to ranked lists of values, such that the
highest scoring value was at the top. We derived ranked lists per
participant per song, and then used Robust Ranking Aggregation (RRA) to
extract a single ranked list per song. Aggregation was conducted using R
version 4.2.2.(R Core Team 2022), and the `RobustRankAggreg` package
(Kolde 2022). Briefly, RRA produces a ranked list by comparing the
probability of the observed ranking of items to rankings from a uniform
distribution. Essentially, scores are determined by comparing the height
of an item on a set of lists to where it would appear if its rank were
randomly distributed across lists. These scores are then subjected to
statistical tests, where the resulting *p* value is Bonferronni
corrected by the number of input lists(Kolde et al. 2012). Thus, when an
item appears in different positions on a list, the resulting p value is
high, as its position appears randomly distributed.

As lyrics are ambiguous, we expect that some songs’ values are
completely subjective. We operationalize these as randomly distributed
rankings for all personal values for completely subjective songs, i.e.
*p* values above .05 for all 10 items on the ranked list. Results from
the RRA show 62 songs with *p* values above .05 for all 10 values, and
96 songs with only 1 value ranked. At most, 5 values were ranked, which
occurred for 35 songs. Thus, we confirm that although there was
correspondence in the scores that participants assigned per value per
song, ranked lists did not always agree.

<figure id="fig:rank_corr">

<figcaption>Rank correlations between NLP systems / word counts and
Robust Ranking Aggregation lists, by normalization scheme.</figcaption>
</figure>

# Automated Scoring

For automated scoring, we use a dictionary of words associated with the
10 Schwartz values (Ponizovskiy et al. 2020). With this dictionary as
reference, we computationally estimate the degree to which each value is
reflected in the lyrics text according to traditional word
counting (Ponizovskiy et al. 2020), as well as by assessing cosine
similarity between dictionary words and lyrics texts using four classes
of pre-trained word embeddings: `word2vec`, a generic English word
embedding trained on Google News dataset (Mikolov et al. 2013); `glove`,
another generic English word embedding trained on Common Crawl
dataset (Pennington, Socher, and Manning 2014);
`mxm-far-[1`$`\sim`$`10]`, trained on the collected initial lyrics
candidate pool, employing the Glove model (Pennington, Socher, and
Manning 2014) (using ten models populated from ten cross-validation
folds, whose parameters are tuned based on English word similarity
judgement data (Faruqui and Dyer 2014).); `mxm-cv-[1`$`\sim`$`10]`, ten
variants of lyrics based word-embeddings from cross-validation folds
selected by Glove loss values on the validation set; and finally,
`sent-bert`, a transformer model that encodes sentence into a embedding
vector, fine-tuning of a generic self-supervised language model called
MPNet, which is trained on a large scale English corpus (Reimers and
Gurevych 2019). Our process thus resulted in 24 sets of scores: 5 from
models and one from word-counting, normalized using four methods.

We take the perspective from theory that that value assessments should
be seen as ranked lists, and thus coerce scores to ranked lists per
model per song. We then compute rank correlations between ranked lists
derived from model scores and RRA lists from participants. As RRA lists
assess lack of consensus on rankings, personal values with high *p*
values received tied rankings, at the bottom of the list. Correlations
were computed using Kendall’s $`\tau`$ which is robust to ties
(Figure <a href="#fig:rank_corr" data-reference-type="ref"
data-reference="fig:rank_corr">4</a>).

In earlier work (Richard, Bond Jr, and Stokes-Zoota 2003; Ponizovskiy et
al. 2020), Pearson correlations of 0.1-0.2 were considered as moderate
evidence of the validity of a proposed dictionary in relation to a
psychometrically validated instrument. Although we are using a different
metric, we observe several models whose mean rank correlations exceed
the .10 mark. The mean Kendall’s $`\tau`$ values were highest for the
word2vec, sent-bert, and wordcount models with null normalization
(SD=.24, .30, and .34 respectively). We further observe that 76% of the
rank correlations for word2vec exceed the .10 mark, followed by
56.1% from sent-bert, and 47.8% from wordcounts. Although none of these
models had been thoroughly optimized and thus this cannot be interpreted
as a thorough benchmark, we do see evidence of higher than expected
correlations.

We also explored whether our fuzzy strata might hint towards more or
less automatically scorable lyrics. We found most strata to be
uninformative. However, when examining the rank correlations for our
overall best performing model, word2vec, we did observe higher mean
correlations for some artist tag topics than others
(Figure <a href="#fig:artist_tag_topic" data-reference-type="ref"
data-reference="fig:artist_tag_topic">5</a>). In particular, topics 10
(which included the tags: ‘jazz’, ‘chillout’, ‘lounge’, ‘trip-hop’,
‘downtempo’), 11 (which included the tags like: ‘metal’, ‘celtic’,
‘thrash metal’, ‘dutch’, ‘seen live’), and 16 (which included tags like:
’country’, ‘Soundtrack’, ‘americana’, ‘danish’, ‘Disney’). Although
speculative, we do expect that certain genres are more difficult to
interpret than others, in particular for people who are generally
unfamiliar with such music.

<figure id="fig:artist_tag_topic">

<figcaption>Rank correlations between word2vec scores Robust Ranking
Aggregation lists, per genre grouping operationalized as Artist Tag
Topic.</figcaption>
</figure>

# Descriptive Analyses

We conduct a further exploratory data analysis by examining the gathered
value annotations with respect to the song strata introduced in
Section <a href="#sec:primarylyricdata:fuzzysampling" data-reference-type="ref"
data-reference="sec:primarylyricdata:fuzzysampling">3.1</a>. To better
understand the overall patterns of value rankings in songs we visualize
the average ranking of each value for each level of each stratum. To
reflect the uncertainty of aggregated ranking from RRA, we employ
‘truncated’ rankings: the values within each aggregated ranked list are
considered ties if their p-values higher than the threshold
($`p=0.05`$), hence with high uncertainty in their ranking
positions.[^13]

In all results, we observe that there is a tendency of overall value
ranking: 1) a generally strong presence of HEDONISM in higher ranks in
all cases, followed by STIMULATION and SELF (SELF-DIRECTION). 2)
ACHIEVEMENT and POWER generally follow next across all figures, and 3)
the rest of the values, including BENEVOLENCE, UNIVERSALISM, SECURITY,
CONFORMITY, and TRADITION overall rank lower, but show higher
variability across strata. We refer to these three groups of values as
*Group1* (HEDONISM, STIMULATION and SELF), *Group2* (ACHIEVEMENT and
POWER), and *Group3* (the rest) for the rest of the section.

<figure id="fig:avg_rank_trend_12">

<figcaption>Average value ranking from ‘release year’ (A) and
‘artist-playlist frequency’ (B). <span
class="math inline"><em>x</em></span> and <span
class="math inline"><em>y</em></span> axis represent the strata and
average ranking measure from RRA, respectively. Each point in different
point shapes and vertical bars denote the average ranking value and its
confidence interval (at 95% level). For visual convenience, we connected
the same values with lines. </figcaption>
</figure>

Zooming in each to stratum, in Figure
<a href="#fig:avg_rank_trend_12" data-reference-type="ref"
data-reference="fig:avg_rank_trend_12">6</a>, we observe that the
‘release year’ (sub-figure **A**) strata show the most consistent and
visible trend especially for Group3, which generally declines over time.
Such a trend is not as obvious in Group1 and only partially observed in
Group2. The low presence of Group3 is especially noticeable in the
1990s, although it regained its presence to some degree, a pattern which
the SELF value from Group1 partially shares. Such visible movements
suggest that the rank of specific values may evolve over time. In
sub-figure **B**, we observe the most flat response across all strata
considered: beyond the fluctuation pattern that is shared by all groups,
there is no substantial variability among groups, which implies that
popularity might not be as correlated as the ‘release year’.

<figure id="fig:avg_rank_trend_34">

<figcaption>Average value ranking from ‘artist-tag topic’ (C) and
‘lyrics topic’ (D).</figcaption>
</figure>

Moving onto
Figure <a href="#fig:avg_rank_trend_34" data-reference-type="ref"
data-reference="fig:avg_rank_trend_34">7</a>, we discuss the value
presence pattern in two ‘topic’ strata. First, in sub-figure **C**, we
observe that Group3 values show overall higher variability than ‘artist
playlist frequency’. It is notable that there are a few distinct topics
in which Group3 values show a significant difference; the sixth, seventh
and fourteenth topics, which correspond to the ‘under 2000
listeners/musical’, ‘folk/singer-songwriter’, and ‘Hip-Hop/rap’ topics
when represented in primary topic terms. Specifically, we see that first
two topics show a high presence of Group3 values, while the latter
topics show the least presence of Group3 values. It suggests that the
artists in these styles/genres were perceived on average to present
clearly different sets of values through their lyrics, distinguished by
the inclusion/exclusion of values such as BENEVOLENCE or UNIVERSALISM.

Finally, considering sub-figure **D**, we observe a similar pattern as
‘artist playlist frequency’ in
<a href="#fig:avg_rank_trend_12" data-reference-type="ref"
data-reference="fig:avg_rank_trend_12">6</a>, albeit with relatively
more variability in Group3 values. Notably, the ‘rap/hip-hop’ lyrics
topic shows the least presence of Group3 values, which aligns to the
observation from previous sub-figure. The ‘sad/romantic1’ topic, on the
other hand, shows the highest ranking of Group3 values. Another
remarkable topic is ‘gospel/reggae’ topic, where HEDONISM value is least
present, which semantically aligns well with the typical lyrical theme
of those songs.

# Limitations and Future Work

In this work we attempt to ground-truth perceptions of ambiguous song
lyrics for perceived human values. We adopt a validated questionnaire
from the social sciences for this purpose, in addition to a purposeful,
if conceptually ’fuzzy’, stratified sampling strategy, and estimate the
average number of ratings needed to estimate the average perception of
values in a song. We acknowledge our current sample of 360 lyrics is
small and may need expansion for more typical work, and that, while we
had a representative population sample, not every member of the sample
rated every song. We thus did gather diverse opinions, but cannot claim
they fully represent the target population. In addition, the small
sample of songs allowed for only limited observation of patterns that
might emerge in larger samples with relation to our defined strata, and
indefinite conclusions given the overall massive population of songs in
existence. We also did not assess whether variations on the annotation
instrument might result in substantial differences in the annotations we
received (Kern et al. 2023), nor did we repeat our procedure (Inel,
Draws, and Aroyo 2023). In addition, we acknowledge that participants
from different groups will perceive and thus annotate corpora
differently (Homan et al. 2022; Prabhakaran et al. 2023). Thus, we
expect that lyrics may be especially sensitive to varying perceptions,
which we did not explore in this work. Finally, we only provide a
preliminary comparison to automated scoring methods, and did not
leverage the most contemporary tools for this purpose (e.g. Large
Language Models). All of these are rich and promising avenues for future
work.

The most interesting avenues are potential relationships that could be
revealed with more annotated songs, and eventual automated scoring
methods. In particular, we see potential in understanding music
consumption more broadly from patterns revealed in the dominant value
hierarchies in specific music genres, popularity segments, lyrical
topics, and even release year. And for understanding music consumption
more narrowly, from patterns revealed in an individual’s music
preferences, and the degree to which they conform with their own value
hierarchy.

# Conclusion

Song lyrics remain a widely and repeatedly consumed, yet ambiguous form
of text, and thus a promising and challenging avenue for research into
better understanding the people that consume them. We observe promising
initial results for the annotation of personal values in songs, despite
our limitations. MDS plots of aggregated ratings showed the beginnings
of the expected structure of values, conforming more closely than might
be expected from as little as 360 songs. We also observed high
inter-rater reliability in the raw scores, suggesting a sufficiently
reliable annotation procedure with 25 ratings. Thus, we see promise on
our method for ground-truthing lyrics despite their ambiguity. A
post-hoc procedure revealed that 15 ratings may be enough on average: we
repeatedly subsampled 5, 10, 15 and 20 ratings for each value within
each song, and calculated pearson correlations between subsample means
and canonical means. From this, we see Pearson correlations to the
canonical mean exceed 0.9 for all values from 15 subsampled ratings.
Further lyric annotation may thus require fewer annotations per song
than what was gathered in this work. In addition, we observe promising
rank correlations between ranked rater scores and our automated methods,
with over 75% of the rankings in our best performing model above a
minimal threshold of .10. Despite inherent challenges in the task, our
method shows initial promise, and multiple fruitful avenues for future
work.

# Ethics Statement

Our study includes data gathered from people, and was approved by the
Human Research Ethics board of our university. We follow Prolific’s
guidelines on fair compensation to set our compensation rates. Survey
design and data handling were pre-discussed with our institutional data
management and research ethics advisors, we obtained formal data
management plan and human research ethics approval. Participants gave
informed consent before proceeding with the survey, which informed them
of the intentions of use for their data, and that it could be withdrawn
at any time.

<div id="refs" class="references csl-bib-body hanging-indent"
entry-spacing="0">

<div id="ref-ali2006songs" class="csl-entry">

Ali, S Omar, and Zehra F Peynircioğlu. 2006. “Songs and Emotions: Are
Lyrics and Melodies Equal Partners?” *Psychology of Music* 34 (4):
511–34.

</div>

<div id="ref-boyd2017language" class="csl-entry">

Boyd, Ryan L, and James W Pennebaker. 2017. “Language-Based Personality:
A New Approach to Personality in a Digital World.” *Current Opinion in
Behavioral Sciences* 18: 63–68.

</div>

<div id="ref-brand2019cultural" class="csl-entry">

Brand, Charlotte O, Alberto Acerbi, and Alex Mesoudi. 2019. “Cultural
Evolution of Emotional Expression in 50 Years of Song Lyrics.”
*Evolutionary Human Sciences* 1.

</div>

<div id="ref-brattico2011functional" class="csl-entry">

Brattico, Elvira, Vinoo Alluri, Brigitte Bogert, Thomas Jacobsen, Nuutti
Vartiainen, Sirke Katriina Nieminen, and Mari Tervaniemi. 2011. “A
Functional MRI Study of Happy and Sad Emotions in Music with and Without
Lyrics.” *Frontiers in Psychology* 2: 308.

</div>

<div id="ref-cabitza2020if" class="csl-entry">

Cabitza, Federico, Andrea Campagner, and Luca Maria Sconfienza. 2020.
“As If Sand Were Stone. New Concepts and Metrics to Probe the Ground on
Which to Build Trustable AI.” *BMC Medical Informatics and Decision
Making* 20 (1): 1–21.

</div>

<div id="ref-DBLP:conf/recsys/ChenLSZ18" class="csl-entry">

Chen, Ching-Wei, Paul Lamere, Markus Schedl, and Hamed Zamani. 2018.
“Recsys Challenge 2018: Automatic Music Playlist Continuation.” In
*Proceedings of the 12th ACM Conference on Recommender Systems, RecSys
2018, Vancouver, BC, Canada, October 2-7, 2018*, edited by Sole Pera,
Michael D. Ekstrand, Xavier Amatriain, and John O’Donovan, 527–28. ACM.
<https://doi.org/10.1145/3240323.3240342>.

</div>

<div id="ref-conrad2019extreme" class="csl-entry">

Conrad, Frederick, Jason Corey, Samantha Goldstein, Joseph Ostrow, and
Michael Sadowsky. 2019. “Extreme Re-Listening: Songs People Love... And
Continue to Love.” *Psychology of Music* 47 (2): 158–72.

</div>

<div id="ref-davison2000multidimensional" class="csl-entry">

Davison, Mark L, and Stephen G Sireci. 2000. “Multidimensional Scaling.”
In *Handbook of Applied Multivariate Statistics and Mathematical
Modeling*, 323–52. Elsevier.

</div>

<div id="ref-DeBruine_Jones_2018" class="csl-entry">

DeBruine, Lisa M, and Benedict C Jones. 2018. “Determining the Number of
Raters for Reliable Mean Ratings.” OSF.
<https://doi.org/10.17605/OSF.IO/X7FUS>.

</div>

<div id="ref-demetriou2018vocals" class="csl-entry">

Demetriou, Andrew, Andreas Jansson, Aparna Kumar, and Rachel M Bittner.
2018. “Vocals in Music Matter: The Relevance of Vocals in the Minds of
Listeners.” In *ISMIR*, 514–20.

</div>

<div id="ref-DBLP:conf/acl/FaruquiD14" class="csl-entry">

Faruqui, Manaal, and Chris Dyer. 2014. “Community Evaluation and
Exchange of Word Vectors at Wordvectors.org.” In *Proceedings of the
52nd Annual Meeting of the Association for Computational Linguistics,
ACL 2014, June 22-27, 2014, Baltimore, MD, USA, System Demonstrations*,
19–24. The Association for Computer Linguistics.
<https://doi.org/10.3115/V1/P14-5004>.

</div>

<div id="ref-gardikiotis2012rock" class="csl-entry">

Gardikiotis, Antonis, and Alexandros Baltzis. 2012. “‘Rock Music for
Myself and Justice to the World!’: Musical Identity, Values, and Music
Preferences.” *Psychology of Music* 40 (2): 143–63.

</div>

<div id="ref-graham2009liberals" class="csl-entry">

Graham, Jesse, Jonathan Haidt, and Brian A Nosek. 2009. “Liberals and
Conservatives Rely on Different Sets of Moral Foundations.” *Journal of
Personality and Social Psychology* 96 (5): 1029.

</div>

<div id="ref-groves2009survey" class="csl-entry">

Groves, Robert M, Floyd J Fowler Jr, Mick P Couper, James M Lepkowski,
Eleanor Singer, and Roger Tourangeau. 2009. *Survey Methodology*. Vol.
561. John Wiley & Sons.

</div>

<div id="ref-holtrop2022exploring" class="csl-entry">

Holtrop, Djurre, Janneke K Oostrom, Ward R J van Breda, Antonis
Koutsoumpis, and Reinout E de Vries. 2022. “Exploring the Application of
a Text-to-Personality Technique in Job Interviews.” *European Journal of
Work and Organizational Psychology* 31 (6): 799–816.

</div>

<div id="ref-homan2022annotator" class="csl-entry">

Homan, Christopher, Tharindu Cyril Weerasooriya, Lora Aroyo, and Chris
Welty. 2022. “Annotator Response Distributions as a Sampling Frame.” In
*Proceedings of the 1st Workshop on Perspectivist Approaches to NLP@
LREC2022*, 56–65.

</div>

<div id="ref-howlin2020patients" class="csl-entry">

Howlin, Claire, and Brendan Rooney. 2020. “Patients Choose Music with
High Energy, Danceability, and Lyrics in Analgesic Music Listening
Interventions.” *Psychology of Music*, 0305735620907155.

</div>

<div id="ref-inel2023collect" class="csl-entry">

Inel, Oana, Tim Draws, and Lora Aroyo. 2023. “Collect, Measure, Repeat:
Reliability Factors for Responsible AI Data Collection.” In *Proceedings
of the AAAI Conference on Human Computation and Crowdsourcing*,
11:51–64. 1.

</div>

<div id="ref-kern2023annotation" class="csl-entry">

Kern, Christoph, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, and
Frauke Kreuter. 2023. “Annotation Sensitivity: Training Data Collection
Methods Affect Model Performance.” *arXiv Preprint arXiv:2311.14212*.

</div>

<div id="ref-kiesel2022identifying" class="csl-entry">

Kiesel, Johannes, Milad Alshomary, Nicolas Handke, Xiaoni Cai, Henning
Wachsmuth, and Benno Stein. 2022. “Identifying the Human Values Behind
Arguments.” In *Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)*,
4459–71. Association for Computational Linguistics.

</div>

<div id="ref-kim2020butter" class="csl-entry">

Kim, Jaehun, Andrew M Demetriou, Sandy Manolios, Maria Stella Tavella,
and Cynthia CS Liem. 2020. “Butter Lyrics over Hominy Grit: Comparing
Audio and Psychology-Based Text Features in MIR Tasks.” In *ISMIR*,
861–68.

</div>

<div id="ref-RRAcitation" class="csl-entry">

Kolde, Raivo. 2022. *RobustRankAggreg: Methods for Robust Rank
Aggregation*. <https://CRAN.R-project.org/package=RobustRankAggreg>.

</div>

<div id="ref-kolde2012robust" class="csl-entry">

Kolde, Raivo, Sven Laur, Priit Adler, and Jaak Vilo. 2012. “Robust Rank
Aggregation for Gene List Integration and Meta-Analysis.”
*Bioinformatics* 28 (4): 573–80.

</div>

<div id="ref-koo2016guideline" class="csl-entry">

Koo, Terry K, and Mae Y Li. 2016. “A Guideline of Selecting and
Reporting Intraclass Correlation Coefficients for Reliability Research.”
*Journal of Chiropractic Medicine* 15 (2): 155–63.

</div>

<div id="ref-Liem2012MusicGap" class="csl-entry">

Liem, Cynthia C. S., Andreas Rauber, Thomas Lidy, Richard Lewis,
Christopher Raphael, Joshua D. Reiss, Tim Crawford, and Alan Hanjalic.
2012. “<span class="nocase">Music Information Technology and
Professional Stakeholder Audiences: Mind the Adoption Gap</span>.” In
*Dagstuhl Follow-Ups*. Vol. 3. Schloss Dagstuhl - Leibniz-Zentrum fuer
Informatik. <https://doi.org/10.4230/DFU.VOL3.11041.227>.

</div>

<div id="ref-lindeman2005measuring" class="csl-entry">

Lindeman, Marjaana, and Markku Verkasalo. 2005. “Measuring Values with
the Short Schwartz’s Value Survey.” *Journal of Personality Assessment*
85 (2): 170–78.

</div>

<div id="ref-maheshwari2017societal" class="csl-entry">

Maheshwari, Tushar, Aishwarya N Reganti, Samiksha Gupta, Anupam Jamatia,
Upendra Kumar, Björn Gambäck, and Amitava Das. 2017. “A Societal
Sentiment Analysis: Predicting the Values and Ethics of Individuals by
Analysing Social Media Content.” In *Proceedings of the 15th Conference
of the European Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers*, 731–41. Association for
Computational Linguistics.

</div>

<div id="ref-maio2010mental" class="csl-entry">

Maio, Gregory R. 2010. “Mental Representations of Social Values.” In
*Advances in Experimental Social Psychology*, 42:1–43. Elsevier.

</div>

<div id="ref-manolios2019influence" class="csl-entry">

Manolios, Sandy, Alan Hanjalic, and Cynthia CS Liem. 2019. “The
Influence of Personal Values on Music Taste: Towards Value-Based Music
Recommendations.” In *Proceedings of the 13th ACM Conference on
Recommender Systems*, 501–5.

</div>

<div id="ref-DBLP:conf/nips/MikolovSCCD13" class="csl-entry">

Mikolov, Tomás, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases
and Their Compositionality.” In *Advances in Neural Information
Processing Systems 26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a Meeting Held December 5-8,
2013, Lake Tahoe, Nevada, United States*, edited by Christopher J. C.
Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger,
3111–19.
<https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html>.

</div>

<div id="ref-north2020relationship" class="csl-entry">

North, Adrian C, Amanda E Krause, and David Ritchie. 2020. “The
Relationship Between Pop Music and Lyrics: A Computerized Content
Analysis of the United Kingdom’s Weekly Top Five Singles, 1999–2013.”
*Psychology of Music*, 0305735619896409.

</div>

<div id="ref-pennebaker2015development" class="csl-entry">

Pennebaker, James W, Ryan L Boyd, Kayla Jordan, and Kate Blackburn.
2015. “The Development and Psychometric Properties of LIWC2015.”

</div>

<div id="ref-DBLP:conf/emnlp/PenningtonSM14" class="csl-entry">

Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014.
“Glove: Global Vectors for Word Representation.” In *Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2014, October 25-29, 2014, Doha, Qatar, A Meeting of SIGDAT, a
Special Interest Group of the ACL*, edited by Alessandro Moschitti, Bo
Pang, and Walter Daelemans, 1532–43. ACL.
<https://doi.org/10.3115/V1/D14-1162>.

</div>

<div id="ref-ponizovskiy2020development" class="csl-entry">

Ponizovskiy, Vladimir, Murat Ardag, Lusine Grigoryan, Ryan Boyd, Henrik
Dobewall, and Peter Holtz. 2020. “Development and Validation of the
Personal Values Dictionary: A Theory–Driven Tool for Investigating
References to Basic Human Values in Text.” *European Journal of
Personality* 34 (5): 885–902.

</div>

<div id="ref-prabhakaran2023framework" class="csl-entry">

Prabhakaran, Vinodkumar, Christopher Homan, Lora Aroyo, Alicia Parrish,
Alex Taylor, Mark Dı́az, and Ding Wang. 2023. “A Framework to Assess
(Dis) Agreement Among Diverse Rater Groups.” *arXiv Preprint
arXiv:2311.05074*.

</div>

<div id="ref-preniqi2022more" class="csl-entry">

Preniqi, Vjosa, Kyriaki Kalimeri, and Charalampos Saitis. 2022. “" More
Than Words": Linking Music Preferences and Moral Values Through Lyrics.”
*arXiv Preprint arXiv:2209.01169*.

</div>

<div id="ref-Rcitation" class="csl-entry">

R Core Team. 2022. *R: A Language and Environment for Statistical
Computing*. Vienna, Austria: R Foundation for Statistical Computing.
<https://www.R-project.org/>.

</div>

<div id="ref-DBLP:conf/emnlp/ReimersG19" class="csl-entry">

Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence
Embeddings Using Siamese BERT-Networks.” In *Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*, edited by
Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, 3980–90.
Association for Computational Linguistics.
<https://doi.org/10.18653/V1/D19-1410>.

</div>

<div id="ref-richard2003one" class="csl-entry">

Richard, F Dan, Charles F Bond Jr, and Juli J Stokes-Zoota. 2003. “One
Hundred Years of Social Psychology Quantitatively Described.” *Review of
General Psychology* 7 (4): 331–63.

</div>

<div id="ref-rokeach1973nature" class="csl-entry">

Rokeach, Milton. 1973. *The Nature of Human Values.* Free press.

</div>

<div id="ref-sagiv2022personal" class="csl-entry">

Sagiv, Lilach, and Shalom H Schwartz. 2022. “Personal Values Across
Cultures.” *Annual Review of Psychology* 73: 517–46.

</div>

<div id="ref-sandri2023don" class="csl-entry">

Sandri, Marta, Elisa Leonardelli, Sara Tonelli, and Elisabetta Ježek.
2023. “Why Don’t You Do It Right? Analysing Annotators’ Disagreement in
Subjective Tasks.” In *Proceedings of the 17th Conference of the
European Chapter of the Association for Computational Linguistics*,
2420–33.

</div>

<div id="ref-schindler2012facilitating" class="csl-entry">

Schindler, Alexander, Rudolf Mayer, and Andreas Rauber. 2012.
“Facilitating Comprehensive Benchmarking Experiments on the Million Song
Dataset.” In *ISMIR*, 469–74. International Society for Music
Information Retrieval.

</div>

<div id="ref-schwartz1992universals" class="csl-entry">

Schwartz, Shalom H. 1992. “Universals in the Content and Structure of
Values: Theoretical Advances and Empirical Tests in 20 Countries.” In
*Advances in Experimental Social Psychology*, 25:1–65. Elsevier.

</div>

<div id="ref-schwartz2012overview" class="csl-entry">

———. 2012. “An Overview of the Schwartz Theory of Basic Values.” *Online
Readings in Psychology and Culture* 2 (1): 11.

</div>

<div id="ref-schwartz1987toward" class="csl-entry">

Schwartz, Shalom H, and Wolfgang Bilsky. 1987. “Toward a Universal
Psychological Structure of Human Values.” *Journal of Personality and
Social Psychology* 53 (3): 550.

</div>

<div id="ref-schwartz2001extending" class="csl-entry">

Schwartz, Shalom H, Gila Melech, Arielle Lehmann, Steven Burgess, Mari
Harris, and Vicki Owens. 2001. “Extending the Cross-Cultural Validity of
the Theory of Basic Human Values with a Different Method of
Measurement.” *Journal of Cross-Cultural Psychology* 32 (5): 519–42.

</div>

<div id="ref-swami2013metalheads" class="csl-entry">

Swami, Viren, Fiona Malpass, David Havard, Karis Benford, Ana Costescu,
Angeliki Sofitiki, and Donna Taylor. 2013. “Metalheads: The Influence of
Personality and Individual Differences on Preference for Heavy Metal.”
*Psychology of Aesthetics, Creativity, and the Arts* 7 (4): 377.

</div>

<div id="ref-van2005world" class="csl-entry">

Van Sickel, Robert W. 2005. “A World Without Citizenship: On (the
Absence of) Politics and Ideology in Country Music Lyrics, 1960–2000.”
*Popular Music and Society* 28 (3): 313–31.

</div>

<div id="ref-Psychcitation" class="csl-entry">

William Revelle. 2023. *Psych: Procedures for Psychological,
Psychometric, and Personality Research*. Evanston, Illinois:
Northwestern University. <https://CRAN.R-project.org/package=psych>.

</div>

</div>

[^1]: <https://www.musicbusinessworldwide.com/files/2022/12/f23d5bc086957241e6177f054507e67b.png>

[^2]: <https://www.gwi.com/reports/music-streaming-around-the-world>

[^3]: <https://en.wikipedia.org/wiki/List_of_instrumental_number_ones_on_the_UK_Singles_Chart>

[^4]: <https://research.atspotify.com/2020/09/the-million-playlist-dataset-remastered/>

[^5]: <https://www.musixmatch.com/>

[^6]: Each member independently screened each lyric and the screening
    process overall was discussed at length.

[^7]: e.g., Spotify reports over 100 million songs in its
    catalogue<https://newsroom.spotify.com/company-info/>

[^8]: Full code of our sampling procedure is
    at <https://anonymous.4open.science/r/lyrics-value-estimators-CE33/1_stimulus_sampling/stratified_sampling.py>

[^9]: <https://prolific.co>

[^10]: <https://qualtrics.com>

[^11]: It has shown correlations ranging from .45-.70 per value with
    longer more established procedures, test-retest reliability, as well
    as the typical values structure shown in
    Figure <a href="#fig:circle" data-reference-type="ref"
    data-reference="fig:circle">2</a>

[^12]: .7 is a commonly considered an acceptable level of reliability in
    the form of internal consistency

[^13]: We assume that the adjusted exact p-value from RRA monotonically
    decreases as the rank position ascends (i.e., the lower the p-value
    is, the higher the ranking position is).
