<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Vocals in Music Matter – Ground Truthing is a Field</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../_chapters/ISMIR_2020/ISMIR_2020.html" rel="next">
<link href="../../summary.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../_chapters/ISMIR_2018/ISMIR_2018.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vocals in Music Matter</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Ground Truthing is a Field</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2018/ISMIR_2018.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vocals in Music Matter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2020/ISMIR_2020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Butter Lyrics Over Harmony Grit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2024/ISMIR_2024.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automatic Estimation of Personal Values in Song Lyrics</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">4</span> Introduction</a></li>
  <li><a href="#vocals-in-semantic-data" id="toc-vocals-in-semantic-data" class="nav-link" data-scroll-target="#vocals-in-semantic-data"><span class="header-section-number">5</span> Vocals in Semantic Data</a>
  <ul class="collapse">
  <li><a href="#playlist-tags-and-search-queries" id="toc-playlist-tags-and-search-queries" class="nav-link" data-scroll-target="#playlist-tags-and-search-queries"><span class="header-section-number">5.1</span> Playlist Tags and Search Queries</a></li>
  <li><a href="#artist-biographies" id="toc-artist-biographies" class="nav-link" data-scroll-target="#artist-biographies"><span class="header-section-number">5.2</span> Artist Biographies</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">5.3</span> Conclusions</a></li>
  </ul></li>
  <li><a href="#vocals-in-survey-data" id="toc-vocals-in-survey-data" class="nav-link" data-scroll-target="#vocals-in-survey-data"><span class="header-section-number">6</span> Vocals in Survey Data</a>
  <ul class="collapse">
  <li><a href="#survey-1-semantic-components-of-music" id="toc-survey-1-semantic-components-of-music" class="nav-link" data-scroll-target="#survey-1-semantic-components-of-music"><span class="header-section-number">6.1</span> Survey 1: Semantic Components of Music</a>
  <ul class="collapse">
  <li><a href="#recruitment" id="toc-recruitment" class="nav-link" data-scroll-target="#recruitment"><span class="header-section-number">6.1.1</span> Recruitment</a></li>
  <li><a href="#survey" id="toc-survey" class="nav-link" data-scroll-target="#survey"><span class="header-section-number">6.1.2</span> Survey</a></li>
  <li><a href="#semantic-categorization" id="toc-semantic-categorization" class="nav-link" data-scroll-target="#semantic-categorization"><span class="header-section-number">6.1.3</span> Semantic Categorization</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">6.1.4</span> Results</a></li>
  </ul></li>
  <li><a href="#survey-2-component-ranking" id="toc-survey-2-component-ranking" class="nav-link" data-scroll-target="#survey-2-component-ranking"><span class="header-section-number">6.2</span> Survey 2: Component Ranking</a>
  <ul class="collapse">
  <li><a href="#recruitment-1" id="toc-recruitment-1" class="nav-link" data-scroll-target="#recruitment-1"><span class="header-section-number">6.2.1</span> Recruitment</a></li>
  <li><a href="#survey-1" id="toc-survey-1" class="nav-link" data-scroll-target="#survey-1"><span class="header-section-number">6.2.2</span> Survey</a></li>
  <li><a href="#analytic-strategy" id="toc-analytic-strategy" class="nav-link" data-scroll-target="#analytic-strategy"><span class="header-section-number">6.2.3</span> Analytic Strategy</a></li>
  <li><a href="#results-and-conclusion" id="toc-results-and-conclusion" class="nav-link" data-scroll-target="#results-and-conclusion"><span class="header-section-number">6.2.4</span> Results and Conclusion</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#new-avenues-for-research" id="toc-new-avenues-for-research" class="nav-link" data-scroll-target="#new-avenues-for-research"><span class="header-section-number">7</span> New Avenues for Research</a></li>
  <li><a href="#discussion-and-conclusions" id="toc-discussion-and-conclusions" class="nav-link" data-scroll-target="#discussion-and-conclusions"><span class="header-section-number">8</span> Discussion and Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vocals in Music Matter</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Introduction</h1>
<p>The Music Information Retrieval (MIR) community has historically focused on content-based understanding of music. The type of content-based analysis studied over time is typically driven by the data available to the task, or the interests of the specific researchers. An alternative motivator could be to study topics that are salient in the minds of listeners, especially with respect to listener’s musical preference. Specifically, understanding which attributes of music contribute the most to music preference, and their relative weight, could help guide research efforts. One attribute of music we would expect to be salient in the minds of listeners is the singing voice.</p>
<p>Psychology research anticipates the importance of the human voice as a salient stimulus, and as a component of music in particular. The human ability to communicate exceeds that of any other species studied thus far, with both speech and singing being cultural universals reliant on vocal production. It is theorized that the advanced human ability to communicate, discriminate, and to experience emotional responses in vocalizations has allowed for the emergence of music&nbsp;(Juslin and Laukka 2003). Our emotions are often accompanied by involuntary changes in our physiology and nonverbal expressions, such as facial expressions and vocalizations (Porges 2001). Our reactions to the emotional content expressed in the vocals in music may have similar effects. As such, much psychological research has focused on the singing voice even more than speech, due to the precision required to execute and process musical vocalizations (S. Hutchins and Moreno 2013). This makes musical vocals a well-anticipated candidate for study as a feature of music, as we would expect people to have a sophisticated ability to deliver, empathize with, and process vocal communications.</p>
<p>We would therefore expect that the vocals in music would be an especially salient component, if not the most salient. While a complete review is beyond the scope of this paper, some research is particularly worth noting. For example, it has been shown that both adults (Weiss, Trehub, and Schellenberg 2012) and children (Weiss et al.&nbsp;2015) recall melodies more correctly when sung with the voice than when played with instruments. Hutchins and Moreno (S. Hutchins and Moreno 2013) review literature that shows relatively precise perception of pitch in the human voice, yet fewer noticeable pitch errors in the voice relative to musical instruments or synthesized voices (S. M. Hutchins and Peretz 2012). Neuroscience studies show specific areas of the brain involved in processing human voices (Belin et al.&nbsp;2000). Although similar regions of the brain are involved in processing both music and voices, there is differential processing of the human voice relative to music (Armony et al.&nbsp;2015). As such, the human voice may be processed as a uniquely significant sound.</p>
<p>However, while prior research suggests that vocals would be especially relevant to music preference, no study to our knowledge has assessed the importance of the voice in music, relative to other musical components. To address this gap, we test the hypothesis that the voice is as or more important than other musical components across implicit and explicit datasets, using traditional social science techniques, as well as data mining techniques. First, we mine data available from Spotify, including playlist titles, search data and artist biographies, to test whether terms related to vocals are prevalent. However, we show that the results of the data mining are inconclusive as to whether or not vocals are salient in the minds of listeners. Specifically, it is not clear whether the vocals can be disentangled from other factors in playlist titles and search queries, such as genre. For more conclusive results, we gather data from users explicitly. To this aim we conduct two online survey studies: the first gathered subjective data on the salient components of music directly from listener reports, which were separated into semantic categories using card-sorting. The second asked participants to rank the semantic categories from the first study in terms of importance to their musical preference. We conclude that two aspects related to the voice are especially salient, namely the voice itself, and the lyrics of the song. Furthermore, we highlight the importance of gathering explicit data to complement implicit techniques, in situations where factors may not be easily disentangled.</p>
</section>
<section id="vocals-in-semantic-data" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Vocals in Semantic Data</h1>
<p>Prior research has shown that semantic descriptors of music may be an appropriate means for users to query music databases (Lesaffre et al. 2008). Given the large amount of semantic data available to Spotify such as playlist titles, search results, and artist biographies, one might hypothesize that terms describing the vocals would commonly appear in this implicit data.</p>
<section id="playlist-tags-and-search-queries" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="playlist-tags-and-search-queries"><span class="header-section-number">5.1</span> Playlist Tags and Search Queries</h2>
<p>Non-common words or groups of words and emojis appearing in the titles of a large number of Spotify’s user-generated playlists were aggregated to create a list of the 1000 most frequently occurring <em>tags</em>. Each of these 1000 tags was assigned a category by a professional curator based on the tag itself and information from the tracks most frequently associated with the tag. The categories, determined by the curator, were Genre (e.g.&nbsp;“K-Pop”), Mood (e.g.&nbsp;“sad”), Activity (e.g.&nbsp;“gym”), Popularity (e.g.&nbsp;“Today’s hits”), Artist (e.g.&nbsp;“Justin Timberlake”), Era (e.g.&nbsp;“70’s”), Culture (e.g.&nbsp;“Latin”), Lyrics (e.g.&nbsp;“clean”), Rhythm (e.g.&nbsp;“groove”), Instrument (e.g.&nbsp;“guitar”), Tempo (e.g.&nbsp;“slow”), Voice (e.g.&nbsp;“female singers”), or Other (e.g.&nbsp;“favorites”, “Jenna”, “hi”). The percentage of playlists containing each of these tag categories is displayed in Figure&nbsp;<a href="#fig:playlist-tags" data-reference-type="ref" data-reference="fig:playlist-tags">1</a>, top.</p>
<p>Surprisingly, we see that tags explicitly related to vocals are not at all common compared to other types of tags, with the most common tags being related to genre, mood, or activity. Playlist titles can be viewed as labels for groups of music, and this analysis suggests that people do not often label groups of music based on explicit characteristics of the vocals. However, specific vocal characteristics (as well as many other musical attributes) may be implicit in many of the other tag categories, particularly for genre, mood, and artist. As vocal delivery style and genre are closely related, emotions communicated by the voice and the mood of the collection of songs may be related, and as each artist has a unique voice, we conclude that the relative weight of vocals may not have been disentangled from other factors.</p>
<figure id="fig:playlist-tags" class="figure">
<div class="center">
<embed src="figs/AllFreqs.pdf">
</div>
<figcaption>
(Top) Percentage of Spotify playlists containing one of the top 1000 tags corresponding to each category. (Middle) Percentage of descriptive search queries corresponding to each tag category, sampled from one day of search data. (Bottom) tf-idf for each term category in artist biographies compared with Wikipedia term frequencies.
</figcaption>
</figure>
<p>We perform a similar analysis on descriptive terms from one day’s worth of Spotify search queries, and obtained similarly inconclusive results, shown in Figure&nbsp;<a href="#fig:playlist-tags" data-reference-type="ref" data-reference="fig:playlist-tags">1</a>, middle.</p>
</section>
<section id="artist-biographies" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="artist-biographies"><span class="header-section-number">5.2</span> Artist Biographies</h2>
<p>Finally, we analyze descriptive terms that occur in 100,000 professionally authored artist biographies on Spotify. We use TF-IDF&nbsp;(Sparck Jones 1972) to retrieve terms that are distinctive to music writers, by comparing the frequency of terms in artist biographies to the frequency of the same terms in Wikipedia. The 100 most distinctive terms, grouped into semantic categories, are displayed in Figure&nbsp;<a href="#fig:playlist-tags" data-reference-type="ref" data-reference="fig:playlist-tags">1</a>, bottom. While many terms are much more frequent in music text (e.g.&nbsp;“bassist”, “jazz”, “songwriter”), vocals specifically were not more frequently mentioned than other musical aspects. One can hypothesize that the TF-IDF method is insufficient for this particular task, due to vocals being commonly discussed outside the context of music, and thus a relatively more common word in Wikipedia.</p>
</section>
<section id="conclusions" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">5.3</span> Conclusions</h2>
<p>Our results thus far do not show support for our general hypothesis. It may be the case that the intuitive notion of the relevance of vocals to user preference is misleading. On the other hand, it may also be the case that the importance of vocals is implicit in this data, as certain vocal styles are indicative of genre or mood. As such, the overlap between the voice and a number of the tags and descriptors analyzed prevents us from disentangling the unique effect of the voice from other musical components.</p>
</section>
</section>
<section id="vocals-in-survey-data" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Vocals in Survey Data</h1>
<p>In order to disentangle the unique effect of the voice among other components, we gathered explicit data from users. Specifically, we conducted two online survey studies in order to collect self-reported data on 1) the salient components of music, and 2) their relative ranking. Unlike prior surveys, such as (Lesaffre et al.&nbsp;2008) that presented users with short musical excerpts and groupings of adjectives to rate, we allowed the users to freely enter their responses to the question “When you listen to music, what things about the music do you notice?”. This allowed us to assess whether vocals would emerge as a salient component of music. In addition, we explored what aspects of the voice users report as being important to their musical taste.</p>
<section id="survey-1-semantic-components-of-music" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="survey-1-semantic-components-of-music"><span class="header-section-number">6.1</span> Survey 1: Semantic Components of Music</h2>
<p>The aim of our first survey was to establish an unranked set of self-reported salient components of music. While our hypothesis was that the vocals would be prominent, it was crucial to avoid biasing respondents as the data collected were explicit. As such, our first survey asked participants what they notice when listening to music that might make them like or dislike a song. We deliberately did not specify anything further, such as the type of music, or that we were interested in components of music, nor were participants asked to listen to musical excerpts so as not to bias responses. As an exploratory measure, we then asked participants to describe what about vocals specifically might make them like or dislike a song <em>after</em> the previous open ended questions, so as not to bias responses. Responses to these two open-response questions were manually sorted into semantic categories by the researchers.</p>
<section id="recruitment" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="recruitment"><span class="header-section-number">6.1.1</span> Recruitment</h3>
<p>A random sample of 50,000 people was drawn from the database of Spotify’s Monthly Active Users (MUAs), divided approximately equally between the United States and Canada. 860 individuals responded to the survey, however 224 did not respond to any questions beyond the consent form, and 9 were removed for giving nonsensical responses. 626 individuals — 338 women (average age 33.6 years with a standard deviation of 16.1); 288 men (average age 30.6 years with a standard deviation of 15.5) — completed the survey in its entirety.</p>
</section>
<section id="survey" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="survey"><span class="header-section-number">6.1.2</span> Survey</h3>
<p>An online consent form was first presented to respondents. We then asked:</p>
<blockquote class="blockquote">
<p><em>Q1:</em> When you listen to music, what things about the music do you notice? Please list as many as you can think of here:</p>
</blockquote>
<p>The respondents were shown a screen with open-response format fields to complete, in which they could complete up to seven fields. On the following screen, respondents were presented with a list of their responses in random order, and asked:</p>
<blockquote class="blockquote">
<p><em>Q2:</em> Please rank how important the aspects you listed are to your musical preference, where 1 is the most important.</p>
</blockquote>
<p>They were then asked the following two questions about the items they ranked from 1 to 3:</p>
<blockquote class="blockquote">
<p><em>Q3:</em> (a) What about ____ would make you like a song? (b) What about ____ would make you dislike a song?</p>
</blockquote>
<p>Lastly, to explore what aspects of vocals may be relevant, participants responded to the following:</p>
<blockquote class="blockquote">
<p><em>Q4:</em> (Please ignore these questions if you’ve already mentioned the vocals, the voice, the singer/rapper etc.) (a) When would vocals make you like a song? (b) When would vocals make you dislike a song?</p>
</blockquote>
<p>They were then given the opportunity to comment on the survey, and were shown a final debriefing screen.</p>
</section>
<section id="semantic-categorization" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="semantic-categorization"><span class="header-section-number">6.1.3</span> Semantic Categorization</h3>
<p>A number of partially completed surveys contained responses sufficiently complete for card sorting. 317 sufficient responses — 262 from the completed surveys as well as 55 sufficiently complete partial – were then card-sorted by a team of researchers. Card-sorting is a common technique used in social sciences and elsewhere to discover clusters of related concepts&nbsp;(Miller 1969). Traditionally, individuals are presented with physical paper “cards” that have terms and/or descriptions printed on them, printed pictures, or a group of objects. They are then asked to group items in a way that makes sense, given the research question. Here, we apply card-sorting to derive semantically meaningful groupings of musical components from the freely entered words and phrases that participants entered in each field.</p>
<p>Participant responses to <em>Q1</em> (i.e.&nbsp;“When you listen to music, what things do you notice?”) were printed twice, once next to their response to <em>Q3a</em> (“What about _ would make you like a song?”), and again next to the response to <em>Q3b</em> (“What about _ would make you dislike a song?”). As such, researchers had respondents’ top 3 terms printed out twice, once next to the positive descriptive aspects of the term, and once next to the negative descriptive aspects. A term (e.g.&nbsp;“the lyrics”) and its descriptor (e.g.&nbsp;“when they have meaning”) comprised a card. Figure&nbsp;<a href="#fig:card-examples" data-reference-type="ref" data-reference="fig:card-examples">2</a> shows examples of positive and negative cards that were used in card sorting.</p>
<figure id="fig:card-examples" class="figure">
<div class="center">
<embed src="figs/cards.pdf">
</div>
<figcaption>
Survey 1 sample answers for <em>Q3</em>. (Top) Card for an answer to <em>Q3a</em>. (Bottom) Card for an answer to <em>Q3b</em>.
</figcaption>
</figure>
<p>As some responses were unclear (e.g.&nbsp;“the melody” was mentioned, but the descriptor clearly focused on the quality of the singer’s voice), the research team was instructed to look at both the term and its descriptor when determining its semantic category. The researchers then reviewed the cards a second time, and defined sub-categories where necessary.</p>
</section>
<section id="results" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="results"><span class="header-section-number">6.1.4</span> Results</h3>
<p>The output of this study was two sets of semantic categories: broad semantic categories of music, and vocal-specific semantic categories. Statistical testing was not possible, given the intentionally imprecise nature of the responses. However, out of the 626 responses to the first question, 186 (29.7%) mentioned the vocals, the voice, or the singer, 348 (55.6%) mentioned the lyrics, or the words, and 101 (16.1%) mentioned both. While this is no indication of relative importance, it does demonstrate that the voice and the lyrics were salient musical components to our respondents.</p>
<p>The broad semantic categories determined by the researchers are presented in the left column of Table&nbsp;<a href="#tab:mus-attributes" data-reference-type="ref" data-reference="tab:mus-attributes">[tab:mus-attributes]</a> (note that the other results in Table&nbsp;<a href="#tab:mus-attributes" data-reference-type="ref" data-reference="tab:mus-attributes">[tab:mus-attributes]</a> are from Study 2). The category of <em>Emotion/mood</em> referred to the ability of a song to evoke emotion, whether the emotion was a match or a mismatch to the current or desired mood or current activity, whether the emotion was desirable or undesirable, and nostalgia. <em>Voice</em> included genre related terms (e.g.&nbsp;mumble rap, metal, auto-tune, speechiness/rapping), descriptions of how the voice is used (e.g.&nbsp;unique/novel, screaming, pitch/pitch range, presence or absence of effects, intensity/effort/power, emotionality, authenticity, whininess/nasality, melodic-ness), skill, the innate qualities of the voice, liking/disliking, and the mix/blend. The <em>Lyrics</em> category represented items that indicated whether or not lyrics were present, their intelligibility, the presence of profanity, how “well” crafted they were, the “message”, the meaning behind them or general lyrical content and how relatable they are. <em>Beat/Rhythm</em> referred to whether it was liked/disliked, whether it “fit” the song, danceability, and uniqueness. The <em>Structure/complexity</em> of songs included liking or disliking the hook or chorus, and the song length. Instrumentation referred to drums, bass, and guitar. <em>Sound</em> referred to audio quality and related concerns. Self-explanatory categories included <em>Tempo/BPM</em>, the mention of a <em>Specific Artist</em>, <em>Genre</em>, <em>Harmony</em>, <em>Chords</em>, <em>Musicianship</em>, <em>Melody</em>, and <em>Popularity/Novelty</em>.</p>
</section>
</section>
<section id="survey-2-component-ranking" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="survey-2-component-ranking"><span class="header-section-number">6.2</span> Survey 2: Component Ranking</h2>
<p>While the first study aimed at determining what attributes of music were salient in the minds of listeners, the aim of the second survey was to determine the relative importance of each of the components. Specifically, we explored whether the voice would be ranked highest among a list of musical attributes. To accomplish this, participants were asked to rank a list of attributes derived from the results of our first survey, thus allowing an assessment of whether or not vocals rank above other components.</p>
<section id="recruitment-1" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="recruitment-1"><span class="header-section-number">6.2.1</span> Recruitment</h3>
<p>A randomized sampling method was employed among the database of Spotify’s Monthly Active Users (MAUs) that had not opted-out of email correspondence. An email with a link to an online survey was sent to 50,000 potential respondents, approximately equally divided among the United States and Canada.</p>
<p>A total of 531 respondents — 263 of which were women (average age 31.8 years, with a standard deviation of 16.5); 268 were men (average age 34.2 years, with a standard deviation of 14.8) — completed the survey in its entirety. 429 participants completed the first half of the survey (broad semantic categories), whereas 360 participants completed the second half (vocal semantic categories).</p>
</section>
<section id="survey-1" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="survey-1"><span class="header-section-number">6.2.2</span> Survey</h3>
<p>An online consent form was first presented to respondents. The derived semantic categories were rephrased to be more easily understood (see Table&nbsp;<a href="#tab:mus-attributes" data-reference-type="ref" data-reference="tab:mus-attributes">[tab:mus-attributes]</a>, Description). Participants were presented with the new list of descriptions in random order, and asked to “Please click all the items below that would make you like or dislike a song.” They were then presented with a list of all the items they had clicked, also in random order, and asked to rank them.</p>
<div class="table*">
<div class="center">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Broad Semantic Category</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: right;"><strong>Borda score</strong></th>
<th style="text-align: right;"><strong><span class="math inline">\(`p`\)</span>-value</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Emotion/mood</td>
<td style="text-align: left;">How it makes you feel - the emotions/mood</td>
<td style="text-align: right;">4641</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Voice</td>
<td style="text-align: left;">Voice/vocals</td>
<td style="text-align: right;">3688</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Lyrics</td>
<td style="text-align: left;">Lyrics</td>
<td style="text-align: right;">3656</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Beat/rhythm</td>
<td style="text-align: left;">Beat/rhythm</td>
<td style="text-align: right;">3460</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Structure/Complexity</td>
<td style="text-align: left;">How it’s composed, the hook, the structure</td>
<td style="text-align: right;">2677</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Musicianship</td>
<td style="text-align: left;">Skill of the musicians, musicianship</td>
<td style="text-align: right;">2583</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Melody</td>
<td style="text-align: left;">The main melody</td>
<td style="text-align: right;">2577</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sound</td>
<td style="text-align: left;">The “sound”, or the recording quality</td>
<td style="text-align: right;">2406</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Specific Artist</td>
<td style="text-align: left;">The specific artist</td>
<td style="text-align: right;">2349</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Genre</td>
<td style="text-align: left;">The specific genre</td>
<td style="text-align: right;">2293</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Instrumentation</td>
<td style="text-align: left;">The musical instruments (e.g.&nbsp;drums, bass, guitar)</td>
<td style="text-align: right;">2084</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tempo/BPM</td>
<td style="text-align: left;">How fast or slow the song is</td>
<td style="text-align: right;">1828</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Harmony</td>
<td style="text-align: left;">Harmony</td>
<td style="text-align: right;">1763</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Chords</td>
<td style="text-align: left;">The chords</td>
<td style="text-align: right;">1086</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Popularity/Novelty</td>
<td style="text-align: left;">How popular or unique it is</td>
<td style="text-align: right;">777</td>
<td style="text-align: right;">1.000</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>As a continuation of our exploratory study of vocal characteristics, a second list was then presented, comprised of terms derived from the vocal and lyrics semantic categories. For clarity, the terms were rephrased as they appear in Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>.</p>
</section>
<section id="analytic-strategy" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="analytic-strategy"><span class="header-section-number">6.2.3</span> Analytic Strategy</h3>
<p>Responses were subjected to Borda counting (Borda 1781) and Robust Rank Aggregation (Kolde et al.&nbsp;2012). Borda counting is a simple procedure for aggregating votes by summing ranks. The Borda score <span class="math inline">\(`B_i`\)</span> for an item <span class="math inline">\(`i`\)</span> is computed as <span class="math inline">\(`B_i = \sum_{p=0}^N \left( |r_p| - r_{p, i} \right)`\)</span> where <span class="math inline">\(`N`\)</span> is the number of participants, <span class="math inline">\(`r_{p,i}`\)</span> is participant <span class="math inline">\(`p`\)</span>’s rank of item <span class="math inline">\(`i`\)</span>, starting at zero, and <span class="math inline">\(`|r_p|`\)</span> is the number of items ranked by <span class="math inline">\(`p`\)</span>. The Borda method does not naturally extend to partial lists(Dwork et al.&nbsp;2001) — we have chosen to award higher scores to preferred items in long lists.</p>
<p>To verify the statistical significance of our findings we supplement the Borda count with Robust Rank Aggregation (RRA), in which we compare our survey results to a null hypothesis. Each item receives a score based on its observed position, compared to an expected random ordering. Upper bounds to <span class="math inline">\(`p`\)</span>-values are computed using Bonferroni correction, with values of 1.0 indicating null findings. In this work we used the implementation provided by the <span class="smallcaps">RobustRankAggreg</span> package<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
</section>
<section id="results-and-conclusion" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="results-and-conclusion"><span class="header-section-number">6.2.4</span> Results and Conclusion</h3>
<p>Results can be found in Tables <a href="#tab:mus-attributes" data-reference-type="ref" data-reference="tab:mus-attributes">[tab:mus-attributes]</a> and <a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, with categories ordered by descending Borda count. We are able to show statistical significance of both the most salient broad and vocal semantic categories. Importantly, our results show that the Vocals and Lyrics ranked second and third among the list of components (Borda scores and RRA agree on the order of the first four broad categories). This indicates that, relative to other musical components, respondents overall indicated the importance of the vocals and lyrics.</p>
<div class="table*">
<div class="center">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: left;"><strong>Vocal Semantic Categories</strong></th>
<th style="text-align: right;"><strong>Borda score</strong></th>
<th style="text-align: right;"><strong><span class="math inline">\(`p`\)</span>-value</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A</td>
<td style="text-align: left;">Singing skill</td>
<td style="text-align: right;">3423</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">B</td>
<td style="text-align: left;">How well the voice fits or matches the rest of the music</td>
<td style="text-align: right;">3380</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;">C</td>
<td style="text-align: left;">Lyrical skill / cleverness / wit</td>
<td style="text-align: right;">3145</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">D</td>
<td style="text-align: left;">The meaning, or the “message” of the words</td>
<td style="text-align: right;">3038</td>
<td style="text-align: right;"><strong>0.048</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;">E</td>
<td style="text-align: left;">Authenticity / “realness”</td>
<td style="text-align: right;">2884</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">F</td>
<td style="text-align: left;">Uniqueness</td>
<td style="text-align: right;">2780</td>
<td style="text-align: right;"><strong>&lt;0.001</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;">G</td>
<td style="text-align: left;">If the voice is emotional</td>
<td style="text-align: right;">2771</td>
<td style="text-align: right;"><strong>0.006</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">H</td>
<td style="text-align: left;">Voice strength / intensity / effort</td>
<td style="text-align: right;">2721</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">I</td>
<td style="text-align: left;">If the voice sounds natural</td>
<td style="text-align: right;">2480</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: center;">J</td>
<td style="text-align: left;">Being able to relate</td>
<td style="text-align: right;">2256</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">K</td>
<td style="text-align: left;">If the voice is melodic</td>
<td style="text-align: right;">2202</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: center;">L</td>
<td style="text-align: left;">Whether or not you can understand the lyrics</td>
<td style="text-align: right;">2056</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">M</td>
<td style="text-align: left;">If it’s whiny or nasal</td>
<td style="text-align: right;">1801</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: left;">Whether or not there’s screaming</td>
<td style="text-align: right;">1771</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">O</td>
<td style="text-align: left;">The overall pitch, or the range of the pitch</td>
<td style="text-align: right;">1400</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: center;">P</td>
<td style="text-align: left;">Whether or not there are lyrics</td>
<td style="text-align: right;">1250</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Q</td>
<td style="text-align: left;">Whether it has production effects on it, like autotune</td>
<td style="text-align: right;">1230</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: center;">R</td>
<td style="text-align: left;">Profanity, explicit lyrics</td>
<td style="text-align: right;">1086</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">S</td>
<td style="text-align: left;">Whether or not there is rapping</td>
<td style="text-align: right;">909</td>
<td style="text-align: right;">1.000</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
</section>
</section>
<section id="new-avenues-for-research" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> New Avenues for Research</h1>
<p>While the musical attributes related to the broad musical categories (Table&nbsp;<a href="#tab:mus-attributes" data-reference-type="ref" data-reference="tab:mus-attributes">[tab:mus-attributes]</a>) are well studied in MIR, the attributes related to vocals (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>) present a number of exciting and unexplored research directions. A limiting factor to studying some of these problems, as is often the case, is the availability of data, and we encourage researchers to focus data collection efforts in these areas as well. A further limiting factor is that users of online musical platforms may come from a specific demographic, e.g.&nbsp;regular internet users typically younger than 35, who engage in music related activities in about one third of the online time, have had at least some musical education, and have a preference for pop, rock and classical music (Lesaffre et al.&nbsp;2008). In addition, our sample was derived from the U.S. and Canada. As such, a cross-cultural sample may differ in their relative preference for vocals.</p>
<p>Our exploratory data suggest that there is a vast space of research in tagging and measuring different qualities of the singing voice, such as whether a singing voice is authentic, powerful, natural, melodic, nasal, or emotional (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, rows E, H, I, K, M and G). In addition to these categories, determined by untrained listeners, there are a number of other more specific categories such as modes of phonation that could be explored. Further, in addition to vocal qualities, there are genre-centric vocal styles, such as identifying rap or screaming (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, rows S and O).</p>
<p>Another interesting and (as far as we are aware) unexplored research area is to measure whether a voice fits or blends well with the background music (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, row B). This is somewhat related to the problem of determining “mashability” in automatic-mashup generation. This is a broad problem that is likely based on many factors, such as the style of the vocalist compared to the background, the way the song is mixed, and the overall expectations of the musical genre. We suspect this could be most easily studied when isolated vocals/backgrounds are available in order to automatically generate examples of vocals that do not match the background by blending random combinations.</p>
<p>The problem of identifying whether a voice is “unique” is likely challenging (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, row F), as it is not necessarily a quality that can be determined in isolation, but rather relative to many other voices. One possible approach to this problem would be to treat the problem as one of outlier detection.</p>
<p>Production effects applied to the singing voice are increasingly common, especially different types of distortion or the infamous auto-tune (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, row Q). Automatic identification of these production effects presents an interesting challenge, and one where data could be automatically generated with the help of plugins for generating effects and databases with isolated vocals with corresponding backgrounds.</p>
<p>Measuring the relatability (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, row J) of a singer is a quality that is relative to the listener, rather than absolute. Factors that could affect a singer’s relatability could include the age, gender, culture or language of the singer relative to the listener, which might require automatic identification of each of these attributes of the singer.</p>
<p>Lyric intelligibility (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, row L) has not been well studied, and also presents a novel challenge&nbsp;(Ibrahim et al.&nbsp;2017). This problem does not necessarily directly require lyric transcription, and may be able to be determined from qualities of the audio. Similarly, determining whether a singing voice contains lyrics or is wordless has not been studied (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, row P).</p>
<p>Automatic lyric transcription has been studied&nbsp;(McVicar, Ellis, and Goto 2014; Kruspe and Fraunhofer 2016) but is not yet solved, and would power the automatic estimation of many of these vocal attributes. For lyric-related terms, given textual lyrics, while some attributes would be relatively simple to estimate (e.g.&nbsp;whether or not there is profanity), others present interesting NLP challenges, such as estimating whether the lyrics are “clever” or are “meaningful” (Table&nbsp;<a href="#tab:voc-attributes" data-reference-type="ref" data-reference="tab:voc-attributes">[tab:voc-attributes]</a>, rows R, C, and D).</p>
</section>
<section id="discussion-and-conclusions" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Discussion and Conclusions</h1>
<p>While our analyses of playlist titles and search queries were inconclusive, we show evidence that English-speaking respondents from the U.S. and Canada clearly indicated that the voice is a salient component of music. Specifically, Spotify users were asked what they notice about music while listening. Despite the unassuming nature of the question, our results showed that the voice was indeed salient among the group of reported musical attributes. Furthermore, users ranked the voice as the second most important component to their musical preference, after emotions.</p>
<p>Our results have a number of implications. With regards to MIR research specifically, our results suggest that the voice and lyrics are indeed relevant attributes that warrant further study. While individuals may not necessarily want or know how to describe vocals themselves, i.e.&nbsp;in their playlists or search queries, surveying listeners directly does indicate that they find vocals to be important. As such, clarifying how the voice relates to music preference is an important topic for future research.</p>
<p>Secondly, users indicated that the ability of a song to evoke emotions was the most important factor. This confirms findings in prior research of the relevance of emotional content in music, and how it is linked to musical preference, e.g.&nbsp;(Krumhansl 2017). Therefore, examining how music affects the emotions of listeners remains an important theme. Interestingly, while genre was the most frequent term used to label playlists or search for music, respondents did not rank the specific genre as important relative to the other attributes. Understanding why this is the case warrants further study.</p>
<p>More relevant to our hypothesis, is that the vocals and the lyrics of a song were ranked second and third by respondents who were directly asked what components of music are important to their preferences. Therefore the link between emotions perceived in the voice and lyrics, and the emotions felt in listeners, is very relevant to questions of music preference. Clarification of these links was out of scope in these studies, and could be addressed in future research.</p>
<p>Lastly, we show the relevance of explicitly collected data that might guide future research. While we showed inconclusive findings regarding the prevalence of vocals in implicit data, we did show that the unique effect of vocals on music preference may be observed using survey data. As such, explicit data-gathering techniques often found in the social sciences, as well as collaborations with social scientists, may be of great use to MIR researchers.</p>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-armony2015specificity" class="csl-entry" role="listitem">
Armony, Jorge L, William Aubé, Arafat Angulo-Perkins, Isabelle Peretz, and Luis Concha. 2015. “The Specificity of Neural Responses to Music and Their Relation to Voice Processing: An fMRI-Adaptation Study.” <em>Neuroscience Letters</em> 593: 35–39.
</div>
<div id="ref-belin2000voice" class="csl-entry" role="listitem">
Belin, Pascal, Robert J Zatorre, Philippe Lafaille, Pierre Ahad, and Bruce Pike. 2000. “Voice-Selective Areas in Human Auditory Cortex.” <em>Nature</em> 403 (6767): 309.
</div>
<div id="ref-Borda1781" class="csl-entry" role="listitem">
Borda, Jean C de. 1781. “Mémoire Sur Les Élections Au Scrutin.” <em>Histoire de l’Academie Royale Des Sciences</em>.
</div>
<div id="ref-dwork2001rank" class="csl-entry" role="listitem">
Dwork, Cynthia, Ravi Kumar, Moni Naor, and Dandapani Sivakumar. 2001. “Rank Aggregation Methods for the Web.” In <em>Proceedings of the 10th International Conference on World Wide Web</em>, 613–22. ACM.
</div>
<div id="ref-hutchins2012frog" class="csl-entry" role="listitem">
Hutchins, Sean Michael, and Isabelle Peretz. 2012. “A Frog in Your Throat or in Your Ear? Searching for the Causes of Poor Singing.” <em>Journal of Experimental Psychology: General</em> 141 (1): 76.
</div>
<div id="ref-hutchins2013linked" class="csl-entry" role="listitem">
Hutchins, Sean, and Sylvain Moreno. 2013. “The Linked Dual Representation Model of Vocal Perception and Production.” <em>Frontiers in Psychology</em> 4: 825.
</div>
<div id="ref-ibrahim2017intelligibility" class="csl-entry" role="listitem">
Ibrahim, Karim M, David Grunberg, Kat Agres, Chitralekha Gupta, and Ye Wang. 2017. “Intelligibility of Sung Lyrics: A Pilot Study.” In. International Society for Music Information Retrieval Conference.
</div>
<div id="ref-juslin2003music" class="csl-entry" role="listitem">
Juslin, Patrik N., and Petri Laukka. 2003. “Communication of Emotions in Vocal Expression and Musical Performance: Different Channels, Same Code?” <em>Psychological Bulletin</em> 129: 770–814.
</div>
<div id="ref-kolde2012robust" class="csl-entry" role="listitem">
Kolde, Raivo, Sven Laur, Priit Adler, and Jaak Vilo. 2012. “Robust Rank Aggregation for Gene List Integration and Meta-Analysis.” <em>Bioinformatics</em> 28 (4): 573–80.
</div>
<div id="ref-krumhansl2017listening" class="csl-entry" role="listitem">
Krumhansl, Carol Lynne. 2017. “Listening Niches Across a Century of Popular Music.” <em>Frontiers in Psychology</em> 8: 431.
</div>
<div id="ref-kruspe2016retrieval" class="csl-entry" role="listitem">
Kruspe, Anna M, and IDMT Fraunhofer. 2016. “Retrieval of Textual Song Lyrics from Sung Inputs.” In <em>INTERSPEECH</em>, 2140–44.
</div>
<div id="ref-lesaffre2008potential" class="csl-entry" role="listitem">
Lesaffre, Micheline, Liesbeth De Voogdt, Marc Leman, Bernard De Baets, Hans De Meyer, and Jean-Pierre Martens. 2008. “How Potential Users of Music Search and Retrieval Systems Describe the Semantic Quality of Music.” <em>Journal of the Association for Information Science and Technology</em> 59 (5): 695–707.
</div>
<div id="ref-mcvicar2014leveraging" class="csl-entry" role="listitem">
McVicar, Matt, Daniel PW Ellis, and Masataka Goto. 2014. “Leveraging Repetition for Improved Automatic Lyric Transcription in Popular Music.” In <em>Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on</em>, 3117–21. IEEE.
</div>
<div id="ref-miller1969psychological" class="csl-entry" role="listitem">
Miller, George A. 1969. “A Psychological Method to Investigate Verbal Concepts.” <em>Journal of Mathematical Psychology</em> 6 (2): 169–91.
</div>
<div id="ref-porges2001polyvagal" class="csl-entry" role="listitem">
Porges, Stephen W. 2001. “The Polyvagal Theory: Phylogenetic Substrates of a Social Nervous System.” <em>International Journal of Psychophysiology</em> 42 (2): 123–46.
</div>
<div id="ref-sparck1972statistical" class="csl-entry" role="listitem">
Sparck Jones, Karen. 1972. “A Statistical Interpretation of Term Specificity and Its Application in Retrieval.” <em>Journal of Documentation</em> 28 (1): 11–21.
</div>
<div id="ref-weiss2015enhanced" class="csl-entry" role="listitem">
Weiss, Michael W, E Glenn Schellenberg, Sandra E Trehub, and Emily J Dawber. 2015. “Enhanced Processing of Vocal Melodies in Childhood.” <em>Developmental Psychology</em> 51 (3): 370.
</div>
<div id="ref-weiss2012something" class="csl-entry" role="listitem">
Weiss, Michael W, Sandra E Trehub, and E Glenn Schellenberg. 2012. “Something in the Way She Sings: Enhanced Memory for Vocal Melodies.” <em>Psychological Science</em> 23 (10): 1074–78.
</div>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="cran.r-project.org/web/packages/RobustRankAggreg" class="uri">cran.r-project.org/web/packages/RobustRankAggreg</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../summary.html" class="pagination-link" aria-label="Summary">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../_chapters/ISMIR_2020/ISMIR_2020.html" class="pagination-link" aria-label="Butter Lyrics Over Harmony Grit">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Butter Lyrics Over Harmony Grit</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>