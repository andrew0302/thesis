<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Towards Automatic Estimation of Personal Values in Song Lyrics – Ground Truthing is a Field</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../_chapters/ISMIR_2020/ISMIR_2020.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../_chapters/ISMIR_2024/ISMIR_2024.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automatic Estimation of Personal Values in Song Lyrics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Ground Truthing is a Field</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2018/ISMIR_2018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vocals in Music Matter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2020/ISMIR_2020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Butter Lyrics Over Harmony Grit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../_chapters/ISMIR_2024/ISMIR_2024.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automatic Estimation of Personal Values in Song Lyrics</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">6</span> Introduction</a></li>
  <li><a href="#personal-values" id="toc-personal-values" class="nav-link" data-scroll-target="#personal-values"><span class="header-section-number">7</span> Personal Values</a></li>
  <li><a href="#primary-lyrics-data" id="toc-primary-lyrics-data" class="nav-link" data-scroll-target="#primary-lyrics-data"><span class="header-section-number">8</span> Primary Lyrics Data</a>
  <ul class="collapse">
  <li><a href="#fuzzy-stratified-sampling" id="toc-fuzzy-stratified-sampling" class="nav-link" data-scroll-target="#fuzzy-stratified-sampling"><span class="header-section-number">8.1</span> Fuzzy Stratified Sampling</a></li>
  </ul></li>
  <li><a href="#ground-truthing-procedure" id="toc-ground-truthing-procedure" class="nav-link" data-scroll-target="#ground-truthing-procedure"><span class="header-section-number">9</span> Ground-Truthing Procedure</a>
  <ul class="collapse">
  <li><a href="#reliability-agreement-and-initial-validation" id="toc-reliability-agreement-and-initial-validation" class="nav-link" data-scroll-target="#reliability-agreement-and-initial-validation"><span class="header-section-number">9.1</span> Reliability, Agreement and Initial Validation</a></li>
  </ul></li>
  <li><a href="#automated-scoring" id="toc-automated-scoring" class="nav-link" data-scroll-target="#automated-scoring"><span class="header-section-number">10</span> Automated Scoring</a></li>
  <li><a href="#descriptive-analyses" id="toc-descriptive-analyses" class="nav-link" data-scroll-target="#descriptive-analyses"><span class="header-section-number">11</span> Descriptive Analyses</a></li>
  <li><a href="#limitations-and-future-work" id="toc-limitations-and-future-work" class="nav-link" data-scroll-target="#limitations-and-future-work"><span class="header-section-number">12</span> Limitations and Future Work</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">13</span> Conclusion</a></li>
  <li><a href="#ethics-statement" id="toc-ethics-statement" class="nav-link" data-scroll-target="#ethics-statement"><span class="header-section-number">14</span> Ethics Statement</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Towards Automatic Estimation of Personal Values in Song Lyrics</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Introduction</h1>
<p>Popular music in Western countries almost always contains lyrics, making song lyrics a widely, repeatedly consumed (Conrad et al.&nbsp;2019) form of text. Over 616 million people subscribe to streaming services worldwide<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, many of whom stream more than an hour of music every day<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Lyrics have been shown to be a salient component of music (Demetriou et al.&nbsp;2018), and out of over 1400 number-1 singles in the UK charts, only 30 were instrumental<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. The two representative US population samples that were our annotators indicate a median 90%&nbsp;of songs in their libraries contain lyrics (Figure&nbsp;<a href="#fig:lyrics_percentages" data-reference-type="ref" data-reference="fig:lyrics_percentages">1</a>).</p>
<figure id="fig:lyrics_percentages" class="figure">
<figcaption>
Distribution of self-reported percentage of music library containing lyrics from two representative US samples, n=505 and n=600 respectively.
</figcaption>
</figure>
<p>It is thus not surprising that informative relationships between popular songs and their lyrical content have been shown: e.g., country music lyrics rarely include political concepts (Van Sickel 2005), and songs with more typical (North, Krause, and Ritchie 2020) and more negative (Brand, Acerbi, and Mesoudi 2019) lyrics appear to be more successful. (Howlin and Rooney 2020) showed that patients are more likely to choose music with lyrics when participating in music-based pain reduction interventions, although melody had an overall larger effect (Ali and Peynircioğlu 2006) showed that lyrics enhance self reported emotional responses to music, and (Brattico et al.&nbsp;2011) showed a number of additional brain regions were active during the listening of sad music with lyrics, vs.&nbsp;sad music without lyrics. In fields closer to MIR, (Kim et al.&nbsp;2020) show that estimating psychological concepts from lyrics showed a small benefit in a number of MIR tasks, and (Preniqi, Kalimeri, and Saitis 2022) showed a correlation between moral principles estimated from song lyrics and music preferences.</p>
<p>A connection between music lyrics and music preferences anticipated by theory involves the personal values perceived in the lyrics by listeners. Prior work has shown correlations between an individual’s values, and the music they listen to (Manolios, Hanjalic, and Liem 2019; Gardikiotis and Baltzis 2012; Swami et al.&nbsp;2013; Preniqi, Kalimeri, and Saitis 2022), suggesting that we seek music in line with our principles. Yet we have not seen an attempt to measure perceived personal values expressed in the lyrics themselves via human annotation or automated methods.</p>
<p>In this work we take a first step towards the automated estimating the values perceived in song lyrics. As artistic and expressive language, lyrics are ambiguous text: they contain different forms of analogy and wordplay (Sandri et al.&nbsp;2023). Thus we take a perspectivist approach to the annotations: because we expect that perceptions will vary substantially more than in other annotation tasks, we aim to represent the general perceptions of only one population. We account for the subjectivity by gathering a large number of ratings (median 27) per song from a targeted population sample (U.S.), of 360 carefully sampled song lyrics, using a psychometric questionnaire that we adjust for this purpose. We treat values in line with theory: as ranked lists, using Robust Ranking Aggregation (RRA) to arrive at our ’ground truth’. We then gather estimates from word embedding models, by measuring semantic similarity between the lyrics and a validated dictionary. We show that ranked lists from estimates correlate moderately with annotation aggregates. We then discuss the implications of our results, the limitations of this project, and anticipated future work.</p>
</section>
<section id="personal-values" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Personal Values</h1>
<p>The modern study of human values spans over 500 samples in nearly 100 countries over the past 30 years, and has shown a relatively stable structure (Sagiv and Schwartz 2022), as illustrated in Figure&nbsp;<a href="#fig:circle" data-reference-type="ref" data-reference="fig:circle">2</a>. Personal values are a component of personality, defined as the hierarchy of principles that guide a person’s thoughts, behaviors, and the way they evaluate events (Schwartz and Bilsky 1987; Schwartz 2012). Basic human values can be used to describe people or groups: social science theory suggests that each person uses a hierarchical list of values as life-guiding principles (Rokeach 1973), such that we prioritize some values over others as we make decisions. Schwartz’s theory is the most widely used in social and cultural psychology, and has shown correlations with important behaviors, ranging from political affiliation to personal preferences (Sagiv and Schwartz 2022).</p>
<p>We communicate our values in order to gain cooperation and coordinate our efforts, according to Schwartz (Schwartz 1992). Thus our values will manifest in the words that we use&nbsp;(Boyd and Pennebaker 2017). Although personal values are traditionally measured by having individual people complete validated psychological questionnaires, it has been argued that values may be clearly expressed in the speech and text that we produce (Boyd and Pennebaker 2017).</p>
<figure id="fig:circle" class="figure">
<figcaption>
Visualization of the Schwartz 10-value inventory from <span class="citation" data-cites="schwartz1992universals">(Schwartz 1992)</span> used in this paper, such that more abstract values of Conservation, vs.&nbsp;Openness to Change, and Self-transcendence vs. Self-enhancement form 4 higher-order abstract values. Illustration adapted from <span class="citation" data-cites="maio2010mental">(Maio 2010)</span>.
</figcaption>
</figure>
<p>A common approach to measuring psychological aspects in text is to validate dictionaries: curated sets of words, with subsets aimed at measuring each component of the psychological aspect in question (Pennebaker et al.&nbsp;2015; Graham, Haidt, and Nosek 2009; Holtrop et al. 2022; Ponizovskiy et al.&nbsp;2020). Some work estimating the values of the authors of text has been conducted on individuals who have written personal essays and social media posts e.g.&nbsp;(Maheshwari et al.&nbsp;2017; Ponizovskiy et al.&nbsp;2020), and in arguments abstracted from various forms of public facing text (Kiesel et al.&nbsp;2022). However, we have not seen work aimed at measuring values <em>perceived</em> in text, measuring them along a scale as in prior work (Schwartz 1992), or ultimately treating them as a hierarchical list in line with theory (Rokeach 1973).</p>
</section>
<section id="primary-lyrics-data" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Primary Lyrics Data</h1>
<p>We aim to collect a sample of lyric data where the lyrics are as accurate as possible, and our sample is as representative as possible. We sampled from the population of songs in the Million Playlist Dataset (MPD)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> as it is large and recent compared to other similar datasets. The lyrics themselves were obtained through the API of <a href="https://www.musixmatch.com/">Musixmatch</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, a lyrics and music language platform. Musixmatch lyrics are crowdsourced by users who add, correct, sync, and translate them. Musixmatch then engages in several steps to verify quality of content, including spam detection, formatting, spelling and translation checking, as well as manual verification by over 2000 community curators, and a local team of Musixmatch editors. Via their API, Musixmatch provided us with an estimated first 30% of the lyrics of each song.</p>
<p>Using the ‘fuzzy’ stratified sampling method described below, we sampled 2200 songs. Three members of the research team manually screened approximately 600 of the 2200 songs for inclusion. Each set of lyrics was confirmed to be a match to the actual song, and for suitability<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Lyrics were unsuitable if they were: 1) not English, 2) completely onomatopoetic, 3) repetitions of single words or phrases, 4) too few words to estimate values present or, 5) were not a match to the meta-data of the song, e.g.&nbsp;artist title, song name. This resulted in 380 songs, 20 of which were used in a pilot study to determine the number of ratings to gather per song, and 360 were used for annotation.</p>
<section id="fuzzy-stratified-sampling" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="fuzzy-stratified-sampling"><span class="header-section-number">8.1</span> Fuzzy Stratified Sampling</h2>
<p>An initial challenge is determining how to represent a corpus. In our case, the population of songs is known to be very large<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. An ideal scenario would be one in which we aim for a known number of songs, randomly sampled from within clearly defined strata, i.e.&nbsp;relevant subgroups, also known as <em>stratified random sampling</em>&nbsp;(Groves et al. 2009). However, for music, we do not know how many songs we would need to sample in order to reach saturation, what the relevant strata to randomly sample within should be, and how to measure relevant parameters from each stratum.</p>
<p>Some measurable strata that affect the use of language in song lyrics are clear: e.g., the year of release, which may reflect different events or time-specific colloquial slang. Others are less clear: e.g., there is no single metric of popularity for music, although it can be estimated from various sources such as hit charts. Some may be very subjective, such as genre, for which there may be some overlap of human labelling, but no clear taxonomy exists in the eyes of musicological domain experts&nbsp;(Liem et al.&nbsp;2012).</p>
<p>Based upon these considerations, we aimed for a stratified random sampling procedure, based on strata that we acknowledge to be justifiable given our purpose, yet in some cases conceptually ‘fuzzy’: (1) release date; (2) popularity, operationalized as artist playlist frequency from the MPD&nbsp;(Chen et al.&nbsp;2018); (3) genre, estimated from topic modeling on Million Song Dataset artist tags (Schindler, Mayer, and Rauber 2012); (4) lyric topic, through a bag-of-words representation of the lyrics data. Popularity and Release date were divided into equally spaced bins; e.g.&nbsp;we divided release year into decades (60s, 70s, 80s, and so on), and genre and lyric topic were divided into categories.</p>
<p>Release date was quantized into 14 bins in 10-year increments from 1890-2030. Popularity was exponentially distributed, and thus manually binned, to make the quantiles per each of the 7 bins as similar as possible. Thus, the first bin contained the lowest 40% of the population in terms of popularity, while the 7th bin contained the highest 9%. Topic modelling was applied on a bag-of-word representation of the lyrics data and artist-tag data to yield <span class="math inline">\(`25`\)</span> estimated genres and <span class="math inline">\(`9`\)</span> lyrics topic strata, respectively.</p>
<p>We observed a skewness of data concentration with regard to several of our strata, e.g., songs that are recent and widely popular are most likely be drawn. To correct for this and thus get a more representative sample of an overall song catalogue, we oversample from less populated bins. For this, we use the maximum-a-posteriori (MAP) estimate of the categorical distribution of each stratum. The oversampling is controlled by concentration parameter <span class="math inline">\(`a`\)</span> of the symmetric Dirichlet distribution. We heuristically set this parameter such that songs in underpopulated bins still will make up up 5-10 % of our overall pool<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Through this method, we subsampled our initial 2200 songs lyrics.</p>
<figure id="fig:mds_plots" class="figure">
<figcaption>
MDS plots derived from the correlation plot reported in <span class="citation" data-cites="schwartz2001extending">(Schwartz et al.&nbsp;2001)</span>, and our participant responses as confidence-weighted means
</figcaption>
</figure>
</section>
</section>
<section id="ground-truthing-procedure" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Ground-Truthing Procedure</h1>
<p>We chose to obtain our annotations from samples of the US population, representative in terms of self-reported sex, ethnicity and age, through the Prolific<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> platform. Annotator pools comprised of two samples, the first n=505 wave participated in a pilot study to estimate the number of ratings per song needed on average, and the second n=600 wave comprised our main data collection. Participants completed the survey on the Qualtrics <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> platform.</p>
<p>We clearly differentiate between the Author and the Speaker of lyrics by explaining to participants that the Author of song lyrics may write from the perspective of someone or something else (the Speaker). 17 randomly selected sets of lyrics were then shown to each participant along with instructions to annotate each with the values of the Speaker. We adapted the 10-item questionnaire used in (Lindeman and Verkasalo 2005) for the value annotations, as it is the shortest questionnaire for assessing personal values whose validity and reliability have been assessed<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. As in (Lindeman and Verkasalo 2005), each questionnaire item is a specific value along with additional descriptive words e.g.&nbsp;POWER (social power, authority, wealth). We adjusted it by asking participants to indicate the values of the Speaker of the lyrics, and by having them indicate on a bar with -100 (opposed to their principles) on one end, and +100 (of supreme importance) on the other end instead of a likert scale. In addition, we asked participants to indicate how confident they were in their ratings, on a scale of 0 (not at all confident) to 100 (extremely confident), inspired by work that has shown that self-reported confidence in ratings can be used to estimate the accuracy of individual ratings (Cabitza, Campagner, and Sconfienza 2020).</p>
<p>We used a procedure similar to (DeBruine and Jones 2018) in order to determine the number of raters. Specifically, we recruited a representative 500+ participant sample of the US using the Prolific platform, who completed our survey for 20 songs. We then computed canonical mean ratings of each of the 10 values per song, and inter-rater reliability using Cronbach’s Alpha. We then estimated Cronbach’s alpha for a range of subsample sizes (5 to 50 participants in increments of 5), for each of the 10 values. We repeated this procedure 10 times per increment, separately for each of the 10 values, and examined the distribution of Cronbach’s Alpha. We specifically looked for the sample size with which Alpha exceeded .7 <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. We arrived at a conservative estimate of 25 ratings per set of lyrics, with songs receiving a median 27 ratings (range 22-30).</p>
<section id="reliability-agreement-and-initial-validation" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="reliability-agreement-and-initial-validation"><span class="header-section-number">9.1</span> Reliability, Agreement and Initial Validation</h2>
<p>The rater reliability was estimated via intra-class correlation for each personal value, (type 2k: see (Koo and Li 2016)) using the ‘<code>psych</code>’ package in R (William Revelle 2023), all of which exceeded .9 (excellent reliability). As an initial validation, we compare data simulated from values in the upper triangle of a correlation matrix reported in&nbsp;(Schwartz et al.&nbsp;2001) to those derived from our study. To aggregate our participants rankings for this purpose, we compute confidence-weighted means inspired by (Cabitza, Campagner, and Sconfienza 2020): we estimate confidence-weights by dividing participant’s self-reported confidence of a given rating by the highest possible response (100), and then compute aggregated means weighted by these. For both the simulated data and confidence-weighted mean scores, we generate a multi-dimensional scaling plot (MDS) (Davison and Sireci 2000) for visual comparison, which has previously been used as method to assess measurements conform to theory&nbsp;(Ponizovskiy et al.&nbsp;2020; Lindeman and Verkasalo 2005). Note: the interpretation is to observe whether each of the values appears next to expected neighboring values, and not each value’s orientation. From these plots (Figure&nbsp;<a href="#fig:mds_plots" data-reference-type="ref" data-reference="fig:mds_plots">3</a>), in as little as our 360 annotated lyrics, we surprisingly see similar clusters and relative positioning relations emerging as those obtained from a formal cross-cultural study.</p>
<p>We coerced the annotated scores to ranked lists of values, such that the highest scoring value was at the top. We derived ranked lists per participant per song, and then used Robust Ranking Aggregation (RRA) to extract a single ranked list per song. Aggregation was conducted using R version 4.2.2.(R Core Team 2022), and the <code>RobustRankAggreg</code> package (Kolde 2022). Briefly, RRA produces a ranked list by comparing the probability of the observed ranking of items to rankings from a uniform distribution. Essentially, scores are determined by comparing the height of an item on a set of lists to where it would appear if its rank were randomly distributed across lists. These scores are then subjected to statistical tests, where the resulting <em>p</em> value is Bonferronni corrected by the number of input lists(Kolde et al.&nbsp;2012). Thus, when an item appears in different positions on a list, the resulting p value is high, as its position appears randomly distributed.</p>
<p>As lyrics are ambiguous, we expect that some songs’ values are completely subjective. We operationalize these as randomly distributed rankings for all personal values for completely subjective songs, i.e. <em>p</em> values above .05 for all 10 items on the ranked list. Results from the RRA show 62 songs with <em>p</em> values above .05 for all 10 values, and 96 songs with only 1 value ranked. At most, 5 values were ranked, which occurred for 35 songs. Thus, we confirm that although there was correspondence in the scores that participants assigned per value per song, ranked lists did not always agree.</p>
<figure id="fig:rank_corr" class="figure">
<figcaption>
Rank correlations between NLP systems / word counts and Robust Ranking Aggregation lists, by normalization scheme.
</figcaption>
</figure>
</section>
</section>
<section id="automated-scoring" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Automated Scoring</h1>
<p>For automated scoring, we use a dictionary of words associated with the 10 Schwartz values&nbsp;(Ponizovskiy et al.&nbsp;2020). With this dictionary as reference, we computationally estimate the degree to which each value is reflected in the lyrics text according to traditional word counting&nbsp;(Ponizovskiy et al.&nbsp;2020), as well as by assessing cosine similarity between dictionary words and lyrics texts using four classes of pre-trained word embeddings: <code>word2vec</code>, a generic English word embedding trained on Google News dataset&nbsp;(Mikolov et al.&nbsp;2013); <code>glove</code>, another generic English word embedding trained on Common Crawl dataset&nbsp;(Pennington, Socher, and Manning 2014); <code>mxm-far-[1</code><span class="math inline">\(`\sim`\)</span><code>10]</code>, trained on the collected initial lyrics candidate pool, employing the Glove model&nbsp;(Pennington, Socher, and Manning 2014) (using ten models populated from ten cross-validation folds, whose parameters are tuned based on English word similarity judgement data&nbsp;(Faruqui and Dyer 2014).); <code>mxm-cv-[1</code><span class="math inline">\(`\sim`\)</span><code>10]</code>, ten variants of lyrics based word-embeddings from cross-validation folds selected by Glove loss values on the validation set; and finally, <code>sent-bert</code>, a transformer model that encodes sentence into a embedding vector, fine-tuning of a generic self-supervised language model called MPNet, which is trained on a large scale English corpus&nbsp;(Reimers and Gurevych 2019). Our process thus resulted in 24 sets of scores: 5 from models and one from word-counting, normalized using four methods.</p>
<p>We take the perspective from theory that that value assessments should be seen as ranked lists, and thus coerce scores to ranked lists per model per song. We then compute rank correlations between ranked lists derived from model scores and RRA lists from participants. As RRA lists assess lack of consensus on rankings, personal values with high <em>p</em> values received tied rankings, at the bottom of the list. Correlations were computed using Kendall’s <span class="math inline">\(`\tau`\)</span> which is robust to ties (Figure&nbsp;<a href="#fig:rank_corr" data-reference-type="ref" data-reference="fig:rank_corr">4</a>).</p>
<p>In earlier work&nbsp;(Richard, Bond Jr, and Stokes-Zoota 2003; Ponizovskiy et al.&nbsp;2020), Pearson correlations of 0.1-0.2 were considered as moderate evidence of the validity of a proposed dictionary in relation to a psychometrically validated instrument. Although we are using a different metric, we observe several models whose mean rank correlations exceed the .10 mark. The mean Kendall’s <span class="math inline">\(`\tau`\)</span> values were highest for the word2vec, sent-bert, and wordcount models with null normalization (SD=.24, .30, and .34 respectively). We further observe that 76%&nbsp;of the rank correlations for word2vec exceed the .10 mark, followed by 56.1%&nbsp;from sent-bert, and 47.8%&nbsp;from wordcounts. Although none of these models had been thoroughly optimized and thus this cannot be interpreted as a thorough benchmark, we do see evidence of higher than expected correlations.</p>
<p>We also explored whether our fuzzy strata might hint towards more or less automatically scorable lyrics. We found most strata to be uninformative. However, when examining the rank correlations for our overall best performing model, word2vec, we did observe higher mean correlations for some artist tag topics than others (Figure&nbsp;<a href="#fig:artist_tag_topic" data-reference-type="ref" data-reference="fig:artist_tag_topic">5</a>). In particular, topics 10 (which included the tags: ‘jazz’, ‘chillout’, ‘lounge’, ‘trip-hop’, ‘downtempo’), 11 (which included the tags like: ‘metal’, ‘celtic’, ‘thrash metal’, ‘dutch’, ‘seen live’), and 16 (which included tags like: ’country’, ‘Soundtrack’, ‘americana’, ‘danish’, ‘Disney’). Although speculative, we do expect that certain genres are more difficult to interpret than others, in particular for people who are generally unfamiliar with such music.</p>
<figure id="fig:artist_tag_topic" class="figure">
<figcaption>
Rank correlations between word2vec scores Robust Ranking Aggregation lists, per genre grouping operationalized as Artist Tag Topic.
</figcaption>
</figure>
</section>
<section id="descriptive-analyses" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Descriptive Analyses</h1>
<p>We conduct a further exploratory data analysis by examining the gathered value annotations with respect to the song strata introduced in Section&nbsp;<a href="#sec:primarylyricdata:fuzzysampling" data-reference-type="ref" data-reference="sec:primarylyricdata:fuzzysampling">3.1</a>. To better understand the overall patterns of value rankings in songs we visualize the average ranking of each value for each level of each stratum. To reflect the uncertainty of aggregated ranking from RRA, we employ ‘truncated’ rankings: the values within each aggregated ranked list are considered ties if their p-values higher than the threshold (<span class="math inline">\(`p=0.05`\)</span>), hence with high uncertainty in their ranking positions.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>In all results, we observe that there is a tendency of overall value ranking: 1) a generally strong presence of HEDONISM in higher ranks in all cases, followed by STIMULATION and SELF (SELF-DIRECTION). 2) ACHIEVEMENT and POWER generally follow next across all figures, and 3) the rest of the values, including BENEVOLENCE, UNIVERSALISM, SECURITY, CONFORMITY, and TRADITION overall rank lower, but show higher variability across strata. We refer to these three groups of values as <em>Group1</em> (HEDONISM, STIMULATION and SELF), <em>Group2</em> (ACHIEVEMENT and POWER), and <em>Group3</em> (the rest) for the rest of the section.</p>
<figure id="fig:avg_rank_trend_12" class="figure">
<figcaption>
Average value ranking from ‘release year’ (A) and ‘artist-playlist frequency’ (B). <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span> axis represent the strata and average ranking measure from RRA, respectively. Each point in different point shapes and vertical bars denote the average ranking value and its confidence interval (at 95% level). For visual convenience, we connected the same values with lines.
</figcaption>
</figure>
<p>Zooming in each to stratum, in Figure <a href="#fig:avg_rank_trend_12" data-reference-type="ref" data-reference="fig:avg_rank_trend_12">6</a>, we observe that the ‘release year’ (sub-figure <strong>A</strong>) strata show the most consistent and visible trend especially for Group3, which generally declines over time. Such a trend is not as obvious in Group1 and only partially observed in Group2. The low presence of Group3 is especially noticeable in the 1990s, although it regained its presence to some degree, a pattern which the SELF value from Group1 partially shares. Such visible movements suggest that the rank of specific values may evolve over time. In sub-figure <strong>B</strong>, we observe the most flat response across all strata considered: beyond the fluctuation pattern that is shared by all groups, there is no substantial variability among groups, which implies that popularity might not be as correlated as the ‘release year’.</p>
<figure id="fig:avg_rank_trend_34" class="figure">
<figcaption>
Average value ranking from ‘artist-tag topic’ (C) and ‘lyrics topic’ (D).
</figcaption>
</figure>
<p>Moving onto Figure&nbsp;<a href="#fig:avg_rank_trend_34" data-reference-type="ref" data-reference="fig:avg_rank_trend_34">7</a>, we discuss the value presence pattern in two ‘topic’ strata. First, in sub-figure <strong>C</strong>, we observe that Group3 values show overall higher variability than ‘artist playlist frequency’. It is notable that there are a few distinct topics in which Group3 values show a significant difference; the sixth, seventh and fourteenth topics, which correspond to the ‘under 2000 listeners/musical’, ‘folk/singer-songwriter’, and ‘Hip-Hop/rap’ topics when represented in primary topic terms. Specifically, we see that first two topics show a high presence of Group3 values, while the latter topics show the least presence of Group3 values. It suggests that the artists in these styles/genres were perceived on average to present clearly different sets of values through their lyrics, distinguished by the inclusion/exclusion of values such as BENEVOLENCE or UNIVERSALISM.</p>
<p>Finally, considering sub-figure <strong>D</strong>, we observe a similar pattern as ‘artist playlist frequency’ in <a href="#fig:avg_rank_trend_12" data-reference-type="ref" data-reference="fig:avg_rank_trend_12">6</a>, albeit with relatively more variability in Group3 values. Notably, the ‘rap/hip-hop’ lyrics topic shows the least presence of Group3 values, which aligns to the observation from previous sub-figure. The ‘sad/romantic1’ topic, on the other hand, shows the highest ranking of Group3 values. Another remarkable topic is ‘gospel/reggae’ topic, where HEDONISM value is least present, which semantically aligns well with the typical lyrical theme of those songs.</p>
</section>
<section id="limitations-and-future-work" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Limitations and Future Work</h1>
<p>In this work we attempt to ground-truth perceptions of ambiguous song lyrics for perceived human values. We adopt a validated questionnaire from the social sciences for this purpose, in addition to a purposeful, if conceptually ’fuzzy’, stratified sampling strategy, and estimate the average number of ratings needed to estimate the average perception of values in a song. We acknowledge our current sample of 360 lyrics is small and may need expansion for more typical work, and that, while we had a representative population sample, not every member of the sample rated every song. We thus did gather diverse opinions, but cannot claim they fully represent the target population. In addition, the small sample of songs allowed for only limited observation of patterns that might emerge in larger samples with relation to our defined strata, and indefinite conclusions given the overall massive population of songs in existence. We also did not assess whether variations on the annotation instrument might result in substantial differences in the annotations we received (Kern et al.&nbsp;2023), nor did we repeat our procedure (Inel, Draws, and Aroyo 2023). In addition, we acknowledge that participants from different groups will perceive and thus annotate corpora differently&nbsp;(Homan et al.&nbsp;2022; Prabhakaran et al.&nbsp;2023). Thus, we expect that lyrics may be especially sensitive to varying perceptions, which we did not explore in this work. Finally, we only provide a preliminary comparison to automated scoring methods, and did not leverage the most contemporary tools for this purpose (e.g.&nbsp;Large Language Models). All of these are rich and promising avenues for future work.</p>
<p>The most interesting avenues are potential relationships that could be revealed with more annotated songs, and eventual automated scoring methods. In particular, we see potential in understanding music consumption more broadly from patterns revealed in the dominant value hierarchies in specific music genres, popularity segments, lyrical topics, and even release year. And for understanding music consumption more narrowly, from patterns revealed in an individual’s music preferences, and the degree to which they conform with their own value hierarchy.</p>
</section>
<section id="conclusion" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Conclusion</h1>
<p>Song lyrics remain a widely and repeatedly consumed, yet ambiguous form of text, and thus a promising and challenging avenue for research into better understanding the people that consume them. We observe promising initial results for the annotation of personal values in songs, despite our limitations. MDS plots of aggregated ratings showed the beginnings of the expected structure of values, conforming more closely than might be expected from as little as 360 songs. We also observed high inter-rater reliability in the raw scores, suggesting a sufficiently reliable annotation procedure with 25 ratings. Thus, we see promise on our method for ground-truthing lyrics despite their ambiguity. A post-hoc procedure revealed that 15 ratings may be enough on average: we repeatedly subsampled 5, 10, 15 and 20 ratings for each value within each song, and calculated pearson correlations between subsample means and canonical means. From this, we see Pearson correlations to the canonical mean exceed 0.9 for all values from 15 subsampled ratings. Further lyric annotation may thus require fewer annotations per song than what was gathered in this work. In addition, we observe promising rank correlations between ranked rater scores and our automated methods, with over 75%&nbsp;of the rankings in our best performing model above a minimal threshold of .10. Despite inherent challenges in the task, our method shows initial promise, and multiple fruitful avenues for future work.</p>
</section>
<section id="ethics-statement" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> Ethics Statement</h1>
<p>Our study includes data gathered from people, and was approved by the Human Research Ethics board of our university. We follow Prolific’s guidelines on fair compensation to set our compensation rates. Survey design and data handling were pre-discussed with our institutional data management and research ethics advisors, we obtained formal data management plan and human research ethics approval. Participants gave informed consent before proceeding with the survey, which informed them of the intentions of use for their data, and that it could be withdrawn at any time.</p>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ali2006songs" class="csl-entry" role="listitem">
Ali, S Omar, and Zehra F Peynircioğlu. 2006. “Songs and Emotions: Are Lyrics and Melodies Equal Partners?” <em>Psychology of Music</em> 34 (4): 511–34.
</div>
<div id="ref-boyd2017language" class="csl-entry" role="listitem">
Boyd, Ryan L, and James W Pennebaker. 2017. “Language-Based Personality: A New Approach to Personality in a Digital World.” <em>Current Opinion in Behavioral Sciences</em> 18: 63–68.
</div>
<div id="ref-brand2019cultural" class="csl-entry" role="listitem">
Brand, Charlotte O, Alberto Acerbi, and Alex Mesoudi. 2019. “Cultural Evolution of Emotional Expression in 50 Years of Song Lyrics.” <em>Evolutionary Human Sciences</em> 1.
</div>
<div id="ref-brattico2011functional" class="csl-entry" role="listitem">
Brattico, Elvira, Vinoo Alluri, Brigitte Bogert, Thomas Jacobsen, Nuutti Vartiainen, Sirke Katriina Nieminen, and Mari Tervaniemi. 2011. “A Functional MRI Study of Happy and Sad Emotions in Music with and Without Lyrics.” <em>Frontiers in Psychology</em> 2: 308.
</div>
<div id="ref-cabitza2020if" class="csl-entry" role="listitem">
Cabitza, Federico, Andrea Campagner, and Luca Maria Sconfienza. 2020. “As If Sand Were Stone. New Concepts and Metrics to Probe the Ground on Which to Build Trustable AI.” <em>BMC Medical Informatics and Decision Making</em> 20 (1): 1–21.
</div>
<div id="ref-DBLP:conf/recsys/ChenLSZ18" class="csl-entry" role="listitem">
Chen, Ching-Wei, Paul Lamere, Markus Schedl, and Hamed Zamani. 2018. “Recsys Challenge 2018: Automatic Music Playlist Continuation.” In <em>Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018</em>, edited by Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John O’Donovan, 527–28. ACM. <a href="https://doi.org/10.1145/3240323.3240342" class="uri">https://doi.org/10.1145/3240323.3240342</a>.
</div>
<div id="ref-conrad2019extreme" class="csl-entry" role="listitem">
Conrad, Frederick, Jason Corey, Samantha Goldstein, Joseph Ostrow, and Michael Sadowsky. 2019. “Extreme Re-Listening: Songs People Love… And Continue to Love.” <em>Psychology of Music</em> 47 (2): 158–72.
</div>
<div id="ref-davison2000multidimensional" class="csl-entry" role="listitem">
Davison, Mark L, and Stephen G Sireci. 2000. “Multidimensional Scaling.” In <em>Handbook of Applied Multivariate Statistics and Mathematical Modeling</em>, 323–52. Elsevier.
</div>
<div id="ref-DeBruine_Jones_2018" class="csl-entry" role="listitem">
DeBruine, Lisa M, and Benedict C Jones. 2018. “Determining the Number of Raters for Reliable Mean Ratings.” OSF. <a href="https://doi.org/10.17605/OSF.IO/X7FUS" class="uri">https://doi.org/10.17605/OSF.IO/X7FUS</a>.
</div>
<div id="ref-demetriou2018vocals" class="csl-entry" role="listitem">
Demetriou, Andrew, Andreas Jansson, Aparna Kumar, and Rachel M Bittner. 2018. “Vocals in Music Matter: The Relevance of Vocals in the Minds of Listeners.” In <em>ISMIR</em>, 514–20.
</div>
<div id="ref-DBLP:conf/acl/FaruquiD14" class="csl-entry" role="listitem">
Faruqui, Manaal, and Chris Dyer. 2014. “Community Evaluation and Exchange of Word Vectors at Wordvectors.org.” In <em>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, System Demonstrations</em>, 19–24. The Association for Computer Linguistics. <a href="https://doi.org/10.3115/V1/P14-5004" class="uri">https://doi.org/10.3115/V1/P14-5004</a>.
</div>
<div id="ref-gardikiotis2012rock" class="csl-entry" role="listitem">
Gardikiotis, Antonis, and Alexandros Baltzis. 2012. “‘Rock Music for Myself and Justice to the World!’: Musical Identity, Values, and Music Preferences.” <em>Psychology of Music</em> 40 (2): 143–63.
</div>
<div id="ref-graham2009liberals" class="csl-entry" role="listitem">
Graham, Jesse, Jonathan Haidt, and Brian A Nosek. 2009. “Liberals and Conservatives Rely on Different Sets of Moral Foundations.” <em>Journal of Personality and Social Psychology</em> 96 (5): 1029.
</div>
<div id="ref-groves2009survey" class="csl-entry" role="listitem">
Groves, Robert M, Floyd J Fowler Jr, Mick P Couper, James M Lepkowski, Eleanor Singer, and Roger Tourangeau. 2009. <em>Survey Methodology</em>. Vol. 561. John Wiley &amp; Sons.
</div>
<div id="ref-holtrop2022exploring" class="csl-entry" role="listitem">
Holtrop, Djurre, Janneke K Oostrom, Ward R J van Breda, Antonis Koutsoumpis, and Reinout E de Vries. 2022. “Exploring the Application of a Text-to-Personality Technique in Job Interviews.” <em>European Journal of Work and Organizational Psychology</em> 31 (6): 799–816.
</div>
<div id="ref-homan2022annotator" class="csl-entry" role="listitem">
Homan, Christopher, Tharindu Cyril Weerasooriya, Lora Aroyo, and Chris Welty. 2022. “Annotator Response Distributions as a Sampling Frame.” In <em>Proceedings of the 1st Workshop on Perspectivist Approaches to NLP@ LREC2022</em>, 56–65.
</div>
<div id="ref-howlin2020patients" class="csl-entry" role="listitem">
Howlin, Claire, and Brendan Rooney. 2020. “Patients Choose Music with High Energy, Danceability, and Lyrics in Analgesic Music Listening Interventions.” <em>Psychology of Music</em>, 0305735620907155.
</div>
<div id="ref-inel2023collect" class="csl-entry" role="listitem">
Inel, Oana, Tim Draws, and Lora Aroyo. 2023. “Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection.” In <em>Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, 11:51–64. 1.
</div>
<div id="ref-kern2023annotation" class="csl-entry" role="listitem">
Kern, Christoph, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, and Frauke Kreuter. 2023. “Annotation Sensitivity: Training Data Collection Methods Affect Model Performance.” <em>arXiv Preprint arXiv:2311.14212</em>.
</div>
<div id="ref-kiesel2022identifying" class="csl-entry" role="listitem">
Kiesel, Johannes, Milad Alshomary, Nicolas Handke, Xiaoni Cai, Henning Wachsmuth, and Benno Stein. 2022. “Identifying the Human Values Behind Arguments.” In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 4459–71. Association for Computational Linguistics.
</div>
<div id="ref-kim2020butter" class="csl-entry" role="listitem">
Kim, Jaehun, Andrew M Demetriou, Sandy Manolios, Maria Stella Tavella, and Cynthia CS Liem. 2020. “Butter Lyrics over Hominy Grit: Comparing Audio and Psychology-Based Text Features in MIR Tasks.” In <em>ISMIR</em>, 861–68.
</div>
<div id="ref-RRAcitation" class="csl-entry" role="listitem">
Kolde, Raivo. 2022. <em>RobustRankAggreg: Methods for Robust Rank Aggregation</em>. <a href="https://CRAN.R-project.org/package=RobustRankAggreg" class="uri">https://CRAN.R-project.org/package=RobustRankAggreg</a>.
</div>
<div id="ref-kolde2012robust" class="csl-entry" role="listitem">
Kolde, Raivo, Sven Laur, Priit Adler, and Jaak Vilo. 2012. “Robust Rank Aggregation for Gene List Integration and Meta-Analysis.” <em>Bioinformatics</em> 28 (4): 573–80.
</div>
<div id="ref-koo2016guideline" class="csl-entry" role="listitem">
Koo, Terry K, and Mae Y Li. 2016. “A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.” <em>Journal of Chiropractic Medicine</em> 15 (2): 155–63.
</div>
<div id="ref-Liem2012MusicGap" class="csl-entry" role="listitem">
Liem, Cynthia C. S., Andreas Rauber, Thomas Lidy, Richard Lewis, Christopher Raphael, Joshua D. Reiss, Tim Crawford, and Alan Hanjalic. 2012. “<span class="nocase">Music Information Technology and Professional Stakeholder Audiences: Mind the Adoption Gap</span>.” In <em>Dagstuhl Follow-Ups</em>. Vol. 3. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik. <a href="https://doi.org/10.4230/DFU.VOL3.11041.227" class="uri">https://doi.org/10.4230/DFU.VOL3.11041.227</a>.
</div>
<div id="ref-lindeman2005measuring" class="csl-entry" role="listitem">
Lindeman, Marjaana, and Markku Verkasalo. 2005. “Measuring Values with the Short Schwartz’s Value Survey.” <em>Journal of Personality Assessment</em> 85 (2): 170–78.
</div>
<div id="ref-maheshwari2017societal" class="csl-entry" role="listitem">
Maheshwari, Tushar, Aishwarya N Reganti, Samiksha Gupta, Anupam Jamatia, Upendra Kumar, Björn Gambäck, and Amitava Das. 2017. “A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content.” In <em>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</em>, 731–41. Association for Computational Linguistics.
</div>
<div id="ref-maio2010mental" class="csl-entry" role="listitem">
Maio, Gregory R. 2010. “Mental Representations of Social Values.” In <em>Advances in Experimental Social Psychology</em>, 42:1–43. Elsevier.
</div>
<div id="ref-manolios2019influence" class="csl-entry" role="listitem">
Manolios, Sandy, Alan Hanjalic, and Cynthia CS Liem. 2019. “The Influence of Personal Values on Music Taste: Towards Value-Based Music Recommendations.” In <em>Proceedings of the 13th ACM Conference on Recommender Systems</em>, 501–5.
</div>
<div id="ref-DBLP:conf/nips/MikolovSCCD13" class="csl-entry" role="listitem">
Mikolov, Tomás, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In <em>Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States</em>, edited by Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, 3111–19. <a href="https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html" class="uri">https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html</a>.
</div>
<div id="ref-north2020relationship" class="csl-entry" role="listitem">
North, Adrian C, Amanda E Krause, and David Ritchie. 2020. “The Relationship Between Pop Music and Lyrics: A Computerized Content Analysis of the United Kingdom’s Weekly Top Five Singles, 1999–2013.” <em>Psychology of Music</em>, 0305735619896409.
</div>
<div id="ref-pennebaker2015development" class="csl-entry" role="listitem">
Pennebaker, James W, Ryan L Boyd, Kayla Jordan, and Kate Blackburn. 2015. “The Development and Psychometric Properties of LIWC2015.”
</div>
<div id="ref-DBLP:conf/emnlp/PenningtonSM14" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. “Glove: Global Vectors for Word Representation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A Meeting of SIGDAT, a Special Interest Group of the ACL</em>, edited by Alessandro Moschitti, Bo Pang, and Walter Daelemans, 1532–43. ACL. <a href="https://doi.org/10.3115/V1/D14-1162" class="uri">https://doi.org/10.3115/V1/D14-1162</a>.
</div>
<div id="ref-ponizovskiy2020development" class="csl-entry" role="listitem">
Ponizovskiy, Vladimir, Murat Ardag, Lusine Grigoryan, Ryan Boyd, Henrik Dobewall, and Peter Holtz. 2020. “Development and Validation of the Personal Values Dictionary: A Theory–Driven Tool for Investigating References to Basic Human Values in Text.” <em>European Journal of Personality</em> 34 (5): 885–902.
</div>
<div id="ref-prabhakaran2023framework" class="csl-entry" role="listitem">
Prabhakaran, Vinodkumar, Christopher Homan, Lora Aroyo, Alicia Parrish, Alex Taylor, Mark Dı́az, and Ding Wang. 2023. “A Framework to Assess (Dis) Agreement Among Diverse Rater Groups.” <em>arXiv Preprint arXiv:2311.05074</em>.
</div>
<div id="ref-preniqi2022more" class="csl-entry" role="listitem">
Preniqi, Vjosa, Kyriaki Kalimeri, and Charalampos Saitis. 2022. “” More Than Words”: Linking Music Preferences and Moral Values Through Lyrics.” <em>arXiv Preprint arXiv:2209.01169</em>.
</div>
<div id="ref-Rcitation" class="csl-entry" role="listitem">
R Core Team. 2022. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.
</div>
<div id="ref-DBLP:conf/emnlp/ReimersG19" class="csl-entry" role="listitem">
Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, edited by Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, 3980–90. Association for Computational Linguistics. <a href="https://doi.org/10.18653/V1/D19-1410" class="uri">https://doi.org/10.18653/V1/D19-1410</a>.
</div>
<div id="ref-richard2003one" class="csl-entry" role="listitem">
Richard, F Dan, Charles F Bond Jr, and Juli J Stokes-Zoota. 2003. “One Hundred Years of Social Psychology Quantitatively Described.” <em>Review of General Psychology</em> 7 (4): 331–63.
</div>
<div id="ref-rokeach1973nature" class="csl-entry" role="listitem">
Rokeach, Milton. 1973. <em>The Nature of Human Values.</em> Free press.
</div>
<div id="ref-sagiv2022personal" class="csl-entry" role="listitem">
Sagiv, Lilach, and Shalom H Schwartz. 2022. “Personal Values Across Cultures.” <em>Annual Review of Psychology</em> 73: 517–46.
</div>
<div id="ref-sandri2023don" class="csl-entry" role="listitem">
Sandri, Marta, Elisa Leonardelli, Sara Tonelli, and Elisabetta Ježek. 2023. “Why Don’t You Do It Right? Analysing Annotators’ Disagreement in Subjective Tasks.” In <em>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2420–33.
</div>
<div id="ref-schindler2012facilitating" class="csl-entry" role="listitem">
Schindler, Alexander, Rudolf Mayer, and Andreas Rauber. 2012. “Facilitating Comprehensive Benchmarking Experiments on the Million Song Dataset.” In <em>ISMIR</em>, 469–74. International Society for Music Information Retrieval.
</div>
<div id="ref-schwartz1992universals" class="csl-entry" role="listitem">
Schwartz, Shalom H. 1992. “Universals in the Content and Structure of Values: Theoretical Advances and Empirical Tests in 20 Countries.” In <em>Advances in Experimental Social Psychology</em>, 25:1–65. Elsevier.
</div>
<div id="ref-schwartz2012overview" class="csl-entry" role="listitem">
———. 2012. “An Overview of the Schwartz Theory of Basic Values.” <em>Online Readings in Psychology and Culture</em> 2 (1): 11.
</div>
<div id="ref-schwartz1987toward" class="csl-entry" role="listitem">
Schwartz, Shalom H, and Wolfgang Bilsky. 1987. “Toward a Universal Psychological Structure of Human Values.” <em>Journal of Personality and Social Psychology</em> 53 (3): 550.
</div>
<div id="ref-schwartz2001extending" class="csl-entry" role="listitem">
Schwartz, Shalom H, Gila Melech, Arielle Lehmann, Steven Burgess, Mari Harris, and Vicki Owens. 2001. “Extending the Cross-Cultural Validity of the Theory of Basic Human Values with a Different Method of Measurement.” <em>Journal of Cross-Cultural Psychology</em> 32 (5): 519–42.
</div>
<div id="ref-swami2013metalheads" class="csl-entry" role="listitem">
Swami, Viren, Fiona Malpass, David Havard, Karis Benford, Ana Costescu, Angeliki Sofitiki, and Donna Taylor. 2013. “Metalheads: The Influence of Personality and Individual Differences on Preference for Heavy Metal.” <em>Psychology of Aesthetics, Creativity, and the Arts</em> 7 (4): 377.
</div>
<div id="ref-van2005world" class="csl-entry" role="listitem">
Van Sickel, Robert W. 2005. “A World Without Citizenship: On (the Absence of) Politics and Ideology in Country Music Lyrics, 1960–2000.” <em>Popular Music and Society</em> 28 (3): 313–31.
</div>
<div id="ref-Psychcitation" class="csl-entry" role="listitem">
William Revelle. 2023. <em>Psych: Procedures for Psychological, Psychometric, and Personality Research</em>. Evanston, Illinois: Northwestern University. <a href="https://CRAN.R-project.org/package=psych" class="uri">https://CRAN.R-project.org/package=psych</a>.
</div>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://www.musicbusinessworldwide.com/files/2022/12/f23d5bc086957241e6177f054507e67b.png" class="uri">https://www.musicbusinessworldwide.com/files/2022/12/f23d5bc086957241e6177f054507e67b.png</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://www.gwi.com/reports/music-streaming-around-the-world" class="uri">https://www.gwi.com/reports/music-streaming-around-the-world</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://en.wikipedia.org/wiki/List_of_instrumental_number_ones_on_the_UK_Singles_Chart" class="uri">https://en.wikipedia.org/wiki/List_of_instrumental_number_ones_on_the_UK_Singles_Chart</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://research.atspotify.com/2020/09/the-million-playlist-dataset-remastered/" class="uri">https://research.atspotify.com/2020/09/the-million-playlist-dataset-remastered/</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://www.musixmatch.com/" class="uri">https://www.musixmatch.com/</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Each member independently screened each lyric and the screening process overall was discussed at length.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>e.g., Spotify reports over 100 million songs in its catalogue<a href="https://newsroom.spotify.com/company-info/" class="uri">https://newsroom.spotify.com/company-info/</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Full code of our sampling procedure is at&nbsp;<a href="https://anonymous.4open.science/r/lyrics-value-estimators-CE33/1_stimulus_sampling/stratified_sampling.py" class="uri">https://anonymous.4open.science/r/lyrics-value-estimators-CE33/1_stimulus_sampling/stratified_sampling.py</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://prolific.co" class="uri">https://prolific.co</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a href="https://qualtrics.com" class="uri">https://qualtrics.com</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>It has shown correlations ranging from .45-.70 per value with longer more established procedures, test-retest reliability, as well as the typical values structure shown in Figure&nbsp;<a href="#fig:circle" data-reference-type="ref" data-reference="fig:circle">2</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>.7 is a commonly considered an acceptable level of reliability in the form of internal consistency<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>We assume that the adjusted exact p-value from RRA monotonically decreases as the rank position ascends (i.e., the lower the p-value is, the higher the ranking position is).<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../_chapters/ISMIR_2020/ISMIR_2020.html" class="pagination-link" aria-label="Butter Lyrics Over Harmony Grit">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Butter Lyrics Over Harmony Grit</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>